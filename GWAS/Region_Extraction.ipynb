{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Extracting data for genomic regions of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Aim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "To extract the summary statistics and genotype on specific genomic regions and calculate their LD matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Pre-requisites\n",
    "\n",
    "Make sure you install the pre-requisited before running this notebook:\n",
    "\n",
    "```\n",
    "pip install pybgen\n",
    "pip install pandas_plink\n",
    "pip install scipy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Input and Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Input\n",
    "\n",
    "- `--region-file`, including a list of regions\n",
    "    - Each locus will be represented by one line in the region file with 3 columns chr, start, and end. e.g. `7 27723990 28723990`\n",
    "- `--geno-path`, the path of a genotype inventory, which lists the path of all genotype file in `bgen` format or in `plink` format.\n",
    "    - The list is a file with 2 columns: `chr genotype_file_chr.ext`. \n",
    "    - The first column is chromosome ID, the 2nd file is genotype for that chromosome.\n",
    "    - When chromosome ID is 0, it implies that the genotype file contains all the genotypes.\n",
    "- `--pheno-path`, the path of a phenotype.\n",
    "    - The phenotype file should have a column with the name `IID`, which is used to represent the sample ID.\n",
    "- `--bgen-sample-path`, the path of a file including the sample in the `bgen` files.\n",
    "    - If the genotype file is in `bgen` format, you should provide this path.\n",
    "- `--sumstats-path`, the path of the GWAS file, including all summary statistics (eg, $\\hat{\\beta}$, $SE(\\hat{\\beta})$ and p-values)\n",
    "    - These summary statistics should contain at least these columns: `chrom, pos, ref, alt, snp_id, bhat, sbhat, p`\n",
    "- `--unrelated-samples`, the file path of unrelated samples with a column named `IID`.   \n",
    "- `--cwd`, the path of output directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Output\n",
    "- `rg_stat`, the reginonal summary stats\n",
    "    - The rowname is the variant ID.\n",
    "    - It should contain at least the following columns: `CHR, BP, SNP, ALT, REF, BETA, SE, Z, P`.\n",
    "- `rg_geno`,the regional genotypes\n",
    "    - The rowname is the variant ID, which should match with the rowname of `rg_stat`.\n",
    "    - The column name is the sample's IID, which is sorted by the sample in phenotype.\n",
    "- `pld`, the regional approximate population LD calculated by unrelated individuals\n",
    "- `sld`, the regional approximate sample LD calcualted by unrelated individuals in a phenotype."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Workflow usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Using our minimal working example data-set where we have already generated results for fastGWA,\n",
    "\n",
    "```\n",
    "sos run LMM.ipynb fastGWA \\\n",
    "    --cwd output \\\n",
    "    --bfile data/genotypes.bed \\\n",
    "    --sampleFile data/imputed_genotypes.sample \\\n",
    "    --genoFile data/imputed_genotypes_chr*.bgen \\\n",
    "    --phenoFile data/phenotypes.txt \\\n",
    "    --formatFile data/fastGWA_template.yml \\\n",
    "    --phenoCol BMI \\\n",
    "    --covarCol SEX \\\n",
    "    --qCovarCol AGE \\\n",
    "    --numThreads 1 \\\n",
    "    --bgenMinMAF 0.001 \\\n",
    "    --bgenMinINFO 0.1 \\\n",
    "    --parts 2 \\\n",
    "    --p-filter 1\n",
    "```\n",
    "\n",
    "```\n",
    "sos run Region_Extraction.ipynb \\\n",
    "    --cwd candidate_loci \\\n",
    "    --region-file data/regions.txt \\\n",
    "    --pheno-path data/phenotypes.txt \\\n",
    "    --geno-path data/genotype_inventory.txt \\\n",
    "    --bgen-sample-path data/imputed_genotypes.sample \\\n",
    "    --sumstats-path output/phenotypes_BMI.fastGWA.snp_stats.gz \\\n",
    "    --unrelated-samples data/unrelated_samples.txt \\\n",
    "    --job-size 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Workflow codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# Work directory where output will be saved to\n",
    "parameter: cwd = path\n",
    "# Region specifications\n",
    "parameter: region_file = path\n",
    "# Genotype file inventory\n",
    "parameter: geno_path = path\n",
    "# Phenotype path\n",
    "parameter: pheno_path = path\n",
    "# Sample file path, for bgen format\n",
    "parameter: bgen_sample_path = path('.')\n",
    "# Path to summary stats file\n",
    "parameter: sumstats_path = path\n",
    "# Path to summary stats format configuration\n",
    "parameter: format_config_path = path('.')\n",
    "# Path to samples of unrelated individuals\n",
    "parameter: unrelated_samples = path\n",
    "# Number of tasks to run in each job on cluster\n",
    "parameter: job_size = int\n",
    "\n",
    "fail_if(not region_file.is_file(), msg = 'Cannot find regions to extract. Please specify them using ``--region-file`` option.')\n",
    "# Load all regions of interest. Each item in the list will be a region: (chr, start, end)\n",
    "regions = list(set([tuple(x.strip().split()) for x in open(region_file).readlines() if x.strip()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[default_1 (export utils script)]\n",
    "depends: Py_Module('xxhash'), Py_Module('pandas'), Py_Module('dask'), Py_Module('scipy')\n",
    "output: f'{cwd:a}/utils.py'\n",
    "report: expand = '${ }', output=f'{cwd:a}/utils.py'\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import dask.dataframe as dd\n",
    "    from xxhash import xxh32 as xxh\n",
    "\n",
    "    def shorten_id(x):\n",
    "        return x if len(x) < 30 else f\"{x.split('_')[0]}_{xxh(x).hexdigest()}\"\n",
    "\n",
    "    def read_sumstat(file, config_file):\n",
    "        sumstats = pd.read_csv(file, compression='gzip', header=0, sep='\\t', quotechar='\"')\n",
    "        if config_file is not None:\n",
    "            import yaml\n",
    "            config = yaml.safe_load(open(config_file, 'r'))\n",
    "            try:\n",
    "                sumstats = sumstats.loc[:,list(config.values())]\n",
    "            except:\n",
    "                raise ValueError(f'According to {config_file}, input summary statistics should have the following columns: {list(config.values())}.')\n",
    "            sumstats.columns = list(config.keys())\n",
    "        sumstats.SNP = sumstats.SNP.apply(shorten_id)\n",
    "        sumstats.CHR = sumstats.CHR.astype(int)\n",
    "        sumstats.POS = sumstats.POS.astype(int)\n",
    "        return sumstats\n",
    "\n",
    "    def regional_stats(sumstats, region):\n",
    "        ss = sumstats[(sumstats.CHR == region[0]) & (sumstats.POS >= region[1]) & (sumstats.POS <= region[2])]\n",
    "        ss.loc[:,'Z'] = p2z(ss.P,ss.BETA)\n",
    "        return ss\n",
    "\n",
    "    from scipy.stats import norm\n",
    "    def p2z(pval,beta,twoside=True):\n",
    "        if twoside:\n",
    "            pval = pval/2\n",
    "        z=np.abs(norm.ppf(pval))\n",
    "        ind=beta<0\n",
    "        z[ind]=-z[ind]\n",
    "        return z\n",
    "\n",
    "    def plink_slice(p,region):\n",
    "        import time\n",
    "        t = time.localtime()\n",
    "        \n",
    "        (bim,fam,bed)=p\n",
    "        print(f'{time.strftime(\"%H:%M:%S\", t)}: slicing geno to get bim')\n",
    "        \n",
    "        # slice geno to obtain all geno info within specified region\n",
    "        bim = p[0].loc[(p[0]['chrom'] == str(region[0])) & (p[0]['pos'] >= region[1]) & (p[0]['pos'] <= region[2])]\n",
    "        \n",
    "        print(f'{time.strftime(\"%H:%M:%S\", t)}: obtaining bed index')\n",
    "        bed = bed[bim.index.tolist(), :]\n",
    "        print(f'{time.strftime(\"%H:%M:%S\", t)}: computing bed with num_workers=1')\n",
    "        bed = bed.compute(num_workers=1)\n",
    "        \n",
    "        return(bim,fam,bed)\n",
    "\n",
    "    def LD_matrix(bed):\n",
    "        snps = pd.DataFrame(bed.transpose())\n",
    "        # use mean imputation to fill missing first, before computing correlations\n",
    "        ld = snps.fillna( snps.mean() ).corr()\n",
    "        return ld\n",
    "\n",
    "    def bgen_region(region,geno,dtype='float16'):\n",
    "        snps,genos=[],[]\n",
    "        i=0\n",
    "        for t,g in geno[0].iter_variants_in_region('0'+str(region[0]) if region[0]<10 else str(region[0]),region[1],region[2]):\n",
    "            snps.append([int(t.chrom),t.name,0.0,t.pos,t.a1,t.a2,i])\n",
    "            genos.append(g.astype(dtype))\n",
    "            i+=1\n",
    "        return(pd.DataFrame(snps,columns=['chrom','snp','cm','pos','a0','a1','i']),np.array(genos))\n",
    "    \n",
    "    def check_unique(idx, variable):\n",
    "        if idx.duplicated().any():\n",
    "            raise ValueError(f\"{variable} index has duplicated elements!\")\n",
    "\n",
    "    def extract_region(org_region, input_sumstats_path, input_format_config, geno_file, input_pheno_path, input_unrelated_samples, output_sumstats, output_genotype, output_pld, output_sld, output_general):\n",
    "        import os\n",
    "        \n",
    "        # Load the file of summary statistics and standardize it.\n",
    "        gwas = read_sumstat(input_sumstats_path, input_format_config)\n",
    "        # Load phenotype file\n",
    "        pheno = pd.read_csv(input_pheno_path, header=0, delim_whitespace=True, quotechar='\"')\n",
    "        # Load unrelated sample file\n",
    "        unr = pd.read_csv(input_unrelated_samples, header=0, delim_whitespace=True, quotechar='\"')\n",
    "        \n",
    "        if geno_file.endswith('.bed'):\n",
    "            plink = True\n",
    "            from pandas_plink import read_plink\n",
    "            geno = read_plink(geno_file)\n",
    "        elif geno_file.endswith('.bgen'):\n",
    "            plink = False\n",
    "            from pybgen import PyBGEN\n",
    "            bgen = PyBGEN(geno_file)\n",
    "            sample_file = geno_file.replace('.bgen', '.sample')\n",
    "            if not os.path.isfile(sample_file):\n",
    "                if not os.path.isfile(${bgen_sample_path:r}):\n",
    "                    raise ValueError(f\"Cannot find the matching sample file ``{sample_file}`` for ``{geno_file}``.\\nYou can specify path to sample file for all BGEN files using ``--bgen-sample-path``.\")\n",
    "                else:\n",
    "                    sample_file = ${bgen_sample_path:r}\n",
    "            bgen_fam = pd.read_csv(sample_file, header=0, delim_whitespace=True, quotechar='\"',skiprows=1)\n",
    "            bgen_fam.columns = ['fid','iid','missing','sex']\n",
    "            geno = [bgen,bgen_fam]\n",
    "        else:\n",
    "            raise ValueError('Plesae provide the genotype files with PLINK binary format or BGEN format')\n",
    "\n",
    "        # extraction starts here\n",
    "        import gc\n",
    "        import time\n",
    "        t = time.localtime()\n",
    "        # Extract the summary stat\n",
    "        print(f'{time.strftime(\"%H:%M:%S\", t)}: Extracting summary statistics ...')\n",
    "        \n",
    "        # chose the method of incrementing the regions by a certain amount and then doing all checking calculations to decrease\n",
    "        # the time it takes for execution and to decrease the likelihood of reaching the memory capacity\n",
    "        \n",
    "        \n",
    "        region_inc = 100000 # size of the incrementer we will be doing\n",
    "        curr_region_lbound = org_region[1] # the current left bound for the regions\n",
    "        curr_region_rbound = org_region[1] + region_inc # the current right bound for the regions\n",
    "        \n",
    "        # WILL NEED THESE FOR LD MATRIX AND ONWARD\n",
    "        iid_ph = []\n",
    "        rg_stat_SNP = []\n",
    "        phenoIID = []\n",
    "        batch_id = 0\n",
    "        while curr_region_lbound <= org_region[2]: # since we want to increment, we want to make sure our left bound is less than max right\n",
    "            # check to see if right bound works\n",
    "            if curr_region_rbound < org_region[2]:\n",
    "                sub_region = (org_region[0], curr_region_lbound, curr_region_rbound)\n",
    "            else:\n",
    "                sub_region = (org_region[0], curr_region_lbound, org_region[2])\n",
    "                \n",
    "            # increment for the next iteration\n",
    "            curr_region_lbound += region_inc + 1\n",
    "            curr_region_rbound += region_inc + 1\n",
    "            \n",
    "            # call and do checks on rg_stat\n",
    "            rg_stat = regional_stats(gwas, region) # only calling on a fraction of the region\n",
    "            rg_stat.index = rg_stat.CHR.astype(str) + '_' + rg_stat.POS.astype(str) + '_' + rg_stat.REF.astype(str) + '_' + rg_stat.ALT.astype(str)\n",
    "            print(f'The regional summary statistics of {sub_region[0]}_{sub_region[1]}_{sub_region[2]} has {len(rg_stat.index)} variants')\n",
    "            check_unique(rg_stat.index, \"Summary statistics\")\n",
    "            \n",
    "            # geno, pheno, unr, and plink are defined prior to the while loop\n",
    "            print(f'{time.strftime(\"%H:%M:%S\", t)}: Extracting genotypes in {\"plink\" if plink else \"bgen\"} format ...')\n",
    "            if plink:\n",
    "                rg_bim,rg_fam,rg_bed = plink_slice(geno,sub_region)\n",
    "            else:\n",
    "                rg_bim,rg_bed=bgen_region(sub_region,geno,dtype='float16')\n",
    "                rg_fam = geno[1]\n",
    "\n",
    "            # FIXME: why do we have duplicates? Let's see in practice how many duplicates are reported. I hope none.\n",
    "            rg_bim.index = rg_bim.chrom.astype(str) + '_' + rg_bim.pos.astype(str) + '_' + rg_bim.a1.astype(str) + '_' + rg_bim.a0.astype(str)\n",
    "            check_unique(rg_bim.index, 'SNPs in reference genotype')\n",
    "            rg_fam.index = rg_fam.iid\n",
    "            check_unique(rg_fam.index, 'FAM info')\n",
    "            rg_bed = pd.DataFrame(rg_bed,index=rg_bim.index,columns=rg_fam.index)\n",
    "            exclude_idx = rg_bed.index.duplicated(keep='first')\n",
    "\n",
    "            exc = []\n",
    "            i = 0\n",
    "            for each in exclude_idx:\n",
    "                if each == True:\n",
    "                    exc.append(i)\n",
    "                i += 1\n",
    "            rg_bed.drop(exc, inplace=True)\n",
    "            \n",
    "            print(f'The regional genotype file of {sub_region[0]}_{sub_region[1]}_{sub_region[2]} has {len(rg_bed.index)} variants')\n",
    "            if not list(rg_stat.index)==list(rg_bed.index):\n",
    "               # overlapping variants\n",
    "                com_row_idx = rg_bed.index.intersection(rg_stat.index)\n",
    "                if len(com_row_idx) == 0:\n",
    "                    raise ValueError(\"Variants ID between summary statistics and reference genotype are completely different\")\n",
    "\n",
    "                print(f'The regional genotype file ({len(rg_bed.index)} variants) and the regional summary statistics ({len(rg_stat.index)} variants) do not match with each other. The overlapping variants ({len(com_row_idx)} variants) will be selected.')\n",
    "                rg_stat = rg_stat.loc[com_row_idx,:]\n",
    "                rg_bed = rg_bed.loc[com_row_idx,:]\n",
    "                \n",
    "                temp_iid_unr = rg_fam.index.intersection(pd.Index(unr.IID)) # iid_unr\n",
    "                \n",
    "                pheno.index = pheno.IID\n",
    "                check_unique(pheno.index, \"Phenotype\")\n",
    "                temp_iid_ph = pheno.index.intersection(rg_fam.index) # iid_ph\n",
    "                \n",
    "                # mean imputation for missing genotypes\n",
    "                rg_bed.fillna( rg_bed.mean(), inplace = True )\n",
    "                \n",
    "                temp_three_intersec = pd.Index(temp_iid_unr).intersection(temp_iid_ph)\n",
    "\n",
    "            batch_id += 1\n",
    "            rg_stat.to_pickle(f'{output_sumstats + \".batch_\" + str(batch_id) + \".pickle\"}')\n",
    "            rg_bed.loc[:,temp_iid_ph].to_pickle(f'{output_genotype + \".batch_\" + str(batch_id) + \".pickle\"}')\n",
    "            rg_bed.loc[:,temp_iid_unr].to_pickle(f'{output_general + \"pre_pop_ld.batch_\" + str(batch_id) + \".pickle\"}')\n",
    "            rg_bed.loc[:,temp_three_intersec].to_pickle(f'{output_general + \"pre_sample_ld.batch_\" + str(batch_id) + \".pickle\"}')\n",
    "                \n",
    "            for each in temp_iid_ph: #order based on pheno\n",
    "                iid_ph.append(each)\n",
    "            for each in rg_stat.SNP:\n",
    "                rg_stat_SNP.append(each)\n",
    "            for each in pheno.IID:\n",
    "                phenoIID.append(each)\n",
    "                \n",
    "            gc.collect()\n",
    "\n",
    "        # genotypes in the sample of a specific phenotype with ordering match\n",
    "        if not iid_ph == phenoIID:\n",
    "            print('Warning: Some samples with phenotype do not have genotypes')    \n",
    "\n",
    "        # merge data into CSV files\n",
    "        print(f'{time.strftime(\"%H:%M:%S\", t)}: Merging data batches ...')\n",
    "        rg_stat = pd.concat([pd.read_pickle(f'{output_sumstats + \".batch_\" + str(b+1) + \".pickle\"}') for b in range(batch_id)])\n",
    "        rg_stat.to_csv(output_sumstats, sep = \"\\t\", header = True, index = True)\n",
    "        rg_bed = pd.concat([pd.read_pickle(f'{output_genotype + \".batch_\" + str(b+1) + \".pickle\"}') for b in range(batch_id)])\n",
    "        rg_bed.to_csv(output_genotype, sep = \"\\t\", header = True, index = True)\n",
    "\n",
    "        # merge genotypes for LD computation and save to pickle files\n",
    "        rg_bed = pd.concat([pd.read_pickle(f'{output_general + \"pre_pop_ld.batch_\" + str(b+1) + \".pickle\"}') for b in range(batch_id)])\n",
    "        rg_bed.to_pickle(f'{output_general + \".pre_pop_ld.pickle\"}')\n",
    "        rg_bed = pd.concat([pd.read_pickle(f'{output_general + \"pre_sample_ld.batch_\" + str(b+1) + \".pickle\"}') for b in range(batch_id)])\n",
    "        rg_bed.to_pickle(f'{output_general + \".pre_sample_ld.pickle\"}')\n",
    "\n",
    "        import os\n",
    "        for b in range(batch_id):\n",
    "            os.remove(f'{output_sumstats + \".batch_\" + str(b+1) + \".pickle\"}')\n",
    "            os.remove(f'{output_genotype + \".batch_\" + str(b+1) + \".pickle\"}')\n",
    "            os.remove(f'{output_general + \"pre_pop_ld.batch_\" + str(b+1) + \".pickle\"}')\n",
    "            os.remove(f'{output_general + \"pre_sample_ld.batch_\" + str(b+1) + \".pickle\"}')\n",
    "\n",
    "    def get_ld(geno_file, output_file, n_partitions=10):\n",
    "        geno = dd.from_pandas(pd.read_pickle(geno_file).transpose(), npartitions=n_partitions)\n",
    "        corr = geno.corr().compute()\n",
    "        corr.to_csv(output_file, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Extract data\n",
    "\n",
    "This step runs in parallel for all loci listed in the region file (via `for_each`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[default_2 (extract genotypes)]\n",
    "depends: Py_Module('pandas_plink'), Py_Module('pybgen'), f'{cwd:a}/utils.py'\n",
    "input: geno_path, pheno_path, sumstats_path, unrelated_samples, for_each = 'regions'\n",
    "output: sumstats = f'{cwd:a}/{_regions[0]}_{_regions[1]}_{_regions[2]}/{sumstats_path:bn}_{_regions[0]}_{_regions[1]}_{_regions[2]}.sumstats.gz',\n",
    "        genotype = f'{cwd:a}/{_regions[0]}_{_regions[1]}_{_regions[2]}/{sumstats_path:bn}_{_regions[0]}_{_regions[1]}_{_regions[2]}.genotype.gz',\n",
    "        pld = f'{cwd:a}/{_regions[0]}_{_regions[1]}_{_regions[2]}/{sumstats_path:bn}_{_regions[0]}_{_regions[1]}_{_regions[2]}.pre_pop_ld.pickle',\n",
    "        sld = f'{cwd:a}/{_regions[0]}_{_regions[1]}_{_regions[2]}/{sumstats_path:bn}_{_regions[0]}_{_regions[1]}_{_regions[2]}.pre_sample_ld.pickle'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '4h', mem = '60G', cores = 1, tags = f'{step_name}_{_output[0]:bn}'\n",
    "python: expand = '${ }', input = f'{cwd:a}/utils.py', stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout'\n",
    "    \n",
    "\n",
    "    import os\n",
    "    # output path files that we will need in our final version\n",
    "    output_sumstats = ${_output['sumstats']:r}\n",
    "    output_genotype = ${_output['genotype']:r}\n",
    "    output_pld = ${_output['pld']:r}\n",
    "    output_sld = ${_output['sld']:r}\n",
    "\n",
    "    # this general path is used to create other temporary files that we need to calculate the ld matrices later on\n",
    "    cwd = os.getcwd()\n",
    "    output_general = '${cwd}/${_regions[0]}_${_regions[1]}_${_regions[2]}/${sumstats_path:bn}_${_regions[0]}_${_regions[1]}_${_regions[2]}'\n",
    "\n",
    "    input_sample_path = ${bgen_sample_path:r}\n",
    "    input_geno_path = ${_input[0]:r}\n",
    "    input_pheno_path = ${_input[1]:r}\n",
    "    input_sumstats_path = ${_input[2]:r}\n",
    "    input_unrelated_samples = ${_input[3]:r}\n",
    "    input_format_config = ${format_config_path:r} if ${format_config_path.is_file()} else None\n",
    "\n",
    "    \n",
    "    # Load genotype file for the region of interest\n",
    "    geno_inventory = dict([x.strip().split() for x in open(${_input[0]:r}).readlines() if x.strip()])\n",
    "    chrom = \"${_regions[0]}\"\n",
    "    if chrom.startswith('chr'):\n",
    "        chrom = chrom[3:]\n",
    "    if chrom not in geno_inventory:\n",
    "        geno_file = geno_inventory['0']\n",
    "    else:\n",
    "        geno_file = geno_inventory[chrom]\n",
    "\n",
    "\n",
    "    if not os.path.isfile(geno_file):\n",
    "        # relative path\n",
    "        if not os.path.isfile('${_input[0]:ad}/' + geno_file):\n",
    "            raise ValueError(f\"Cannot find genotype file {geno_file}\")\n",
    "        else:\n",
    "            geno_file = '${_input[0]:ad}/' + geno_file\n",
    "\n",
    "\n",
    "    region = (int(chrom), ${_regions[1]}, ${_regions[2]})\n",
    "    rg_info = extract_region(region, input_sumstats_path, input_format_config, geno_file, input_pheno_path, input_unrelated_samples,\n",
    "                                output_sumstats, output_genotype, output_pld, output_sld, output_general)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[default_3 (compute LD)]\n",
    "output: pld = f\"{_input['pld']:nn}.population_ld.gz\", \n",
    "        sld = f\"{_input['sld']:nn}.sample_ld.gz\"\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '1h', mem = '60G', cores = 4, tags = f'{step_name}_{_output[0]:bn}'\n",
    "python: expand = '${ }', input = f'{cwd:a}/utils.py', stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout'\n",
    "        get_ld(${_input[\"pld\"]:r}, ${_output[\"pld\"]:r})\n",
    "        get_ld(${_input[\"sld\"]:r}, ${_output[\"sld\"]:r})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "R",
     "ir",
     "R",
     "#DCDCDA",
     "r"
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "panel": {
    "displayed": true,
    "height": 0
   },
   "version": "0.22.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
