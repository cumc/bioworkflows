{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Extracting data for genomic regions of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Aim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "To extract the summary statistics and genotype on specific genomic regions and calculate their LD matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Pre-requisites\n",
    "\n",
    "Make sure you install the pre-requisited before running this notebook:\n",
    "\n",
    "```\n",
    "pip install pybgen\n",
    "pip install bed-reader\n",
    "pip install scipy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Input and Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Input\n",
    "\n",
    "- `--region-file`, including a list of regions\n",
    "    - Each locus will be represented by one line in the region file with 3 columns chr, start, and end. e.g. `7 27723990 28723990`\n",
    "- `--geno-path`, the path of a genotype inventory, which lists the path of all genotype file in `bgen` format or in `plink` format.\n",
    "    - The list is a file with 2 columns: `chr genotype_file_chr.ext`. \n",
    "    - The first column is chromosome ID, the 2nd file is genotype for that chromosome.\n",
    "    - When chromosome ID is 0, it implies that the genotype file contains all the genotypes.\n",
    "- `--pheno-path`, the path of a phenotype.\n",
    "    - The phenotype file should have a column with the name `IID`, which is used to represent the sample ID.\n",
    "- `--bgen-sample-path`, the path of a file including the sample in the `bgen` files.\n",
    "    - If the genotype file is in `bgen` format, you should provide this path.\n",
    "- `--sumstats-path`, the path of the GWAS file, including all summary statistics (eg, $\\hat{\\beta}$, $SE(\\hat{\\beta})$ and p-values)\n",
    "    - These summary statistics should contain at least these columns: `chrom, pos, ref, alt, snp_id, bhat, sbhat, p`\n",
    "- `--unrelated-samples`, the file path of unrelated samples with a column named `IID`.   \n",
    "- `--cwd`, the path of output directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Output\n",
    "- `rg_stat`, the reginonal summary stats\n",
    "    - The rowname is the variant ID.\n",
    "    - It should contain at least the following columns: `CHR, BP, SNP, ALT, REF, BETA, SE, Z, P`.\n",
    "- `rg_geno`,the regional genotypes\n",
    "    - The rowname is the variant ID, which should match with the rowname of `rg_stat`.\n",
    "    - The column name is the sample's IID, which is sorted by the sample in phenotype.\n",
    "- `pld`, the regional approximate population LD calculated by unrelated individuals\n",
    "- `sld`, the regional approximate sample LD calcualted by unrelated individuals in a phenotype."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Workflow usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Using our minimal working example data-set where we have already generated results for fastGWA,\n",
    "\n",
    "### For `bgen` format input\n",
    "\n",
    "```\n",
    "sos run LMM.ipynb fastGWA \\\n",
    "    --cwd output \\\n",
    "    --bfile data/genotypes.bed \\\n",
    "    --sampleFile data/imputed_genotypes.sample \\\n",
    "    --genoFile data/imputed_genotypes_chr*.bgen \\\n",
    "    --phenoFile data/phenotypes.txt \\\n",
    "    --formatFile data/fastGWA_template.yml \\\n",
    "    --phenoCol BMI \\\n",
    "    --covarCol SEX \\\n",
    "    --qCovarCol AGE \\\n",
    "    --numThreads 1 \\\n",
    "    --bgenMinMAF 0.001 \\\n",
    "    --bgenMinINFO 0.1 \\\n",
    "    --parts 2 \\\n",
    "    --p-filter 1\n",
    "```\n",
    "\n",
    "```\n",
    "sos run Region_Extraction.ipynb \\\n",
    "    --cwd candidate_loci \\\n",
    "    --region-file data/regions.txt \\\n",
    "    --pheno-path data/phenotypes.txt \\\n",
    "    --geno-path data/genotype_inventory.txt \\\n",
    "    --bgen-sample-path data/imputed_genotypes.sample \\\n",
    "    --sumstats-path output/phenotypes_BMI.fastGWA.snp_stats.gz \\\n",
    "    --unrelated-samples data/unrelated_samples.txt \\\n",
    "    --job-size 1\n",
    "```\n",
    "\n",
    "### For PLINK format input\n",
    "\n",
    "```\n",
    "sos run LMM.ipynb fastGWA \\\n",
    "    --cwd output \\\n",
    "    --bfile data/genotypes.bed \\\n",
    "    --genoFile data/genotypes21_22.bed \\\n",
    "    --phenoFile data/phenotypes.txt \\\n",
    "    --formatFile data/fastGWA_template.yml \\\n",
    "    --phenoCol BMI \\\n",
    "    --covarCol SEX \\\n",
    "    --qCovarCol AGE \\\n",
    "    --numThreads 1 \\\n",
    "    --bgenMinMAF 0.001 \\\n",
    "    --bgenMinINFO 0.1 \\\n",
    "    --parts 2 \\\n",
    "    --p-filter 1 \n",
    "```\n",
    "\n",
    "```\n",
    "sos run Region_Extraction.ipynb \\\n",
    "    --cwd candidate_loci \\\n",
    "    --region-file data/regions_plink.txt \\\n",
    "    --pheno-path data/phenotypes.txt \\\n",
    "    --geno-path data/genotype_inventory_plink.txt \\\n",
    "    --bgen-sample-path data/imputed_genotypes.sample \\\n",
    "    --sumstats-path output/phenotypes_BMI.fastGWA.snp_stats.gz \\\n",
    "    --unrelated-samples data/unrelated_samples.txt \\\n",
    "    --job-size 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Workflow codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# Work directory where output will be saved to\n",
    "parameter: cwd = path\n",
    "# Region specifications\n",
    "parameter: region_file = path\n",
    "# Genotype file inventory\n",
    "parameter: geno_path = path\n",
    "# Phenotype path\n",
    "parameter: pheno_path = path\n",
    "# Sample file path, for bgen format\n",
    "parameter: bgen_sample_path = path('.')\n",
    "# Path to summary stats file\n",
    "parameter: sumstats_path = path\n",
    "# Path to summary stats format configuration\n",
    "parameter: format_config_path = path('.')\n",
    "# Path to samples of unrelated individuals\n",
    "parameter: unrelated_samples = path\n",
    "# Number of tasks to run in each job on cluster\n",
    "parameter: job_size = int\n",
    "# Specify the container to use\n",
    "parameter: container_lmm = 'statisticalgenetics/lmm:2.9'\n",
    "fail_if(not region_file.is_file(), msg = 'Cannot find regions to extract. Please specify them using ``--region-file`` option.')\n",
    "# Load all regions of interest. Each item in the list will be a region: (chr, start, end)\n",
    "regions = list(set([tuple(x.strip().split()) for x in open(region_file).readlines() if x.strip()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[default_1 (export utils script)]\n",
    "depends: Py_Module('xxhash'), Py_Module('pandas'), Py_Module('dask'), Py_Module('scipy')\n",
    "parameter: scan_window = 500000\n",
    "output: f'{cwd:a}/utils.py'\n",
    "report:container=container_lmm, expand = '${ }', output=f'{cwd:a}/utils.py'\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import dask.dataframe as dd\n",
    "    from xxhash import xxh32 as xxh\n",
    "\n",
    "    def shorten_id(x):\n",
    "        return x if len(x) < 30 else f\"{x.split('_')[0]}_{xxh(x).hexdigest()}\"\n",
    "\n",
    "    def read_sumstat(file, config_file):\n",
    "        sumstats = pd.read_csv(file, compression='gzip', header=0, sep='\\t', quotechar='\"')\n",
    "        if config_file is not None:\n",
    "            import yaml\n",
    "            config = yaml.safe_load(open(config_file, 'r'))\n",
    "            try:\n",
    "                sumstats = sumstats.loc[:,list(config.values())]\n",
    "            except:\n",
    "                raise ValueError(f'According to {config_file}, input summary statistics should have the following columns: {list(config.values())}.')\n",
    "            sumstats.columns = list(config.keys())\n",
    "        sumstats.SNP = sumstats.SNP.apply(shorten_id)\n",
    "        sumstats.CHR = sumstats.CHR.astype(int)\n",
    "        sumstats.POS = sumstats.POS.astype(int)\n",
    "        return sumstats\n",
    "\n",
    "    def regional_stats(sumstats, region):\n",
    "        ss = sumstats[(sumstats.CHR == region[0]) & (sumstats.POS >= region[1]) & (sumstats.POS <= region[2])].copy()\n",
    "        ss['Z'] = pd.Series(p2z(ss.P,ss.BETA))\n",
    "        return ss\n",
    "\n",
    "    from scipy.stats import norm\n",
    "    def p2z(pval,beta,twoside=True):\n",
    "        if twoside:\n",
    "            pval = pval/2\n",
    "        z=np.abs(norm.ppf(pval))\n",
    "        ind=beta<0\n",
    "        z[ind]=-z[ind]\n",
    "        return z\n",
    "\n",
    "    def plink_slice(p,region):\n",
    "        bim = pd.DataFrame(dict(chrom = p.chromosome, pos = p.bp_position, a1 = p.allele_2, a0 = p.allele_1))\n",
    "        fam = pd.DataFrame(dict(fid=p.fid,iid=p.iid))\n",
    "        # see https://github.com/fastlmm/bed-reader\n",
    "        variants = (p.chromosome==str(region[0])) & (p.bp_position >= region[1]) & (p.bp_position <= region[2])\n",
    "        bim = bim[variants]\n",
    "        bed = p.read(index=np.s_[:,variants]).T\n",
    "        return bim, fam, bed\n",
    "\n",
    "    def bgen_region(region,geno,dtype='float16'):\n",
    "        snps,genos=[],[]\n",
    "        i=0\n",
    "        for t,g in geno[0].iter_variants_in_region('0'+str(region[0]) if region[0]<10 else str(region[0]),region[1],region[2]):\n",
    "            snps.append([int(t.chrom),t.name,0.0,t.pos,t.a1,t.a2,i])\n",
    "            genos.append(g.astype(dtype))\n",
    "            i+=1\n",
    "        return(pd.DataFrame(snps,columns=['chrom','snp','cm','pos','a0','a1','i']),np.array(genos))\n",
    "    \n",
    "    def check_unique(idx, variable):\n",
    "        if idx.duplicated().any():\n",
    "            raise ValueError(f\"{variable} index has duplicated elements!\")\n",
    "\n",
    "    def extract_region(org_region, input_sumstats_path, input_format_config, geno_file, input_pheno_path, input_unrelated_samples, output_sumstats, output_genotype, output_pld, output_sld, output_general):\n",
    "        import os\n",
    "        \n",
    "        # Load the file of summary statistics and standardize it.\n",
    "        gwas = read_sumstat(input_sumstats_path, input_format_config)\n",
    "        # Load phenotype file\n",
    "        pheno = pd.read_csv(input_pheno_path, header=0, delim_whitespace=True, quotechar='\"')\n",
    "        # Load unrelated sample file\n",
    "        unr = pd.read_csv(input_unrelated_samples, header=0, delim_whitespace=True, quotechar='\"')\n",
    "        \n",
    "        if geno_file.endswith('.bed'):\n",
    "            plink = True\n",
    "            from bed_reader import open_bed\n",
    "            geno = open_bed(geno_file)\n",
    "        elif geno_file.endswith('.bgen'):\n",
    "            plink = False\n",
    "            from pybgen import PyBGEN\n",
    "            bgen = PyBGEN(geno_file)\n",
    "            sample_file = geno_file.replace('.bgen', '.sample')\n",
    "            if not os.path.isfile(sample_file):\n",
    "                if not os.path.isfile(${bgen_sample_path:r}):\n",
    "                    raise ValueError(f\"Cannot find the matching sample file ``{sample_file}`` for ``{geno_file}``.\\nYou can specify path to sample file for all BGEN files using ``--bgen-sample-path``.\")\n",
    "                else:\n",
    "                    sample_file = ${bgen_sample_path:r}\n",
    "            bgen_fam = pd.read_csv(sample_file, header=0, delim_whitespace=True, quotechar='\"',skiprows=1)\n",
    "            bgen_fam.columns = ['fid','iid','missing','sex']\n",
    "            geno = [bgen,bgen_fam]\n",
    "        else:\n",
    "            raise ValueError('Plesae provide the genotype files with PLINK binary format or BGEN format')\n",
    "\n",
    "        # extraction starts here\n",
    "        import gc\n",
    "        import time\n",
    "        t = time.localtime()\n",
    "        # Extract the summary stat\n",
    "        print(f'{time.strftime(\"%H:%M:%S\", t)}: Extracting summary statistics ...')\n",
    "        \n",
    "        # chose the method of incrementing the regions by a certain amount and then doing all checking calculations to decrease\n",
    "        # the time it takes for execution and to decrease the likelihood of reaching the memory capacity\n",
    "        \n",
    "        \n",
    "        region_inc = ${scan_window} # size of the incrementer we will be doing\n",
    "        curr_region_lbound = org_region[1] # the current left bound for the regions\n",
    "        curr_region_rbound = org_region[1] + region_inc # the current right bound for the regions\n",
    "        \n",
    "        # WILL NEED THESE FOR LD MATRIX AND ONWARD\n",
    "        iid_ph = []\n",
    "        rg_stat_SNP = []\n",
    "        phenoIID = []\n",
    "        batch_id = 0\n",
    "\n",
    "        pop_ld_ind = []\n",
    "        sample_ld_ind = []\n",
    "\n",
    "        while curr_region_lbound <= org_region[2]: # since we want to increment, we want to make sure our left bound is less than max right\n",
    "            # check to see if right bound works\n",
    "            if curr_region_rbound < org_region[2]:\n",
    "                sub_region = (org_region[0], curr_region_lbound, curr_region_rbound)\n",
    "            else:\n",
    "                sub_region = (org_region[0], curr_region_lbound, org_region[2])\n",
    "                \n",
    "            # increment for the next iteration\n",
    "            curr_region_lbound += region_inc + 1\n",
    "            curr_region_rbound += region_inc + 1\n",
    "            \n",
    "            # call and do checks on rg_stat\n",
    "            rg_stat = regional_stats(gwas, region) # only calling on a fraction of the region\n",
    "            rg_stat.index = rg_stat.CHR.astype(str) + '_' + rg_stat.POS.astype(str) + '_' + rg_stat.REF.astype(str) + '_' + rg_stat.ALT.astype(str)\n",
    "            print(f'The regional summary statistics of {sub_region[0]}_{sub_region[1]}_{sub_region[2]} has {len(rg_stat.index)} variants')\n",
    "            check_unique(rg_stat.index, \"Summary statistics\")\n",
    "            \n",
    "            # geno, pheno, unr, and plink are defined prior to the while loop\n",
    "            print(f'{time.strftime(\"%H:%M:%S\", t)}: Extracting genotypes in {\"plink\" if plink else \"bgen\"} format ...')\n",
    "            if plink:\n",
    "                rg_bim,rg_fam,rg_bed = plink_slice(geno,sub_region)\n",
    "            else:\n",
    "                rg_bim,rg_bed=bgen_region(sub_region,geno,dtype='float16')\n",
    "                rg_fam = geno[1]\n",
    "            print(f'{time.strftime(\"%H:%M:%S\", t)}: Checking SNP and sample IDs ...')\n",
    "            # FIXME: why do we have duplicates? Let's see in practice how many duplicates are reported. I hope none.\n",
    "            rg_bim.index = rg_bim.chrom.astype(str) + '_' + rg_bim.pos.astype(str) + '_' + rg_bim.a1.astype(str) + '_' + rg_bim.a0.astype(str)\n",
    "            check_unique(rg_bim.index, 'SNPs in reference genotype')\n",
    "            rg_fam.index = rg_fam.iid\n",
    "            check_unique(rg_fam.index, 'FAM info')\n",
    "            rg_bed = pd.DataFrame(rg_bed,index=rg_bim.index,columns=rg_fam.index)\n",
    "            exclude_idx = rg_bed.index.duplicated(keep='first')\n",
    "\n",
    "            exc = []\n",
    "            i = 0\n",
    "            for each in exclude_idx:\n",
    "                if each == True:\n",
    "                    exc.append(i)\n",
    "                i += 1\n",
    "            rg_bed.drop(exc, inplace=True)\n",
    "            \n",
    "            print(f'The regional genotype file of {sub_region[0]}_{sub_region[1]}_{sub_region[2]} has {len(rg_bed.index)} variants')\n",
    "            if not list(rg_stat.index)==list(rg_bed.index):\n",
    "               # overlapping variants\n",
    "                com_row_idx = rg_bed.index.intersection(rg_stat.index)\n",
    "                if len(com_row_idx) == 0:\n",
    "                    print(\"Variants ID between sub-region summary statistics and reference genotype are non-overlapping. This sub-region is skipped.\")\n",
    "                    continue\n",
    "                print(f'The regional genotype file ({len(rg_bed.index)} variants) and the regional summary statistics ({len(rg_stat.index)} variants) do not match with each other. The overlapping variants ({len(com_row_idx)} variants) will be selected.')\n",
    "                rg_stat = rg_stat.loc[com_row_idx,:]\n",
    "                rg_bed = rg_bed.loc[com_row_idx,:]\n",
    "                \n",
    "            temp_iid_unr = rg_fam.index.intersection(pd.Index(unr.IID)) # iid_unr\n",
    "\n",
    "            pheno.index = pheno.IID\n",
    "            check_unique(pheno.index, \"Phenotype\")\n",
    "            temp_iid_ph = pheno.index.intersection(rg_fam.index) # iid_ph\n",
    "\n",
    "            # mean imputation for missing genotypes\n",
    "            rg_bed.fillna( rg_bed.mean(), inplace = True )\n",
    "\n",
    "            temp_three_intersec = pd.Index(temp_iid_unr).intersection(temp_iid_ph)\n",
    "                \n",
    "\n",
    "            batch_id += 1\n",
    "            rg_stat.to_pickle(f'{output_sumstats + \".batch_\" + str(batch_id) + \".pickle\"}')\n",
    "            rg_bed.loc[:,temp_iid_ph].to_pickle(f'{output_genotype + \".batch_\" + str(batch_id) + \".pickle\"}')\n",
    "\n",
    "            pop_ld = rg_bed.loc[:,temp_iid_unr]\n",
    "            pop_ld_ind.extend(pop_ld.index.to_list())\n",
    "            pop_ld = pop_ld.to_numpy(dtype='float32')\n",
    "            np.save(f'{output_general + \"pre_pop_ld.batch_\" + str(batch_id) + \".npy\"}', pop_ld, allow_pickle=True)\n",
    "\n",
    "            sample_ld = rg_bed.loc[:,temp_three_intersec]\n",
    "            sample_ld_ind.extend(sample_ld.index.to_list())\n",
    "            sample_ld = sample_ld.to_numpy(dtype='float32')\n",
    "            np.save(f'{output_general + \"pre_sample_ld.batch_\" + str(batch_id) + \".npy\"}', sample_ld, allow_pickle=True)\n",
    "    \n",
    "            for each in temp_iid_ph: #order based on pheno\n",
    "                iid_ph.append(each)\n",
    "            for each in rg_stat.SNP:\n",
    "                rg_stat_SNP.append(each)\n",
    "            for each in pheno.IID:\n",
    "                phenoIID.append(each)\n",
    "                \n",
    "            gc.collect()\n",
    "\n",
    "        # genotypes in the sample of a specific phenotype with ordering match\n",
    "        if not iid_ph == phenoIID:\n",
    "            print('Warning: Some samples with phenotype do not have genotypes')    \n",
    "\n",
    "        # merge data into CSV files\n",
    "        print(f'{time.strftime(\"%H:%M:%S\", t)}: Merging data batches ...')\n",
    "        if batch_id == 0:\n",
    "            raise ValueError(\"Region data extraction failed because variants ID between region summary statistics and reference genotype are completely non-overlapping.\")\n",
    "        rg_stat = pd.concat([pd.read_pickle(f'{output_sumstats + \".batch_\" + str(b+1) + \".pickle\"}') for b in range(batch_id)])\n",
    "        rg_stat.to_csv(output_sumstats, sep = \"\\t\", header = True, index = True)\n",
    "        rg_bed = pd.concat([pd.read_pickle(f'{output_genotype + \".batch_\" + str(b+1) + \".pickle\"}') for b in range(batch_id)])\n",
    "        rg_bed.to_csv(output_genotype, sep = \"\\t\", header = True, index = True)\n",
    "\n",
    "        import pickle\n",
    "        \n",
    "        ld = dict()\n",
    "        \n",
    "        ld[\"rg_bed\"] = np.concatenate([np.load(f'{output_general + \"pre_pop_ld.batch_\" + str(b+1) + \".npy\"}', allow_pickle=True) for b in range(batch_id)], axis=0)\n",
    "        ld[\"index\"] = pop_ld_ind\n",
    "    \n",
    "        with open(f'{output_general + \".pre_pop_ld.pickle\"}', 'wb') as handle:\n",
    "            pickle.dump(ld, handle)\n",
    "\n",
    "        ld[\"rg_bed\"] = np.concatenate([np.load(f'{output_general + \"pre_sample_ld.batch_\" + str(b+1) + \".npy\"}', allow_pickle=True) for b in range(batch_id)], axis=0)\n",
    "        ld[\"index\"] = sample_ld_ind\n",
    "\n",
    "        with open(f'{output_general + \".pre_sample_ld.pickle\"}', 'wb') as handle:\n",
    "            pickle.dump(ld, handle)\n",
    "\n",
    "\n",
    "        import os\n",
    "        for b in range(batch_id):\n",
    "            os.remove(f'{output_sumstats + \".batch_\" + str(b+1) + \".pickle\"}')\n",
    "            os.remove(f'{output_genotype + \".batch_\" + str(b+1) + \".pickle\"}')\n",
    "            os.remove(f'{output_general + \"pre_pop_ld.batch_\" + str(b+1) + \".npy\"}')\n",
    "            os.remove(f'{output_general + \"pre_sample_ld.batch_\" + str(b+1) + \".npy\"}')\n",
    "\n",
    "\n",
    "    def get_ld(geno_file, output_file):\n",
    "        import pickle\n",
    "        with open(geno_file, 'rb') as handle:\n",
    "            b = pickle.load(handle)\n",
    "        \n",
    "\n",
    "        index = b[\"index\"]\n",
    "        x = b[\"rg_bed\"]\n",
    "\n",
    "        batch_size = 5\n",
    "        curr = 0\n",
    "\n",
    "        stdev_percol = []\n",
    "        means = []\n",
    "\n",
    "        # for each row in x, compute the stdev and append it\n",
    "        for _, i in enumerate(x):\n",
    "            mean = i.mean()\n",
    "            i = i - mean\n",
    "            i = np.dot(i,i)\n",
    "            stdev_percol.append(np.sqrt(i))\n",
    "            means.append(mean)\n",
    "        \n",
    "        mylis = [[0 if i != j else 1 for i in range(len(x))] for j in range(len(x))] \n",
    "        # first's row information\n",
    "        for i in range(len(x)):\n",
    "            row = []\n",
    "            f = x[i] - means[i]\n",
    "\n",
    "            # second's row information\n",
    "            for j in range(i+1, len(x)):\n",
    "                s = x[j] - means[j]\n",
    "                a = np.dot(f,s)\n",
    "\n",
    "                val = a / (stdev_percol[i] * stdev_percol[j])\n",
    "\n",
    "                mylis[i][j] = val\n",
    "                mylis[j][i] = val\n",
    "\n",
    "        corr = pd.DataFrame(mylis, columns=index)\n",
    "        corr.to_csv(output_file, sep = \"\\t\", header = True, index = False, mode='w')\n",
    "\n",
    "        import os\n",
    "        os.remove(f'{geno_file}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Extract data\n",
    "\n",
    "This step runs in parallel for all loci listed in the region file (via `for_each`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[default_2 (extract genotypes)]\n",
    "depends: Py_Module('bed_reader'), Py_Module('pybgen'), f'{cwd:a}/utils.py'\n",
    "input: geno_path, pheno_path, sumstats_path, unrelated_samples, for_each = 'regions'\n",
    "output: sumstats = f'{cwd:a}/{_regions[0]}_{_regions[1]}_{_regions[2]}/{sumstats_path:bn}_{_regions[0]}_{_regions[1]}_{_regions[2]}.sumstats.gz',\n",
    "        genotype = f'{cwd:a}/{_regions[0]}_{_regions[1]}_{_regions[2]}/{sumstats_path:bn}_{_regions[0]}_{_regions[1]}_{_regions[2]}.genotype.gz',\n",
    "        pld = f'{cwd:a}/{_regions[0]}_{_regions[1]}_{_regions[2]}/{sumstats_path:bn}_{_regions[0]}_{_regions[1]}_{_regions[2]}.pre_pop_ld.pickle',\n",
    "        sld = f'{cwd:a}/{_regions[0]}_{_regions[1]}_{_regions[2]}/{sumstats_path:bn}_{_regions[0]}_{_regions[1]}_{_regions[2]}.pre_sample_ld.pickle'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '4h', mem = '60G', cores = 1, tags = f'{step_name}_{_output[0]:bn}'\n",
    "python: container=container_lmm, expand = '${ }', input = f'{cwd:a}/utils.py', stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout'\n",
    "    \n",
    "\n",
    "    import os\n",
    "    # output path files that we will need in our final version\n",
    "    output_sumstats = ${_output['sumstats']:r}\n",
    "    output_genotype = ${_output['genotype']:r}\n",
    "    output_pld = ${_output['pld']:r}\n",
    "    output_sld = ${_output['sld']:r}\n",
    "\n",
    "    # this general path is used to create other temporary files that we need to calculate the ld matrices later on\n",
    "    cwd = os.getcwd()\n",
    "    output_general = '${cwd}/${_regions[0]}_${_regions[1]}_${_regions[2]}/${sumstats_path:bn}_${_regions[0]}_${_regions[1]}_${_regions[2]}'\n",
    "\n",
    "    input_sample_path = ${bgen_sample_path:r}\n",
    "    input_geno_path = ${_input[0]:r}\n",
    "    input_pheno_path = ${_input[1]:r}\n",
    "    input_sumstats_path = ${_input[2]:r}\n",
    "    input_unrelated_samples = ${_input[3]:r}\n",
    "    input_format_config = ${format_config_path:r} if ${format_config_path.is_file()} else None\n",
    "\n",
    "    \n",
    "    # Load genotype file for the region of interest\n",
    "    geno_inventory = dict([x.strip().split() for x in open(${_input[0]:r}).readlines() if x.strip()])\n",
    "    chrom = \"${_regions[0]}\"\n",
    "    if chrom.startswith('chr'):\n",
    "        chrom = chrom[3:]\n",
    "    if chrom not in geno_inventory:\n",
    "        geno_file = geno_inventory['0']\n",
    "    else:\n",
    "        geno_file = geno_inventory[chrom]\n",
    "\n",
    "\n",
    "    if not os.path.isfile(geno_file):\n",
    "        # relative path\n",
    "        if not os.path.isfile('${_input[0]:ad}/' + geno_file):\n",
    "            raise ValueError(f\"Cannot find genotype file {geno_file}\")\n",
    "        else:\n",
    "            geno_file = '${_input[0]:ad}/' + geno_file\n",
    "\n",
    "\n",
    "    region = (int(chrom), ${_regions[1]}, ${_regions[2]})\n",
    "    rg_info = extract_region(region, input_sumstats_path, input_format_config, geno_file, input_pheno_path, input_unrelated_samples,\n",
    "                                output_sumstats, output_genotype, output_pld, output_sld, output_general)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[default_3 (compute LD sld)]\n",
    "output: sld = f\"{_input['sld']:nn}.sample_ld.gz\"\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '24h', mem = '64G', cores = 4, tags = f'{step_name}_{_output[0]:bn}'\n",
    "python: container=container_lmm, expand = '${ }', input = f'{cwd:a}/utils.py', stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout'\n",
    "    get_ld(${_input[\"sld\"]:r}, ${_output[\"sld\"]:r})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[default_4 (compute LD pld)]\n",
    "input: output_from('default_2')\n",
    "output: pld = f\"{_input['pld']:nn}.population_ld.gz\"\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '24h', mem = '64G', cores = 4, tags = f'{step_name}_{_output[0]:bn}'\n",
    "python: container=container_lmm, expand = '${ }', input = f'{cwd:a}/utils.py', stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout'\n",
    "    get_ld(${_input[\"pld\"]:r}, ${_output[\"pld\"]:r})\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "R",
     "ir",
     "R",
     "#DCDCDA",
     "r"
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "panel": {
    "displayed": true,
    "height": 0
   },
   "version": "0.22.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
