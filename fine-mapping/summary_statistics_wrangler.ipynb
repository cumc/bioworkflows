{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Summary statistics data mungling in preparation for fine-mapping pipelines\n",
    "\n",
    "This pipeline extracts loci of interest from association analysis summary statistics data, intersecting it with genotype data to compute correlation matrix, and output the data-set per loci with summary statistics and genotype correlations matched. Additionally it extracts prior inclusion probability (annotation scores) for each variant in the data-set, if the information is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "This pipeline was devised by Gao Wang and implemented by Min Qiao at The University of Chicago. It can be downloaded [from here](https://github.com/gaow/fine-mapping/tree/master/workflow)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Input\n",
    "\n",
    "- Summary statistics file, `gzip` compressed, containing information of 6 or 7 columns of core information\n",
    "    - chromsome\n",
    "    - position\n",
    "    - reference allele\n",
    "    - alternative allele\n",
    "    - [${\\beta}$ (effect size) and standard error (se)] or $z$ score\n",
    "    \n",
    "  The input file does not have to follow this ordering. It can be specified by `--columns` parameter later. Additionally it is possible to input a column `loci ID` using `--loci-column`. It is relevant for QTL analysis where the same position can belong to different loci.    \n",
    "\n",
    "- Loci file, similar in flavor to `bed` files.\n",
    "\n",
    "      1st column is chr; 2nd is chunk start position; 3rd is chunk end position; 4th is loci identifier.\n",
    "\n",
    "        chr22\t44995308\t46470495\t1699\n",
    "        chr22\t46470495\t47596318\t1700\n",
    "        chr22\t47596318\t48903703\t1701\n",
    "        chr22\t48903703\t49824534\t1702\n",
    "        chr22\t49824534\t51243298\t1703\n",
    "        \n",
    "  If the last column is not available the first 3 columns will be concatenated to become the loci identifier. \n",
    "\n",
    "  Note that default data-base for loci can be, naturally, LD blocks. For example European genomes are typically divided into 1703 LD chunks (exclude X chromosome).\n",
    "\n",
    "\n",
    "- Prior inclusion probability, for example:\n",
    "\n",
    "      1st columns format ‚Äúchr:bp:ref:alt‚ÄùÔºå2nd is prior\n",
    "\n",
    "        1:1847856:T:G  1.4413e-04\n",
    "        1:1847979:T:C  7.3716e-05\n",
    "        1:1848109:C:G  1.4413e-04\n",
    "        1:1848160:A:G  1.4413e-04\n",
    "        1:1848734:A:G  7.3716e-05\n",
    "        \n",
    "  This is the default format from the enrichment analysis tool called `torus`.\n",
    "  \n",
    "- Genotype data reference panel (or panels if by chromosome), in VCF format. Ideally this is the genotype data used to generate the summary statistics; but external reference panel can also be used. \n",
    "A popular choice is [1000 Genomes (data download)](http://hgdownload.cse.ucsc.edu/gbdb/hg19/1000Genomes/phase3) for genotypes of \n",
    "[different population (data download)](ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/integrated_call_samples_v3.20130502.ALL.panel).\n",
    "    - There are 503 Europeans (`EUR`) in 1000 Genomes data.\n",
    "  \n",
    "  For genotype in multiple VCF files the input have to be a 'manifest' file under the same directory with the VCF files and reads like:\n",
    "  ```\n",
    "  1 ALL.chr1.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.EUR.vcf.gz\n",
    "  2 ALL.chr2.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.EUR.vcf.gz\n",
    "  ...\n",
    "  ```\n",
    "  where each line is white-space separated with the first the chromosome number and second the file name.\n",
    "  \n",
    "  **For UChicago midway users**: reference genotype data for 1000 Genomes EUR samples are extracted using workflow `prepare_1KG_reference` below. The output can be found under `/project2/xinhe/mqiao/SuSiE_GWAS/1KG_EUR/`. The manifest file `1KG_EUR.manifest` was prepared by:\n",
    "  ```\n",
    "  for i in {1..22} X; do echo $i ALL.chr$i.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.EUR.vcf.gz; done > 1KG_EUR.manifest\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "The end of this workflow notebook has some example commands running on some toy data. These toy data (500MB; did not bother to make it smaller due to my laziness) [can be downloaded from here](http://shiny.stephenslab.uchicago.edu/gaow/finemapping_summary_stats_example.tar.gz) if you want to reproduce these commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Pitfalls in data mungling with external reference panel\n",
    "\n",
    "1. Genomic coordinate can be zero- or one-based. That means SNP positions can start from **start from 0**, instead of 1. This is indeed the case for 1000 Genomes data. In order to be consistent with one-based summary statistics, we need to **add 1** to all 1000 genome SNPs position.\n",
    "2. Reference and alternative alleles may mismatch between summary statistics and the reference panel. If it is a simple ref / alt flip we need to also flip the coding of genotypes or sign of summary statistics to adjust for the effect size direction. If it is strand flip we need to convert summary statistics and genotype data to use the same strand first and take from there. \n",
    "3. When strand flip is involved, cases such as A/T and C/G are no longer identifiable -- whether it be strand flip or ref / alt flip. We will have to remove these locus (having A/T or C/G genotypes) from analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Data extraction steps\n",
    "\n",
    "1. Denote summary statistics matrix in the specific loci as $S_1$, and annotation (prior) matrix as $A_1$. The number of row in these two matrices is the number of SNPs in summary statistics of the study of interest.\n",
    "2. Extract corresponding genotype from reference panel in VCF format for this loci. Denote this genotype matrix as $G_1$. Rows are SNPs, columns are population genotypes. \n",
    "    - Genotype coding: we use numeric coding 0, 1 and 2 indicating the number of \"non-reference allele\". In other words we have the following \"mapping rule\":\n",
    "        ```\n",
    "        0|0  ->  0\n",
    "        1|0  ->  1\n",
    "        1|1  ->  2\n",
    "        2|0  ->  1\n",
    "        2|1  ->  2\n",
    "        2|2  ->  2\n",
    "        ```\n",
    "    - Non-variant sites (lines having identical genotypes for everybody) will be removed.\n",
    "    \n",
    "3. Find overlapped SNPs of $S_1$ and $G_1$, then extract new genotype matrix from $G_1$ excluding non-overlaps, denote as $G_2$, and new summary statistics matrix from $S_1$ excluding non-overlaps, denote as $S_2$.\n",
    "4. Compare $G_2$ and $S_2$'s reference and alternative allele, flip coding as necessary, generating new summay statistic matrix $S_3$ and new genotype matrix $G_3$. There could be several situations as follows:\n",
    "\n",
    "    - completely identical;\n",
    "    - Not identical, but identical after switching ref and alt in $S_2$: add opposite sign for z score and beta; does not apply to `A/T`, `T/A`, `G/C` or `C/G` for `ref/alt`;\n",
    "    - Not identical, but identical after strand flip ref and alt in $S_2$: keep the sign of z score and beta; does not apply to `A/T`, `T/A`, `G/C` or `C/G` for `ref/alt`;\n",
    "    - Not identical, but identical after strand flip ref and alt then swith their positions: add opposite sign for z score and beta; only apply to `A/G`, `G/A`, `A/C`, `C/A`, `T/C`, `C/T`, `T/G` and `G/T`.\n",
    "    - Not identical, `A/T`, `T/A`, `G/C` or `C/G` for `ref/alt`: consider this situation at last; if flip strand has applied in this LD block, then flip strand and keep the sign of z score and beta; if not, switch ref and alt of $S_2$, add opposite sign for z score and beta.\n",
    "    - Not identical after previous 5 substeps: drop.\n",
    "5. Calculate row correlation matrix of $G_3$, denote as $R$. The number of rows and columns of $R$ is the number of SNPs in $G_3$.\n",
    "6. Obtain overlapped SNPs for $S_3$ and $A_1$ and use overlapped SNPs to generate new annotation/prior $A_2$.\n",
    "\n",
    "Notice that if genotype data used to generate the summary statistics is available as reference panel, there should not be a need for 4. But it is a good double-check anyways to do 4 -- we therefore provide an option to specify whether or not we expect step 4 is unnecessary; and if so, throw an error when we found discrepency in 4, instead of trying to fix those. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Outputs\n",
    "- $S_3$: the number of rows of $S_3$ is the same with $A_2$. It can be 4 columns, `chr:pos  beta  se  ID`, or 3 columns, `chr:pos z ID`.\n",
    "- $R$: correlation matrix, #SNPs $\\times$ #SNPs of $G_3$.\n",
    "- $A_2$: adjusted annotation/prior information, the number of rows is the same with $S_3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Software requirement\n",
    "\n",
    "- Python package `cyvcf2`, `conda install -c bioconda cyvcf2`\n",
    "- `tabix` and `bcftools`, `conda install -c tabix bcftools`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/midway2/gaow/GIT/github/fine-mapping"
     ]
    }
   ],
   "source": [
    "%cd ~/GIT/github/fine-mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: sos run workflow/summary_statistics_wrangler.ipynb\n",
      "               [workflow_name | -t targets] [options] [workflow_options]\n",
      "  workflow_name:        Single or combined workflows defined in this script\n",
      "  targets:              One or more targets to generate\n",
      "  options:              Single-hyphen sos parameters (see \"sos run -h\" for details)\n",
      "  workflow_options:     Double-hyphen workflow-specific parameters\n",
      "\n",
      "Workflows:\n",
      "  prepare_1KG_reference\n",
      "  default\n",
      "\n",
      "Global Workflow Options:\n",
      "  --reference . (as path)\n",
      "                        Reference panel, either VCF file or a manifest file each\n",
      "                        line is \"chrom number <white space> VCF file name\" the\n",
      "                        manifest file have to be in the same folder as the VCF\n",
      "                        files it refers to.\n",
      "  --loci . (as path)\n",
      "                        Loci file\n",
      "  --ss-data . (as path)\n",
      "                        summary statistics\n",
      "  --[no-]strand-flip (default to True)\n",
      "                        Use --no-strand-flip to set it to false if you are sure\n",
      "                        there is no strand_flip involved\n",
      "  --[no-]ref-flip (default to True)\n",
      "                        Use --no-ref-flip to set it to false if you are sure\n",
      "                        there is no reference / alternative mismatch involved\n",
      "  --job-size 80 (as int)\n",
      "                        For cluster jobs, number of loci to analyze per job\n",
      "\n",
      "Sections\n",
      "  prepare_1KG_reference: Get information about a specified race\n",
      "    Workflow Options:\n",
      "      --race EUR\n",
      "                        Race identifier in 1000 genomes\n",
      "      --panel-meta . (as path)\n",
      "                        Path to\n",
      "                        `integrated_call_samples_v3.20130502.ALL.panel.txt`\n",
      "  default_1:            Get summary stats\n",
      "    Workflow Options:\n",
      "      --columns 1 2 3 4 5 6 (as list)\n",
      "                        Have to be 5 or 6 numbers indicating the required\n",
      "                        columns to be extracted from summary statistics file 6\n",
      "                        numbers for ['chr','bp','ref','alt','beta','se'] 5\n",
      "                        numbers fo ['chr','bp','ref','alt','z']\n",
      "      --loci-column 0 (as int)\n",
      "                        ID column index, optional\n",
      "      --[no-]header (default to True)\n",
      "                        use --no-header to indicate that input summary\n",
      "                        statistics file does not have a header\n",
      "      --adjust-panel-position 0 (as int)\n",
      "                        when set to N (typically 0, 1 or -1) the genomic\n",
      "                        position in the panel will be adjusted by N, ie.\n",
      "                        position = position + N This is useful when summary\n",
      "                        stats and reference panel have different coordinates\n",
      "                        (off by 1 position)\n",
      "      --panel-chrom-prefix ''\n",
      "                        Either empty, or `chr`\n",
      "  default_2:            Get annotations\n",
      "    Workflow Options:\n",
      "      --annotation 'name:/path/to/annotation/file'\n"
     ]
    }
   ],
   "source": [
    "sos run workflow/summary_statistics_wrangler.ipynb -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# Reference panel, either VCF file or a manifest file each line is \"chrom number <white space> VCF file name\"\n",
    "# the manifest file have to be in the same folder as the VCF files it refers to.\n",
    "parameter: reference = path()\n",
    "# Loci file\n",
    "parameter: loci = path()\n",
    "# summary statistics\n",
    "parameter: ss_data = path()\n",
    "# Use --no-strand-flip to set it to false\n",
    "# if you are sure there is no strand_flip involved\n",
    "parameter: strand_flip = True\n",
    "# Use --no-ref-flip to set it to false\n",
    "# if you are sure there is no reference / alternative mismatch involved\n",
    "parameter: ref_flip = True\n",
    "# For cluster jobs, number of loci to analyze per job\n",
    "parameter: job_size = 80\n",
    "\n",
    "# Decide whether or not reference panel matches summary statistics\n",
    "if not ref_flip and not strand_flip:\n",
    "    strictly_match = True\n",
    "else:\n",
    "    strictly_match = False\n",
    "\n",
    "# Check if files exist\n",
    "fail_if(not reference.is_file(), msg = 'Please specify valid path for --reference')\n",
    "fail_if(not loci.is_file(), msg = 'Please specify valid path for --loci')\n",
    "fail_if(not ss_data.is_file(), msg = 'Please specify valid path for --ss-data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Preparing external reference genotypes\n",
    "\n",
    "For 1000 genomes data, using `integrated_call_samples_v3.20130502.ALL.panel.txt` file to extract sample ID for given population (eg `EUR`) and subset the data. We only need to run it once.\n",
    "\n",
    "If loci manifest is provided it generate both a new loci manifest file and the files it contains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run workflow/summary_statistics_wrangler.ipynb prepare_1KG_reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Get information about a specified race\n",
    "[prepare_1KG_reference]\n",
    "depends: executable(\"bcftools\"), executable(\"tabix\")\n",
    "# Race identifier in 1000 genomes\n",
    "parameter: race = \"EUR\"\n",
    "# Path to `integrated_call_samples_v3.20130502.ALL.panel.txt`\n",
    "parameter: panel_meta = path()\n",
    "stop_if(not panel_meta.is_file(), msg = 'Please specify valid path for --panel-meta')\n",
    "\n",
    "if str(reference).endswith('vcf.gz'):\n",
    "    chroms = ['0']\n",
    "    genotypes = [reference]\n",
    "else:\n",
    "    manifest = [x.strip().split() for x in open(f'{reference:a}').readlines()]\n",
    "    meta = [x[0] for x in manifest]\n",
    "    genotypes = [x[1] for x in manifest]\n",
    "\n",
    "input: genotypes, group_by = 1, paired_with = dict(chrom=chroms)\n",
    "output: f\"{_input:nn}.{race}.vcf.gz\"\n",
    "task: trunk_workers = 1, trunk_size = 1, walltime = '5m', mem = '2G', cores = 1, tags = f'{step_name}_{_output:bn}'\n",
    "\n",
    "bash: expand = True\n",
    "    grep -w \"{race}\" {panel_meta} | cut -f 1 > {_output:nn}_extracted.txt\n",
    "    bcftools view -S {_output:nn}_extracted.txt {_input} -Oz > {_output}\n",
    "    tabix -p vcf {_output}\n",
    "    rm {_output:nn}_extracted.txt\n",
    "\n",
    "if not str(reference).endswith('vcf.gz'):\n",
    "    with open(f\"{reference:n}.{race}.{reference:x}\", 'w') as f:\n",
    "        for item in _output:\n",
    "            f.write(f'{item.chrom} {item}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Obtain summary statistics data-set\n",
    "\n",
    "1. Obtain reference genotypes for given loci: matrix $G1$\n",
    "2. Obtain summary statistics and the matching genotype correlation for given loci\n",
    "\n",
    "For step 2, we obtain genotype matrix ùê∫2, summary statistics ùëÜ2, compare ref/alt in ùëÜ2 and ùê∫2 => ùëÜ3 and ùê∫3, then calculate ùëÖ=row_corr(ùê∫3) and save to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Get summary stats\n",
    "[default_1 (get summary statistics)]\n",
    "depends: Py_Module('cyvcf2')\n",
    "# Have to be 5 or 6 numbers indicating the required columns\n",
    "# to be extracted from summary statistics file\n",
    "# 6 numbers for ['chr','bp','ref','alt','beta','se']\n",
    "# 5 numbers fo ['chr','bp','ref','alt','z']\n",
    "parameter: columns = [1,2,3,4,5,6]\n",
    "# ID column index, optional\n",
    "parameter: loci_column = 0\n",
    "# use --no-header to indicate that input summary statistics file does not have a header\n",
    "parameter: header = True\n",
    "# when set to N (typically 0, 1 or -1) the genomic position\n",
    "# in the panel will be adjusted by N, ie. position = position + N\n",
    "# This is useful when summary stats and reference panel have different coordinates (off by 1 position)\n",
    "parameter: adjust_panel_position = 1\n",
    "# Either empty, or `chr`\n",
    "parameter: panel_chrom_prefix = ''\n",
    "import os\n",
    "\n",
    "fail_if(len(columns) not in (5,6), msg = 'Input column ID has to be of length 5 or 6.')\n",
    "\n",
    "if str(reference).endswith('vcf.gz'):\n",
    "    genotype = reference\n",
    "else:\n",
    "    genotype = dict([tuple(x.strip().split()) for x in open(f'{reference:a}').readlines()])\n",
    "    for k in genotype:\n",
    "        genotype[k] = os.path.join(f'{reference:ad}', genotype[k])\n",
    "chunks = [x.strip().split() for x in open(f'{loci:a}').readlines() if x.strip() and not x.strip().startswith('#')]\n",
    "prefix = paths([f\"{ss_data:n}/{c[-1] if len(c) > 3 else '%s_%s_%s' % (c[0], c[1], c[2])}\" for c in chunks])\n",
    "    \n",
    "input: for_each = 'chunks'\n",
    "output: summary_stats = f\"{prefix[_index]}/{prefix[_index]:b}.summary_stats.gz\",\n",
    "        ld_matrix = f\"{prefix[_index]}/{prefix[_index]:b}.LD.gz\"\n",
    "        \n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '5m', mem = '5G', cores = 1, tags = f'{step_name}_{_output[\"summary_stats\"]:bn}'\n",
    "\n",
    "python3: expand = \"${ }\", stdout = f'{_output[\"summary_stats\"]:nn}.allele_flip.log'\n",
    "    from cyvcf2 import VCF\n",
    "    import pandas as pd, numpy as np\n",
    "    import datetime, sys\n",
    "\n",
    "    def exit_if_empty(x):\n",
    "        is_empty = (len(x) == 0) if isinstance(x, list) else x.empty\n",
    "        if is_empty:\n",
    "            open(\"${_output['summary_stats']}\", 'w').close()\n",
    "            open(\"${_output['ld_matrix']}\", 'w').close()\n",
    "            sys.exit(0)\n",
    "\n",
    "    # Step 1: extract relevant summary statistics\n",
    "    print(datetime.datetime.now(),\"\\n\")\n",
    "    locus = ${_chunks}\n",
    "    col_to_use = ${[x-1 for x in columns]}\n",
    "    loci_col = ${loci_column}\n",
    "    if loci_col > 0:\n",
    "        col_to_use.append(loci_col-1)\n",
    "    col_ranks = [x for x in sorted(range(len(col_to_use)), key=col_to_use.__getitem__)]\n",
    "    S1 = pd.read_table(${ss_data:ar}, compression='gzip', header = ${0 if header else None}, index_col=False, usecols = col_to_use)\n",
    "    if loci_col > 0:\n",
    "        columns = ['chr','bp','ref','alt','beta','se','locus_id'] if S1.shape[1] == 7 else ['chr','bp','ref','alt','z','locus_id']\n",
    "    else:\n",
    "        columns = ['chr','bp','ref','alt','beta','se'] if S1.shape[1] == 6 else ['chr','bp','ref','alt','z']\n",
    "    S1.columns = [columns[r] for r in col_ranks]\n",
    "    S1 = S1[columns]\n",
    "    S1[\"chr\"] = S1[\"chr\"].apply(lambda x: str(x).replace(\"chr\", \"\"))\n",
    "    locus[0] = str(locus[0]).replace(\"chr\", \"\")\n",
    "    if \"locus_id\" in S1.columns and len(locus) == 4:\n",
    "        S1 = S1[(S1[\"chr\"] == locus[0]) & (S1[\"bp\"] >= int(locus[1])) & (S1[\"bp\"] < int(locus[2])) & (S1[\"locus_id\"] == locus[3])]\n",
    "    else:\n",
    "        S1 = S1[(S1[\"chr\"] == locus[0]) & (S1[\"bp\"] >= int(locus[1])) & (S1[\"bp\"] < int(locus[2]))]\n",
    "    exit_if_empty(S1)\n",
    "    print(\"SAMPLE: %s\\tLENGTH:%d\" % (\"S1\",len(S1)))\n",
    "    S1[\"ID\"] = S1[\"chr\"] + \":\" + S1[\"bp\"].astype(str)\n",
    "\n",
    "    # Step 2: get genotypes\n",
    "    chromosome = '${panel_chrom_prefix}'+'${_chunks[0].replace(\"chr\",\"\")}'\n",
    "    panel = ${path(genotype[_chunks[0].replace(\"chr\",\"\")] if isinstance(genotype, dict) else genotype):ar}\n",
    "    # loci region\n",
    "    queryid = chromosome + \":\" + \"${_chunks[1]}\" + \"-\" + \"${_chunks[2]}\"\n",
    "    off_set = ${adjust_panel_position}\n",
    "    # scan VCF chunk\n",
    "    vcf = VCF(panel, gts012=False)\n",
    "    res = []\n",
    "    n_G0 = 0\n",
    "    var_ids = set(S1[\"ID\"].tolist())\n",
    "    ## Attention: 1000 Genome position start from 0, not 1! So need to set off_set=1 for variant.start\n",
    "    for variant in vcf(queryid):\n",
    "        n_G0 += 1\n",
    "        var_id = f'{variant.CHROM.replace(\"chr\",\"\")}:{variant.start+off_set}'\n",
    "        if var_id not in var_ids:\n",
    "            continue\n",
    "        for i in range(len(variant.ALT)):\n",
    "            line = [var_id, variant.REF, variant.ALT[i]] + \\\n",
    "                    [x[:-1].count(i+1) for x in variant.genotypes]\n",
    "            if len(set(line[3:])) == 1:\n",
    "                # remove non-variant site in reference panel\n",
    "                continue\n",
    "            res.append(line)\n",
    "    print(\"SAMPLE: %s\\tLENGTH:%d\" % (\"G0\",n_G0))\n",
    "    exit_if_empty(res)\n",
    "    G1 = pd.DataFrame(res, columns = ['ID', 'ref', 'alt'] + vcf.samples)\n",
    "    print(\"SAMPLE: %s\\tLENGTH:%d\" % (\"G1\",len(G1)))\n",
    "\n",
    "    # Step 3: overlap genotypes and summary statistics for LD matrix\n",
    "    # S2\n",
    "    # have to drop duplicates in S2 because in some files there are duplicates!\n",
    "    S2 = S1[S1[\"ID\"].isin(G1[\"ID\"])].drop_duplicates()\n",
    "    exit_if_empty(S2)\n",
    "    #S2[\"ID\"] = S2[\"ID\"].fillna\n",
    "    \n",
    "    print(\"SAMPLE: %s\\tLENGTH:%d\" % (\"S2\",len(S2)))\n",
    "    # ùê∫2\n",
    "    # have to drop duplicates in S2 because for some reason there are duplicates in reference panel\n",
    "    G2 = G1[G1[\"ID\"].isin(S1[\"ID\"])].drop_duplicates()\n",
    "    exit_if_empty(G2)\n",
    "    print(\"SAMPLE: %s\\tLENGTH:%d\" % (\"G2\",len(G2)))\n",
    "    # ùëÜ2 and ùê∫2 : reference and alternative\n",
    "    # at ta cg gc\n",
    "    def gt(s1,s2,s3,s4):\n",
    "        if (s1+s2 == \"AT\" and s3+s4 == \"TA\") or (s1+s2 == \"GC\" and s3+s4 == \"CG\" ) or (s1+s2 == \"TA\" and s3+s4 == \"AT\") or (s1+s2 == \"CG\" and s3+s4 == \"GC\"):\n",
    "            return ${0 if strand_flip else 1}\n",
    "        else:\n",
    "            return 1\n",
    "    # flip\n",
    "    def atcg(inp):\n",
    "        if inp == \"A\":\n",
    "            return \"T\"\n",
    "        elif inp == \"T\":\n",
    "            return \"A\"\n",
    "        elif inp == \"G\":\n",
    "            return \"C\"\n",
    "        elif inp == \"C\":\n",
    "            return \"G\"\n",
    "    # S3\n",
    "    FLIPD = []\n",
    "    # adjusted\n",
    "    ADJ = 0\n",
    "    # equal\n",
    "    EQUAL = 0\n",
    "    # flipped by strand \n",
    "    FLIPNUM = 0\n",
    "    # flipped by reference and alternative\n",
    "    REFALTNUM = 0\n",
    "    \n",
    "    # keep at/ta/cg/gc line numbers\n",
    "    at_cg_id = []\n",
    "    for i in range(len(S2)):\n",
    "        old_id = S2.iloc[i,0] + \":\" + str(S2.iloc[i,1]) + \":\" + S2.iloc[i,2] + \":\" + S2.iloc[i,3]\n",
    "        line = S2.iloc[i,:-1].tolist() + [old_id]\n",
    "        G2S2 = G2[G2[\"ID\"]==S2.iloc[i,-1]]\n",
    "        if all([set([S2.iloc[i,2],S2.iloc[i,3]]) != set([G2S2[\"ref\"].iloc[idx], G2S2[\"alt\"].iloc[idx]]) for idx in range(G2S2.shape[0])]):\n",
    "            ADJ += 1\n",
    "        for idx in range(G2S2.shape[0]):\n",
    "            # not at/ta/gc/cg\n",
    "            if gt(S2.iloc[i,2],S2.iloc[i,3],G2S2[\"ref\"].iloc[idx],G2S2[\"alt\"].iloc[idx])==1:\n",
    "                if G2S2[\"ref\"].iloc[idx] == S2.iloc[i,2] and G2S2[\"alt\"].iloc[idx] == S2.iloc[i,3]:\n",
    "                    EQUAL+=1\n",
    "                    FLIPD.append(line)\n",
    "                elif G2S2[\"alt\"].iloc[idx] == S2.iloc[i,2] and G2S2[\"ref\"].iloc[idx] == S2.iloc[i,3]:\n",
    "                    REFALTNUM+=1\n",
    "                    line[2], line[3] = line[3], line[2]\n",
    "                    line[4] = -line[4]\n",
    "                    FLIPD.append(line)\n",
    "                elif atcg(S2.iloc[i,2]) == G2S2[\"ref\"].iloc[idx] and atcg(S2.iloc[i,3]) == G2S2[\"alt\"].iloc[idx]:\n",
    "                    FLIPNUM+=1\n",
    "                    line[2], line[3] = atcg(line[2]), atcg(line[3])\n",
    "                    FLIPD.append(line)\n",
    "                elif atcg(S2.iloc[i,2]) == G2S2[\"alt\"].iloc[idx] and atcg(S2.iloc[i,3]) == G2S2[\"ref\"].iloc[idx]:\n",
    "                    REFALTNUM+=1\n",
    "                    FLIPNUM+=1\n",
    "                    line[2], line[3] = atcg(line[3]), atcg(line[2])\n",
    "                    line[4] = -line[4]\n",
    "                    FLIPD.append(line)\n",
    "                else:\n",
    "                    # not sure what to do yet\n",
    "                    # print(1, line)\n",
    "                    pass\n",
    "            # at/ta/cg/gc only do ref/alt flip\n",
    "            elif gt(S2.iloc[i,2],S2.iloc[i,3],G2S2[\"ref\"].iloc[idx],G2S2[\"alt\"].iloc[idx])==0:\n",
    "                if G2S2[\"ref\"].iloc[idx] == S2.iloc[i,2] and G2S2[\"alt\"].iloc[idx] == S2.iloc[i,3]:\n",
    "                    EQUAL+=1\n",
    "                    FLIPD.append(line)\n",
    "                    at_cg_id.append(len(FLIPD) - 1)\n",
    "                elif G2S2[\"alt\"].iloc[idx] == S2.iloc[i,2] and G2S2[\"ref\"].iloc[idx] == S2.iloc[i,3]:\n",
    "                    REFALTNUM+=1\n",
    "                    line[2], line[3] = line[3], line[2]\n",
    "                    line[4] = -line[4]                \n",
    "                    FLIPD.append(line)\n",
    "                    at_cg_id.append(len(FLIPD) - 1)\n",
    "                else:\n",
    "                    # print(0, line)\n",
    "                    pass\n",
    "    if ${strictly_match} and (FLIPNUM > 0 or REFALTNUM > 0):\n",
    "        raise ValueError(f\"Strict panel match failed for locus {locus}.\")\n",
    "    #\n",
    "    MISMATCH = len(S2) - len(FLIPD)\n",
    "    if FLIPNUM > 0:\n",
    "        # flips involved, we need to remove at/ta/cg/gc\n",
    "        FLIPD = [i for j, i in enumerate(FLIPD) if j not in at_cg_id]\n",
    "    else:\n",
    "        at_cg_id = []\n",
    "    columns = [x for x in S1.columns.tolist() if x != 'ID'] + [\"original_ID\"]\n",
    "    S3 = pd.DataFrame(FLIPD, columns=columns)\n",
    "    S3[\"ID\"] = S3[\"chr\"] +\":\"+S3[\"bp\"].astype(str) + \":\" + S3[\"ref\"] + \":\" + S3[\"alt\"]\n",
    "    G2[\"ID\"] = G2[['ID', 'ref', 'alt']].apply(lambda x: ':'.join(x), axis=1)\n",
    "    # ùê∫3\n",
    "    G3 = G2[G2[\"ID\"].isin(S3[\"ID\"])]\n",
    "    assert G3.shape[0] == S3.shape[0]\n",
    "    exit_if_empty(S3)\n",
    "    # recover original ID\n",
    "    S3[\"ID\"] = S3.pop(\"original_ID\")\n",
    "    S3 = S3.sort_values(by = [\"chr\", \"bp\"]).drop(columns = [x for x in S3.columns if not x in ('ID', 'z', 'beta', 'se')])\n",
    "    columns = S3.columns.tolist()\n",
    "    columns.insert(0, columns.pop(columns.index('ID')))\n",
    "    S3[columns].to_csv(\"${_output['summary_stats']}\",sep=\"\\t\",index=False,header=None, compression = 'gzip')\n",
    "    print(\"SAMPLE: %s\\tLENGTH:%d\" % (\"S3\",len(S3)))\n",
    "    print(\"equal:%s\\nflipped by strand:%d\\nflipped by reference and alternative:%d\\ntotal adjusted:%d\\ntotal mismatch:%d\\ntotal A/T and C/G removed:%d\" % (EQUAL,FLIPNUM,REFALTNUM,ADJ,MISMATCH,len(at_cg_id)))\n",
    "    # ùëÖ=row_corr(ùê∫3) OUT\n",
    "    np.savetxt(\"${_output['ld_matrix']}\", np.corrcoef(G3.drop(columns=['ID', \"ref\",\"alt\"]).values) if G3.shape[0] > 1 else np.array([[1]]), fmt = '%.5f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Obtain annotation $A_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Get annotations\n",
    "[default_2 (get annotations)]\n",
    "parameter: annotation = \"name:/path/to/annotation/file\"\n",
    "anno, prior_file = annotation.split(':')\n",
    "prior_file = path(prior_file)\n",
    "stop_if(not prior_file.is_file(), msg = 'Quit because annotation file is not found')\n",
    "output: f\"{_input['summary_stats']:nn}.{anno}.gz\"\n",
    "skip_if(_input['summary_stats'].stat().st_size == 0 or _input['ld_matrix'].stat().st_size == 0)\n",
    "\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '10m', mem = '2G', cores = 1, tags = f'{step_name}_{_output:bn}'\n",
    "\n",
    "python3: expand=\"${ }\"\n",
    "    import pandas as pd\n",
    "    A1 = pd.read_table(\"${prior_file}\", compression='gzip', sep=\"\\s+\", header=None)\n",
    "    A1.columns = [\"ID\",\"VALUE\"]\n",
    "    S3 = pd.read_table(\"${_input['summary_stats']}\",sep=\"\\t\", header=None, compression='gzip')\n",
    "    # have to drop duplicates in A2 because in some files there are duplicates!\n",
    "    A2 = A1[A1[\"ID\"].isin(S3.iloc[:,0])][[\"ID\",\"VALUE\"]].drop_duplicates()\n",
    "    if set(S3.iloc[:,0]) != set(A2[\"ID\"]):\n",
    "        raise ValueError(\"SNPs in summary statistics and annotation are not identical.\")\n",
    "    else:\n",
    "        A2 = A2.set_index(\"ID\")\n",
    "        A2 = A2.reindex(S3.iloc[:,0])\n",
    "        A2 = A2.reset_index()\n",
    "    A2.to_csv(\"${_output}\",sep=\"\\t\",index=False,header=None, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Run preprocessing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "eQTL data example run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run workflow/summary_statistics_wrangler.ipynb --reference /home/gaow/tmp/19-Dec-2018/1KG_EUR/test.manifest \\\n",
    "        --loci /home/gaow/tmp/19-Dec-2018/metasoft/one_loci.txt \\\n",
    "        --ss-data /home/gaow/tmp/19-Dec-2018/metasoft/gtex_metasoft_summary_stats.gz \\\n",
    "        --columns 1 2 3 4 6 7 --loci-column 5 --no-header \\\n",
    "        -q none -j 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "GWAS data example run (with annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run workflow/summary_statistics_wrangler.ipynb --reference /home/gaow/tmp/19-Dec-2018/1KG_EUR/test.manifest \\\n",
    "        --loci /home/gaow/tmp/19-Dec-2018/SCZ/chunks.list \\\n",
    "        --ss-data /home/gaow/tmp/19-Dec-2018/SCZ/Summary_statistics.gz \\\n",
    "        --columns 1 2 3 4 6 7 --adjust-panel-position 1 \\\n",
    "        --annotation atac-seq:/home/gaow/tmp/19-Dec-2018/SCZ/Annotation_atac-seq.gz \\\n",
    "        -q none -j 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "**For UChicago midway users**: to run these pipelines on RCC cluster, please take a look at `midway2.yml` file (found in this repository), modify it as you see fit, and replace `-q none -j 8` with: \n",
    "\n",
    "```\n",
    "-c workflow/midway2.yml -q midway2 -J 40\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Results preview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Reference panel matching summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"sos_hint\">%preview /home/gaow/tmp/19-Dec-2018/SCZ/Summary_statistics/chr9_84630941_84813641.allele_flip.log</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"sos_hint\">> /home/gaow/tmp/19-Dec-2018/SCZ/Summary_statistics/chr9_84630941_84813641.allele_flip.log (271 B):</div>"
      ],
      "text/plain": [
       "\n",
       "> /home/gaow/tmp/19-Dec-2018/SCZ/Summary_statistics/chr9_84630941_84813641.allele_flip.log (271 B):"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"sos_hint\">13 lines</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-21 12:48:22.194216 \n",
      "\n",
      "SAMPLE: S1\tLENGTH:184\n",
      "SAMPLE: G1\tLENGTH:1034\n",
      "SAMPLE: S2\tLENGTH:182\n",
      "SAMPLE: G2\tLENGTH:182\n",
      "SAMPLE: S3\tLENGTH:182\n",
      "equal:94\n",
      "flipped by strand:0\n",
      "flipped by reference and alternative:88\n",
      "total adjusted:0\n",
      "total mismatch:0\n",
      "total A/T and C/G removed:0"
     ]
    }
   ],
   "source": [
    "%preview /home/gaow/tmp/19-Dec-2018/SCZ/Summary_statistics/chr9_84630941_84813641/chr9_84630941_84813641.allele_flip.log -n --limit 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Summary statistics matrix S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"sos_hint\">%preview /home/gaow/tmp/19-Dec-2018/SCZ/Summary_statistics/chr9_84630941_84813641.summary_stats.txt</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"sos_hint\">> /home/gaow/tmp/19-Dec-2018/SCZ/Summary_statistics/chr9_84630941_84813641.summary_stats.txt (6.0 KiB):</div>"
      ],
      "text/plain": [
       "\n",
       "> /home/gaow/tmp/19-Dec-2018/SCZ/Summary_statistics/chr9_84630941_84813641.summary_stats.txt (6.0 KiB):"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"sos_hint\">182 lines (10 displayed, see --limit)</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9:84631606:A:G\t-0.024303\t0.0176\n",
      "9:84632950:A:G\t0.021595\t0.0174\n",
      "9:84635599:A:G\t-0.0587\t0.0555\n",
      "9:84636858:A:T\t0.011901\t0.025\n",
      "9:84636881:A:G\t0.022104\t0.0174\n",
      "9:84638794:T:C\t-0.038596\t0.0337\n",
      "9:84638880:A:C\t-0.121004\t0.0643\n",
      "9:84639829:A:G\t-0.058703\t0.0558\n",
      "9:84640522:A:G\t0.024605000000000002\t0.0175\n",
      "9:84640717:T:C\t-0.046501999999999995\t0.0345"
     ]
    }
   ],
   "source": [
    "%preview /home/gaow/tmp/19-Dec-2018/SCZ/Summary_statistics/chr9_84630941_84813641/chr9_84630941_84813641.summary_stats.txt -n -l 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Python3"
   },
   "source": [
    "### Annotation/prior matrix A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"sos_hint\">%preview /home/gaow/tmp/19-Dec-2018/SCZ/Summary_statistics/chr9_84630941_84813641.Annotation_atac-seq.txt</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"sos_hint\">> /home/gaow/tmp/19-Dec-2018/SCZ/Summary_statistics/chr9_84630941_84813641.Annotation_atac-seq.txt (4.6 KiB):</div>"
      ],
      "text/plain": [
       "\n",
       "> /home/gaow/tmp/19-Dec-2018/SCZ/Summary_statistics/chr9_84630941_84813641.Annotation_atac-seq.txt (4.6 KiB):"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"sos_hint\">182 lines (10 displayed, see --limit)</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9:84631606:A:G\t7.3588e-05\n",
      "9:84632950:A:G\t7.3588e-05\n",
      "9:84635599:A:G\t7.3588e-05\n",
      "9:84636858:A:T\t7.3588e-05\n",
      "9:84636881:A:G\t7.3588e-05\n",
      "9:84638794:T:C\t7.3588e-05\n",
      "9:84638880:A:C\t7.3588e-05\n",
      "9:84639829:A:G\t0.00023664\n",
      "9:84640522:A:G\t7.3588e-05\n",
      "9:84640717:T:C\t7.3588e-05"
     ]
    }
   ],
   "source": [
    "%preview /home/gaow/tmp/19-Dec-2018/SCZ/Summary_statistics/chr9_84630941_84813641/chr9_84630941_84813641.Annotation_atac-seq.txt -n -l 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### ùëÖ: LD matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "kernel": "Python3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.16511</td>\n",
       "      <td>-0.08450</td>\n",
       "      <td>-0.05728</td>\n",
       "      <td>-0.16511</td>\n",
       "      <td>-0.09862</td>\n",
       "      <td>0.26204</td>\n",
       "      <td>-0.08450</td>\n",
       "      <td>-0.16958</td>\n",
       "      <td>-0.09553</td>\n",
       "      <td>...</td>\n",
       "      <td>0.19386</td>\n",
       "      <td>0.23134</td>\n",
       "      <td>0.28432</td>\n",
       "      <td>-0.09078</td>\n",
       "      <td>-0.13401</td>\n",
       "      <td>-0.06267</td>\n",
       "      <td>0.17942</td>\n",
       "      <td>-0.13495</td>\n",
       "      <td>0.22525</td>\n",
       "      <td>0.36580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.16511</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.35094</td>\n",
       "      <td>-0.12045</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.08884</td>\n",
       "      <td>-0.05748</td>\n",
       "      <td>0.35094</td>\n",
       "      <td>0.99163</td>\n",
       "      <td>-0.08591</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.08443</td>\n",
       "      <td>0.00125</td>\n",
       "      <td>-0.02916</td>\n",
       "      <td>0.70175</td>\n",
       "      <td>-0.08891</td>\n",
       "      <td>0.29019</td>\n",
       "      <td>0.00745</td>\n",
       "      <td>-0.07106</td>\n",
       "      <td>-0.12696</td>\n",
       "      <td>-0.04784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.08450</td>\n",
       "      <td>0.35094</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.06756</td>\n",
       "      <td>0.35094</td>\n",
       "      <td>-0.05645</td>\n",
       "      <td>-0.02924</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.35389</td>\n",
       "      <td>-0.05572</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.11853</td>\n",
       "      <td>0.27991</td>\n",
       "      <td>-0.02222</td>\n",
       "      <td>-0.02840</td>\n",
       "      <td>-0.04367</td>\n",
       "      <td>0.04285</td>\n",
       "      <td>0.29319</td>\n",
       "      <td>0.00498</td>\n",
       "      <td>-0.04503</td>\n",
       "      <td>-0.04552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.05728</td>\n",
       "      <td>-0.12045</td>\n",
       "      <td>-0.06756</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.12045</td>\n",
       "      <td>-0.00022</td>\n",
       "      <td>-0.05648</td>\n",
       "      <td>-0.06756</td>\n",
       "      <td>-0.11798</td>\n",
       "      <td>-0.01960</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.19109</td>\n",
       "      <td>-0.01319</td>\n",
       "      <td>0.00848</td>\n",
       "      <td>-0.08558</td>\n",
       "      <td>-0.08434</td>\n",
       "      <td>-0.01702</td>\n",
       "      <td>-0.01805</td>\n",
       "      <td>-0.06870</td>\n",
       "      <td>0.00707</td>\n",
       "      <td>-0.08792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.16511</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.35094</td>\n",
       "      <td>-0.12045</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.08884</td>\n",
       "      <td>-0.05748</td>\n",
       "      <td>0.35094</td>\n",
       "      <td>0.99163</td>\n",
       "      <td>-0.08591</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.08443</td>\n",
       "      <td>0.00125</td>\n",
       "      <td>-0.02916</td>\n",
       "      <td>0.70175</td>\n",
       "      <td>-0.08891</td>\n",
       "      <td>0.29019</td>\n",
       "      <td>0.00745</td>\n",
       "      <td>-0.07106</td>\n",
       "      <td>-0.12696</td>\n",
       "      <td>-0.04784</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 182 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0        1        2        3        4        5        6        7    \\\n",
       "0  1.00000 -0.16511 -0.08450 -0.05728 -0.16511 -0.09862  0.26204 -0.08450   \n",
       "1 -0.16511  1.00000  0.35094 -0.12045  1.00000 -0.08884 -0.05748  0.35094   \n",
       "2 -0.08450  0.35094  1.00000 -0.06756  0.35094 -0.05645 -0.02924  1.00000   \n",
       "3 -0.05728 -0.12045 -0.06756  1.00000 -0.12045 -0.00022 -0.05648 -0.06756   \n",
       "4 -0.16511  1.00000  0.35094 -0.12045  1.00000 -0.08884 -0.05748  0.35094   \n",
       "\n",
       "       8        9     ...         172      173      174      175      176  \\\n",
       "0 -0.16958 -0.09553   ...     0.19386  0.23134  0.28432 -0.09078 -0.13401   \n",
       "1  0.99163 -0.08591   ...    -0.08443  0.00125 -0.02916  0.70175 -0.08891   \n",
       "2  0.35389 -0.05572   ...    -0.11853  0.27991 -0.02222 -0.02840 -0.04367   \n",
       "3 -0.11798 -0.01960   ...    -0.19109 -0.01319  0.00848 -0.08558 -0.08434   \n",
       "4  0.99163 -0.08591   ...    -0.08443  0.00125 -0.02916  0.70175 -0.08891   \n",
       "\n",
       "       177      178      179      180      181  \n",
       "0 -0.06267  0.17942 -0.13495  0.22525  0.36580  \n",
       "1  0.29019  0.00745 -0.07106 -0.12696 -0.04784  \n",
       "2  0.04285  0.29319  0.00498 -0.04503 -0.04552  \n",
       "3 -0.01702 -0.01805 -0.06870  0.00707 -0.08792  \n",
       "4  0.29019  0.00745 -0.07106 -0.12696 -0.04784  \n",
       "\n",
       "[5 rows x 182 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_table('/home/gaow/tmp/19-Dec-2018/SCZ/Summary_statistics/chr9_84630941_84813641/chr9_84630941_84813641.LD.txt', sep = ' ', header = None, nrows = 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF"
    ],
    [
     "Python3",
     "python3",
     "Python3",
     "#FFD91A"
    ],
    [
     "SoS",
     "sos",
     "",
     ""
    ]
   ],
   "panel": {
    "displayed": true,
    "height": 0,
    "style": "side"
   },
   "version": "0.19.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
