{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# SuSiE RSS\n",
    "\n",
    "Bayesian sum of single-effect (SuSiE) linear regression using z scores\n",
    "\n",
    "After applying LD_Clumping.ipynb and Region_Extraction.ipynb to select regions that overlap between traits, the current pipeline focuses on SuSiE to do fine mapping of those regions to see if theres something of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "To run this notebook follow the example:\n",
    "\n",
    "```\n",
    "sos run SuSiE_RSS.ipynb \\\n",
    "    --cwd /gpfs/gibbs/pi/dewan/data/UKBiobank/results/fine_mapping/f3393_hearing_aid \\\n",
    "    --region_dir /gpfs/gibbs/pi/dewan/data/UKBiobank/results/region_extraction/f3393_hearing_aid \\\n",
    "    --region_file /gpfs/gibbs/pi/dewan/data/UKBiobank/results/region_extraction/f3393_hearing_aid/regions.txt \\\n",
    "    --sumstats_path /gpfs/gibbs/pi/dewan/data/UKBiobank/results/FastGWA_results/results_imputed_data/f3393_hearing_aid/*.snp_stats.gz \\\n",
    "    --container_lmm /home/dc2325/scratch60/lmm_v_1_4.sif \\\n",
    "    --container_marp /gpfs/gibbs/pi/dewan/data/UKBiobank/marp.sif -s build\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# Path to region extraction files\n",
    "parameter: region_dir = path\n",
    "#The region file after LD clumping\n",
    "parameter: region_file = path\n",
    "parameter: sumstats_path = path\n",
    "#The directory for output files\n",
    "parameter: cwd = path\n",
    "## The container with the lmm/marp software. Can be either a dockerhub image or a singularity `sif` file.\n",
    "parameter: container_lmm = 'statisticalgenetics/lmm:2.0'\n",
    "parameter: container_marp = 'gaow/marp'\n",
    "# Specific number of threads to use\n",
    "parameter: numThreads = 2\n",
    "# the pip probability threshold for variant to be considered interesting\n",
    "parameter: pip_cutoff = 0.1\n",
    "# the coverage needed for a CS\n",
    "parameter: coverage = 0.95\n",
    "\n",
    "fail_if(not region_file.is_file(), msg = 'Cannot find regions to fine map. Please specify them using ``--region-file`` option.')\n",
    "# Load all regions of interest. Each item in the list will be a region: (chr, start, end)\n",
    "regions = [x.strip() for x in open(region_file).readlines()]\n",
    "regions = [x.replace(' ', '_' ) for x in regions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[default_1]\n",
    "input: [(f\"{region_dir}/{x}/{sumstats_path:bn}_{x}.sumstats.gz\", f\"{region_dir}/{x}/{sumstats_path:bn}_{x}.sample_ld.gz\") for x in regions], group_by = 2\n",
    "output: [f'{cwd}/{x}.{sumstats_path:bnn}.SuSiE_RSS.rds' for x in regions], group_by=1\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '12h', mem = '20G', cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "R: container=container_lmm, expand = \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout'\n",
    "  \n",
    "    ConvertP2Z <- function(pval, beta) {\n",
    "      z <- abs(qnorm(pval / 2))\n",
    "      z[which(beta < 0)] <- -1 * z[which(beta < 0)]\n",
    "      return(z)\n",
    "    }\n",
    "\n",
    "    fixCorMatrix = function(z, R, maf = NULL, maf_thresh = 0, z_ld_weight = 0, null_weight = NULL, is_cov = FALSE) {\n",
    "        # ignoring the R dimension check because we assume susie_rss was able to properly run\n",
    "        # use the same R here that you added to susie_rss\n",
    "\n",
    "        # MAF filter.\n",
    "        if (!is.null(maf)) {\n",
    "            if (length(maf) != length(z))\n",
    "                stop(paste0(\"The length of maf does not agree with expected \",length(z)))\n",
    "            id = which(maf > maf_thresh)\n",
    "            R = R[id,id]\n",
    "            z = z[id]\n",
    "        }\n",
    "\n",
    "        # Check for NAs in R.\n",
    "        if (any(is.na(R)))\n",
    "            stop(\"R matrix contains missing values\")\n",
    "\n",
    "        # Modify R as needed.\n",
    "        # this is no longer recommended with current susieR implementation\n",
    "        if (z_ld_weight > 0) {\n",
    "            R = susieR:::muffled_cov2cor((1-z_ld_weight)*R + z_ld_weight * tcrossprod(z))\n",
    "            R = (R + t(R))/2\n",
    "        }\n",
    "\n",
    "        if (is.numeric(null_weight) && null_weight == 0)\n",
    "            null_weight = NULL\n",
    "        if (!is.null(null_weight)) {\n",
    "            if (!is.numeric(null_weight))\n",
    "                stop(\"Null weight must be numeric\")\n",
    "            if (null_weight < 0 || null_weight >= 1)\n",
    "                stop(\"Null weight must be between 0 and 1\")\n",
    "            R = cbind(rbind(R,0),0)\n",
    "            z = c(z,0)\n",
    "        }\n",
    "        if (is_cov) {\n",
    "            # Convert any input R to correlation matrix.\n",
    "            # If R has 0 colums and rows, cov2cor produces NaN and warning.\n",
    "            X0 = diag(R) == 0\n",
    "            R = susieR:::muffled_cov2cor(R)\n",
    "            if (sum(X0) > 0)\n",
    "                R[X0,] = R[,X0] = 0\n",
    "        }\n",
    "        return(list(R=R,z=z))\n",
    "    }\n",
    "  \n",
    "    sumstat = read.csv(${_input[0]:r}, sep = '\\t', header=T,stringsAsFactors=F)\n",
    "    sumstat$Z = as.double(ConvertP2Z(sumstat$P, sumstat$BETA))\n",
    "    ld = as.matrix(read.csv(${_input[1]:r}, sep = '\\t', header=T, stringsAsFactors=F))  \n",
    "    R = fixCorMatrix(sumstat$Z, ld)$R\n",
    "    res = susieR::susie_rss(as.double(sumstat$Z), ld, L = 10, coverage = ${coverage})\n",
    "\n",
    "    res$pos = as.integer(sumstat$POS)\n",
    "    res$z = as.double(sumstat$Z)\n",
    "    res$p = as.double(sumstat$P)\n",
    "    res$var_names = sumstat$SNP\n",
    "    res$chr = as.integer(sumstat$CHR)\n",
    "    res$ref = sumstat$REF\n",
    "    res$alt = sumstat$ALT\n",
    "    \n",
    "    corr = susieR:::get_cs_correlation(res, X = NULL, Xcorr = R, max = FALSE)\n",
    "    rownames(corr) <- names(res$cs)\n",
    "    colnames(corr) <- names(res$cs)\n",
    "    \n",
    "    res$cscorr = corr\n",
    "    \n",
    "    if (length(res$sets$cs) > 1) {\n",
    "        index_combos = expand.grid(1:length(res$sets$cs),1:length(res$sets$cs))\n",
    "        in_common = apply(index_combos, 1, function(x) intersect(res$sets$cs[[x[1]]], res$sets$cs[[x[2]]]))\n",
    "        counts = unlist(lapply(in_common, length))\n",
    "  \n",
    "        ovlp_mat = matrix(counts, ncol = length(res$sets$cs), byrow = T)\n",
    "        ovlp_mat[lower.tri(ovlp_mat)] = NA\n",
    "        rownames(ovlp_mat) = names(res$sets$cs)\n",
    "        colnames(ovlp_mat) = names(res$sets$cs)\n",
    "        print(ovlp_mat)\n",
    "        res$sets[[\"ovlp_mat\"]] = ovlp_mat\n",
    "    }\n",
    "    \n",
    "    saveRDS(res, ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[default_2]\n",
    "output: pip_plot = f\"{cwd}/{_input:bn}.png\"\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '12h', mem = '20G', cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "R: container=container_lmm, expand = \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout'\n",
    "    res = readRDS(${_input:r})\n",
    "    png(${_output[0]:r}, width = 14, height=6, unit='in', res=300)\n",
    "    par(mfrow=c(1,2))\n",
    "    susieR::susie_plot(res, y= \"PIP\", pos=list(attr='pos',start=res$pos[1],end=res$pos[length(res$pos)]), add_legend=T, xlab=\"position\")\n",
    "    susieR::susie_plot(res, y= \"z\", pos=list(attr='pos',start=res$pos[1],end=res$pos[length(res$pos)]), add_legend=T, xlab=\"position\", ylab=\"-log10(p)\")\n",
    "    dev.off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[default_3]\n",
    "sep = \"\" #'\\n\\n---\\n'\n",
    "input: group_by = 'all'\n",
    "output: analysis_summary = f'{cwd}/{sumstats_path:bnn}.analysis_summary.md', causalvariants_csv = f'{cwd}/{sumstats_path:bnn}.causalvariants.csv', allvars_csv = f'{cwd}/{sumstats_path:bnn}.allvariants.csv'\n",
    "python: container=container_lmm, expand = \"${ }\"\n",
    "\n",
    "    theme = '''---\n",
    "    theme: base-theme\n",
    "    style: |\n",
    "     p {\n",
    "       font-size: 24px;\n",
    "       height: 900px;\n",
    "       margin-top:1cm;\n",
    "      }\n",
    "      img {\n",
    "        height: 70%;\n",
    "        display: block;\n",
    "        margin-left: auto;\n",
    "        margin-right: auto;\n",
    "      }\n",
    "      body {\n",
    "       margin-top: auto;\n",
    "       margin-bottom: auto;\n",
    "       font-family: verdana;\n",
    "      }\n",
    "    ---    \n",
    "    '''\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # will load the rds file outputted in a previous step\n",
    "    def load_rds(filename, types=None):\n",
    "        import os\n",
    "        import pandas as pd, numpy as np\n",
    "        import rpy2.robjects as RO\n",
    "        import rpy2.robjects.vectors as RV\n",
    "        import rpy2.rinterface as RI\n",
    "        from rpy2.robjects import numpy2ri\n",
    "        numpy2ri.activate()\n",
    "        from rpy2.robjects import pandas2ri\n",
    "        pandas2ri.activate()\n",
    "        def load(data, types, rpy2_version=3):\n",
    "            if types is not None and not isinstance(data, types):\n",
    "                return np.array([])\n",
    "            # FIXME: I'm not sure if I should keep two versions here\n",
    "            # rpy2_version 2.9.X is more tedious but it handles BoolVector better\n",
    "            # rpy2 version 3.0.1 converts bool to integer directly without dealing with\n",
    "            # NA properly. It gives something like (0,1,-234235).\n",
    "            # Possibly the best thing to do is to open an issue for it to the developers.\n",
    "            if rpy2_version == 2:\n",
    "                # below works for rpy2 version 2.9.X\n",
    "                if isinstance(data, RI.RNULLType):\n",
    "                    res = None\n",
    "                elif isinstance(data, RV.BoolVector):\n",
    "                    data = RO.r['as.integer'](data)\n",
    "                    res = np.array(data, dtype=int)\n",
    "                    # Handle c(NA, NA) situation\n",
    "                    if np.sum(np.logical_and(res != 0, res != 1)):\n",
    "                        res = res.astype(float)\n",
    "                        res[res < 0] = np.nan\n",
    "                        res[res > 1] = np.nan\n",
    "                elif isinstance(data, RV.FactorVector):\n",
    "                    data = RO.r['as.character'](data)\n",
    "                    res = np.array(data, dtype=str)\n",
    "                elif isinstance(data, RV.IntVector):\n",
    "                    res = np.array(data, dtype=int)\n",
    "                elif isinstance(data, RV.FloatVector):\n",
    "                    res = np.array(data, dtype=float)\n",
    "                elif isinstance(data, RV.StrVector):\n",
    "                    res = np.array(data, dtype=str)\n",
    "                elif isinstance(data, RV.DataFrame):\n",
    "                    res = pd.DataFrame(data)\n",
    "                elif isinstance(data, RV.Matrix):\n",
    "                    res = np.matrix(data)\n",
    "                elif isinstance(data, RV.Array):\n",
    "                    res = np.array(data)\n",
    "                else:\n",
    "                    # I do not know what to do for this\n",
    "                    # But I do not want to throw an error either\n",
    "                    res = str(data)\n",
    "            else:\n",
    "                if isinstance(data, RI.NULLType):\n",
    "                    res = None\n",
    "                else:\n",
    "                    res = data\n",
    "            if isinstance(res, np.ndarray) and res.shape == (1, ):\n",
    "                res = res[0]\n",
    "            return res\n",
    "        def load_dict(res, data, types):\n",
    "            '''load data to res'''\n",
    "            names = data.names if not isinstance(data.names, RI.NULLType) else [\n",
    "                i + 1 for i in range(len(data))\n",
    "            ]\n",
    "            for name, value in zip(names, list(data)):\n",
    "                if isinstance(value, RV.ListVector):\n",
    "                    res[name] = {}\n",
    "                    res[name] = load_dict(res[name], value, types)\n",
    "                else:\n",
    "                    res[name] = load(value, types)\n",
    "            return res\n",
    "        #\n",
    "        if not os.path.isfile(filename):\n",
    "            raise IOError('Cannot find file ``{}``!'.format(filename))\n",
    "        rds = RO.r['readRDS'](filename)\n",
    "        if isinstance(rds, RV.ListVector):\n",
    "            res = load_dict({}, rds, types)\n",
    "        else:\n",
    "            res = load(rds, types)\n",
    "        return res\n",
    "    \n",
    "    def f7(seq):\n",
    "        seen = set()\n",
    "        seen_add = seen.add\n",
    "        return [x for x in seq if not (x in seen or seen_add(x))]\n",
    "\n",
    "\n",
    "\n",
    "    text = \"\"\n",
    "    sep = '\\n\\n---\\n'\n",
    "    \n",
    "    inp = \"${_input:r}\".split(\" \")\n",
    "    for i, each in enumerate(inp):\n",
    "        inp[i] = \".\".join(each.split(\".\")[:-1])\n",
    "\n",
    "    r = f7(\"${_input:bn}\".split(\" \"))\n",
    "    \n",
    "    num_csets = []\n",
    "    region_info = []\n",
    "    \n",
    "    # this will be a 2d array that stores information about each variant of interest in the phenotype\n",
    "    # this includes all the variants in a cs and all the variants past the cutoff\n",
    "    causalvariant_info = []\n",
    "    allvars_info = []\n",
    "\n",
    "    for reg_i, each in enumerate(f7(inp)):\n",
    "    \n",
    "        rid = r[reg_i].split('.')[0]\n",
    "        \n",
    "        text_temp = \"\"\n",
    "        text_temp += \"#\\n\\n SuSiE RSS {region} \\n\".format(region=r[reg_i])\n",
    "        text_temp += \"![]({region}.png){sep} \\n \\n\".format(region=r[reg_i], sep=sep)\n",
    "\n",
    "        rd = load_rds(each[1:]+\".rds\")\n",
    "        \n",
    "        # find the number of cs in the current region\n",
    "        if rd[\"sets\"][\"cs\"] == None:\n",
    "            num_csets.append(0)\n",
    "        else:\n",
    "            num_csets.append(len(rd[\"sets\"][\"cs\"]))\n",
    "        print(num_csets)\n",
    "        \n",
    "        # this will store the indicies of all variants that cross the threshold\n",
    "        ind_p = []\n",
    "        allvars = []\n",
    "\n",
    "        pval = ${pip_cutoff}\n",
    "\n",
    "        for i, each in enumerate(rd[\"pip\"]):\n",
    "            if each >= pval:\n",
    "                ind_p.append(i)\n",
    "            allvars.append(i)\n",
    "                \n",
    "        sumvars = 0\n",
    "        \n",
    "        # if we have at least one cs in the current region\n",
    "        if num_csets[reg_i] > 0:\n",
    "            tbl_header = \"| chr number | pos at highest pip | ref | alt | region id | cs | highest pip |  \\n\"\n",
    "            tbl_header += \"| --- | --- | --- | --- | --- | --- | --- |  \\n\"\n",
    "\n",
    "            table = \"\"\n",
    "            \n",
    "            sumpips = 0\n",
    "            \n",
    "            for cset in rd[\"sets\"][\"cs\"].keys():\n",
    "                print(cset)\n",
    "                \n",
    "                # if we have many variants in the cs\n",
    "                if isinstance(rd[\"sets\"][\"cs\"][cset], np.ndarray):\n",
    "                    highestpip = 0\n",
    "                    poswhighestpip = -1\n",
    "                    for i in rd[\"sets\"][\"cs\"][cset]:\n",
    "                        i = i.item() - 1\n",
    "                        \n",
    "                        # we make sure that ind_p only stores the variants that aren't in any cs\n",
    "                        if i in ind_p: ind_p.remove(i) \n",
    "                        if i in allvars: allvars.remove(i)\n",
    "                        \n",
    "                        # append variant info\n",
    "                        causalvariant_info.append( [rd[\"chr\"][i], rd[\"pos\"][i], rd[\"ref\"][i], rd[\"alt\"][i], rid, cset, rd[\"pip\"][i]] )\n",
    "                        allvars_info.append( [rd[\"chr\"][i], rd[\"pos\"][i], rd[\"ref\"][i], rd[\"alt\"][i], rid, cset, rd[\"pip\"][i]] )\n",
    "                        \n",
    "                        if rd[\"pip\"][i] > highestpip:\n",
    "                            highestpip = rd[\"pip\"][i]\n",
    "                            poswhighestpip = i\n",
    "                            \n",
    "                        sumpips += rd[\"pip\"][i]\n",
    "                        sumvars += 1\n",
    "                        \n",
    "                    if poswhighestpip > -1:\n",
    "                        i = poswhighestpip\n",
    "                        table += \"| {chr} | {pos} | {ref} | {alt} | {rid} | {cs} | {pip:.2f} |  \\n\".format(chr=rd[\"chr\"][i], pos=rd[\"pos\"][i], ref=rd[\"ref\"][i], alt=rd[\"alt\"][i], rid=rid, cs=cset, pip=rd[\"pip\"][i])\n",
    "                \n",
    "                else: # if we have only one variant in the cs\n",
    "                    i =  rd[\"sets\"][\"cs\"][cset]\n",
    "                    i = i.item() - 1\n",
    "                    \n",
    "                    # we make sure that ind_p only stores the variants that aren't in any cs\n",
    "                    if i in ind_p: ind_p.remove(i)\n",
    "                    if i in allvars: allvars.remove(i)\n",
    "                    \n",
    "                    # append variant info\n",
    "                    causalvariant_info.append( [rd[\"chr\"][i], rd[\"pos\"][i], rd[\"ref\"][i], rd[\"alt\"][i], rid, cset, rd[\"pip\"][i]] )\n",
    "                    allvars_info.append( [rd[\"chr\"][i], rd[\"pos\"][i], rd[\"ref\"][i], rd[\"alt\"][i], rid, cset, rd[\"pip\"][i]] )\n",
    "                    \n",
    "                    table += \"| {chr} | {pos} | {ref} | {alt} | {rid} | {cs} | {pip:.2f} |  \\n\".format(chr=rd[\"chr\"][i], pos=rd[\"pos\"][i], ref=rd[\"ref\"][i], alt=rd[\"alt\"][i], rid=rid, cs=cset, pip=rd[\"pip\"][i])\n",
    "                    \n",
    "                    sumpips += rd[\"pip\"][i]\n",
    "                    sumvars += 1\n",
    "            \n",
    "\n",
    "            text_temp += \"- Total number of variants: {}\\n\".format(len(rd[\"pip\"]))\n",
    "            text_temp += \"- Expected number of causal variants: {:.2f}\\n\".format(sumpips)\n",
    "            text_temp += \"- Number of variants with PIP > {} and not in any CS: {}\\n\\n\".format(pval, len(ind_p))\n",
    "            text_temp += tbl_header + table + sep\n",
    "            \n",
    "            if num_csets[reg_i] > 1:\n",
    "                text_temp += \"#### CORR: Correlation between CS | OLAP: Overlap between CS\\n\"\n",
    "                \n",
    "                cs = list(rd[\"sets\"][\"cs\"].keys())\n",
    "\n",
    "                corrheader = \"|  |\"\n",
    "                corrbreak = \"| --- |\"\n",
    "\n",
    "                for i in cs:\n",
    "                    corrheader += \" CORR {} |\".format(i)\n",
    "                    corrbreak += \" --- |\"\n",
    "                    \n",
    "                corrheader += \"  |\"\n",
    "                corrbreak += \" --- |\"\n",
    "                    \n",
    "                for i in cs:\n",
    "                    corrheader += \" OLAP {} |\".format(i)\n",
    "                    corrbreak += \" --- |\"\n",
    "\n",
    "                corrheader += \"\\n\"\n",
    "                corrbreak += \"\\n\"\n",
    "\n",
    "                body = \"\"\n",
    "\n",
    "                for en, i in enumerate(cs):\n",
    "                    body += \"| {} |\".format(i)\n",
    "                    for j in rd[\"cscorr\"][en]:\n",
    "                        body += \" {:.2f} |\".format(j)\n",
    "                    body += \"  |\"\n",
    "                    for j in rd[\"sets\"][\"cs\"]:\n",
    "                        body += \" {} |\".format(len(np.intersect1d(rd[\"sets\"][\"cs\"][i], rd[\"sets\"][\"cs\"][j])))\n",
    "                    body += \"\\n\"\n",
    "                \n",
    "                text_temp += corrheader + corrbreak + body + sep\n",
    "            \n",
    "        region_info.append(text_temp)\n",
    "            \n",
    "    f = open(${_output[\"analysis_summary\"]:r}, \"w\")\n",
    "    \n",
    "    cset_order = np.argsort(num_csets)\n",
    "    cset_order = cset_order.tolist()\n",
    "    cset_order.reverse()\n",
    "    for c in cset_order:\n",
    "        text += region_info[c]\n",
    "    \n",
    "    f.write(theme + text)\n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "    for i in ind_p:\n",
    "        # append variant info\n",
    "        causalvariant_info.append( [rd[\"chr\"][i], rd[\"pos\"][i], rd[\"ref\"][i], rd[\"alt\"][i], rid, \"None\", rd[\"pip\"][i]] )\n",
    "    for i in allvars:\n",
    "        allvars_info.append( [rd[\"chr\"][i], rd[\"pos\"][i], rd[\"ref\"][i], rd[\"alt\"][i], rid, \"None\", rd[\"pip\"][i]] )\n",
    "        \n",
    "    df = pd.DataFrame(causalvariant_info, columns=[\"chr\", \"pos\", \"ref\", \"alt\", \"rid\", \"cs\", \"pip\"])\n",
    "    df.to_csv(${_output[\"causalvariants_csv\"]:r}, sep = \",\", header = True, index = False)\n",
    "\n",
    "    df = pd.DataFrame(allvars_info, columns=[\"chr\", \"pos\", \"ref\", \"alt\", \"rid\", \"cs\", \"pip\"])\n",
    "    df.to_csv(${_output[\"allvars_csv\"]:r}, sep = \",\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Generate analysis report: HTML file, and optionally PPTX file\n",
    "[default_4]\n",
    "output: f\"{_input['analysis_summary']:n}.html\"\n",
    "sh: container=container_marp, expand = \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout'\n",
    "    node /opt/marp/.cli/marp-cli.js ${_input['analysis_summary']} -o ${_output:a} \\\n",
    "        --title '${region_file:bnn} fine mapping analysis' \\\n",
    "        --allow-local-files\n",
    "    node /opt/marp/.cli/marp-cli.js ${_input['analysis_summary']} -o ${_output:an}.pptx \\\n",
    "        --title '${region_file:bnn} fine mapping analysis' \\\n",
    "        --allow-local-files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "SoS",
     "sos",
     "",
     ""
    ]
   ],
   "version": "0.22.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
