{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Quality control, annotation and rare variant coding of exomes UKBB Q4/2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Aim\n",
    "\n",
    "Prepare the data for further association analyses using the LMM.ipynb on rare variants. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Data description\n",
    "\n",
    "The data consists of 200,000 exomes: 50K exomes were made available in March 2019 and 150K exomes were made available in October 2020\n",
    "\n",
    "The 50K set includes the following family relationships:\n",
    "\n",
    "* 194 parent-offspring pairs\n",
    "* 613 full-sibling pairs\n",
    "* 26 trios\n",
    "* 1 monozygotic twin pair\n",
    "* 195 second degree genetically determined relationships\n",
    "\n",
    "**Quality control published for the 50K set**\n",
    "\n",
    "FASTQ files aligned to GRCh38 with BWA-mem and BAM files generated. \n",
    "\n",
    "In the BAM files identify and mark duplicates using PICARD\n",
    "\n",
    "gVCF files with called variants produced using WeCall\n",
    "\n",
    "Samples excluded if:\n",
    "* Differences between genetic and reported sex\n",
    "* High rates of heterozygosity/contamination (Dstat>0.4)\n",
    "* Low sequence coverage (<85% of bases with 20X coverage)\n",
    "* Sample duplicates \n",
    "* WES variants discordant with genotyping chip\n",
    "\n",
    "Then creation of project-level VCF or pVCF\n",
    "\n",
    "Goldilocks:\n",
    "* SNV with DP<7 changed to no-call\n",
    "* SNV heterozygotes retained if allele balance ratio was AB>=0.15\n",
    "* Multiallelic left-normalized and represented as bi-allelic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Data analysis\n",
    "### Understanding missingness patterns pVCF/PLINK files\n",
    "\n",
    "This pipeline(`UKBB_GWAS_dev/wokflow/plink_missing.ipynb`)is intended to understand the rates of missing data and filter variants for later use in PCA analysis. Please refer to that specific notebook for output format and code to run it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Filter and merge exome plink files (remove duplicated variants, keep SNPs only)\n",
    "\n",
    "In order to run PCA analysis and LMM.ipynb on the exome data. We need to create a single bed file that contains only SNPs and where the duplicated variants have been removed.\n",
    "\n",
    "Refer to the `UKBB_GWAS_dev/wokflow/plink_snps_only.ipynb` for more details\n",
    "\n",
    "In Yale's cluster the filtered bed files are located here:\n",
    "\n",
    "`/gpfs/gibbs/pi/dewan/data/UKBiobank/genotype_files/ukb28374_exomedata/exome_data_OCT2020/exome_files_snpsonly/ukb23155.filtered.merged.bed`\n",
    "\n",
    "In Columbia's cluster they are here:\n",
    "\n",
    "`/mnt/mfs/statgen/UKBiobank/data/exome_data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### PCA analysis\n",
    "\n",
    "This pipeline (`UKBB_GWAS_dev/wokflow/PCA.ipynb`) was used to generate the Principal Components.\n",
    "Sample or variants were removed if:\n",
    "* Sample missingness > 2%\n",
    "* Variant missigness > 1%\n",
    "* MAF < 1%\n",
    "\n",
    "For a more comprenhensive description of the pipeline refer to the specific notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Analysis of common variants\n",
    "\n",
    "After removing the outliers detected using the PCA and Mahalanobis distance, we can proceed to do GWAS using LMM.ipynb on the exome data:\n",
    "\n",
    "Used file `/gpfs/gibbs/pi/dewan/data/UKBiobank/genotype_files/ukb28374_exomedata/exome_data_OCT2020/exome_files_snpsonly/ukb23155.filtered.merged.bed`\n",
    "\n",
    "* Retain variants with MAF>0.001 (same parameters used for inputed data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Analysis of rare variants: quality control of pVCF/PLINK files\n",
    "\n",
    "Before annotating the variants and recoding them for posterior analysis we want to make some filtering to retain only the rare variants.\n",
    "\n",
    "To run:\n",
    "\n",
    "```\n",
    "\n",
    "bfiles=`echo /gpfs/gibbs/pi/dewan/data/UKBiobank/genotype_files/ukb28374_exomedata/exome_data_OCT2020/ukb23155_c{1..22}_b0_v1.bed`\n",
    "\n",
    "\n",
    "sos run ~/project/UKBB_GWAS_dev/workflow/QC_Exome_UKBB.ipynb qc \\\n",
    "    --cwd ~/scratch60 \\\n",
    "    --bfiles $bedfiles\\\n",
    "    --maf_filter 0.001 \\\n",
    "    --geno_filter 0.1 \\\n",
    "    --mind_filter 0.2 \\\n",
    "    --build 'hg38' \\\n",
    "    --container_lmm /gpfs/gibbs/pi/dewan/data/UKBiobank/lmm.sif\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Output file for LMM.ipynb for rare-variants (variant coding 1/0)\n",
    "\n",
    "FIXME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# the output directory for generated files\n",
    "parameter: cwd = path\n",
    "# bed files plink format\n",
    "parameter: bedfiles = paths\n",
    "parameter: bimfiles = paths\n",
    "# Specific number of threads to use\n",
    "parameter: numThreads = 2\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 1\n",
    "# Human genome build\n",
    "parameter: build = 'hg38'\n",
    "# Name for the merged bimfiles\n",
    "parameter: bim_name = path\n",
    "# Prefix for the name based on common/rare variant filtering\n",
    "parameter: name_prefix = str\n",
    "# Filter based on minor allele frequency (use when filtering common variants)\n",
    "parameter: maf_filter = 0.0\n",
    "# Filter based on the maximum maf allowed (use when filtering rare variants)\n",
    "parameter: max_maf_filter = 0.001\n",
    "# Filter out variants with missing call rate higher that this value\n",
    "parameter: geno_filter = 0.0\n",
    "# Filter according to Hardy Weiberg Equilibrium\n",
    "parameter: hwe_filter = 0.0\n",
    "# Fitler out samples with missing rate higher than this value\n",
    "parameter: mind_filter = 0.0\n",
    "# Load annovar module from cluster\n",
    "parameter: annovar_module = '''\n",
    "module load ANNOVAR/2020Jun08-foss-2018b-Perl-5.28.0\n",
    "echo \"Module annovar loaded\"\n",
    "{cmd}\n",
    "'''\n",
    "# Software container option\n",
    "parameter: container_annovar = 'gaow/gatk4-annovar'\n",
    "parameter: container_lmm = 'statisticalgenetics/lmm:1.6'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Format file for plink .bim\n",
    "\n",
    "A text file with no header line, and one line per variant with the following six fields:\n",
    "1. Chromosome code (either an integer, or 'X'/'Y'/'XY'/'MT'; '0' indicates unknown) or name\n",
    "2. Variant identifier\n",
    "3. Position in morgans or centimorgans (safe to use dummy value of '0')\n",
    "4. Base-pair coordinate (1-based; limited to 231-2)\n",
    "5. Allele 1 (corresponding to clear bits in .bed; usually minor)\n",
    "6. Allele 2 (corresponding to set bits in .bed; usually major)\n",
    "\n",
    "In the bim file the second column e.g `1:930232:C:T` contains the alleles in ref/alt mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Merge all the bimfiles into a single file to use later with awk\n",
    "# Only need to run this cell once\n",
    "[bim_merge: provides = bim_name]\n",
    "input: bimfiles\n",
    "output: bim_name\n",
    "task: trunk_workers = 1, walltime = '10h', mem = '10G', cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "bash: expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout' \n",
    "      cat ${_input} > ${_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Get a list of common SNPs above (--maf) or below (--max-maf) certain MAF\n",
    "[get_snps_1]\n",
    "input: bedfiles, group_by=1\n",
    "output: f'{cwd}/cache/{_input:bn}.{name_prefix}.snplist'\n",
    "task: trunk_workers = 1, walltime = '10h', mem = '30G', cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "bash: container=container_lmm, expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout' \n",
    "    plink2 \\\n",
    "      --bfile ${_input:n}\\\n",
    "      ${('--maf %s' % maf_filter) if maf_filter > 0 else ''} \\\n",
    "      ${('--max-maf %s' % max_maf_filter) if max_maf_filter > 0 else ''} \\\n",
    "      ${('--geno %s' % geno_filter) if geno_filter > 0 else ''} \\\n",
    "      ${('--hwe %s' % hwe_filter) if hwe_filter > 0 else ''} \\\n",
    "      ${('--mind %s' % mind_filter) if mind_filter > 0 else ''} \\\n",
    "      --write-snplist --no-id-header\\\n",
    "      --freq \\\n",
    "      --threads ${numThreads} \\\n",
    "      --out ${_output:n} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Merge all of the common_var.snplist into a single file and all the rare_var.snplist into another single file\n",
    "[get_snps_2]\n",
    "input: group_by='all'\n",
    "output: f'{cwd}/{name_prefix}.snplist'\n",
    "task: trunk_workers = 1, walltime = '10h', mem = '10G', cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "bash: expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output:n}.stdout' \n",
    "      cat ${_input} > ${_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Search for common or rare variants in bimfile and generate annovar input file\n",
    "[get_snps_3]\n",
    "depends: bim_name\n",
    "output: f'{_input:n}.avinput'\n",
    "task: trunk_workers = 1, walltime = '10h', mem = '10G', cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "bash: expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout' \n",
    "    awk -F\" \" 'FNR==NR {lines[$1]; next} $2 in lines ' ${_input} ${bim_name} > ${_output:n}.tmp\n",
    "    awk '{if ($2 ~ /D/) {print $1, $4, $4 + (length ($6) - length ($5)), $6, $5 } else {print $1, $4, $4, $6, $5 }}'  ${_output:n}.tmp >  ${_output}\n",
    "    # remove temporary files\n",
    "    rm -f ${_output:n}.tmp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Convert binary plink to vcf for later use with rvtest\n",
    "[convert_to_vcf]\n",
    "input: bedfiles, group_by=1\n",
    "output: f'{cwd}/cache/{_input:bn}.{name_prefix}.vcf.gz'\n",
    "task: trunk_workers = 1, walltime = '10h', mem = '30G', cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "bash: container=container_lmm, expand= \"${ }\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout' \n",
    "    plink2 \\\n",
    "      --bfile ${_input:n} \\\n",
    "      --extract ${_output:nn}.snplist \\\n",
    "      --recode vcf 'id-paste=iid' 'bgz' \\\n",
    "      --threads ${numThreads} \\\n",
    "      --out ${_output:nn} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Annovar details\n",
    "\n",
    "For a list of available [databases](https://hgdownload.soe.ucsc.edu/goldenPath/hg38/database/)\n",
    "\n",
    "On Farnam's Yale HPC there is a folder for shared databases\n",
    "```/gpfs/ysm/datasets/db/annovar/humandb``` \n",
    "\n",
    "and a folder for the x_ref database ```/gpfs/gibbs/pi/dewan/data/UKBiobank/mart_export_2019_LOFtools3.txt```\n",
    "\n",
    "### Format file for annovar input\n",
    "\n",
    "On each line, the first five space- or tab- delimited columns represent \n",
    "\n",
    "1. chromosome \n",
    "2. start position \n",
    "3. end position \n",
    "4. the reference nucleotides\n",
    "5. the observed nucleotides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Create annovar input file\n",
    "[annovar_1]\n",
    "input: bim_name\n",
    "output: f'{cwd}/{_input:bn}.{build}.avinput'\n",
    "task: trunk_workers = 1, walltime = '10h', mem = '10G', cores = numThreads, tags = f'{_output:bn}'\n",
    "bash: expand= \"${ }\", stderr = f'{_output:n}.err', stdout = f'{_output:n}.out' \n",
    "    awk '{if ($2 ~ /D/) {print $1, $4, $4 + (length ($6) - length ($5)), $6, $5, $2} else {print $1, $4, $4, $6, $5, $2}}'  ${_input} >  ${_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Annotate vcf file using ANNOVAR\n",
    "[annovar_2]\n",
    "# humandb path for ANNOVAR\n",
    "parameter: humandb = path\n",
    "parameter: ukbb = path\n",
    "#add xreffile to option without -exonicsplicing\n",
    "#mart_export_2019_LOFtools3.txt #xreffile latest option -> Phenotype description,HGNC symbol,MIM morbid description,CGD_CONDITION,CGD_inh,CGD_man,CGD_comm,LOF_tools\n",
    "parameter: x_ref = path(f\"{ukbb}/mart_export_2019_LOFtools3.txt\")\n",
    "# Annovar protocol\n",
    "parameter: protocol = ['refGene', 'refGeneWithVer', 'knownGene', 'ensGene', 'phastConsElements30way', 'encRegTfbsClustered', 'gwasCatalog', 'gnomad211_genome', 'gnomad211_exome', 'gme', 'kaviar_20150923', 'abraom', 'avsnp150', 'dbnsfp41a', 'dbscsnv11', 'regsnpintron', 'clinvar_20200316', 'gene4denovo201907']\n",
    "# Annovar operation\n",
    "parameter: operation = ['g', 'g', 'g', 'gx', 'r', 'r', 'r', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f']\n",
    "# Annovar args\n",
    "parameter: arg = ['\"-splicing 12 -exonicsplicing\"', '\"-splicing 30\"', '\"-splicing 12 -exonicsplicing\"', '\"-splicing 12\"', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
    "#input: output_from('convert_to_vcf'), group_by=1\n",
    "#input: output_from('get_snps'), group_by=1\n",
    "output: f'{cwd}/{_input:bn}.{build}_multianno.csv'\n",
    "task: trunk_workers = 1, walltime = '60h', mem = '48G', cores = numThreads, tags = f'{_output:bn}'\n",
    "bash: container=container_annovar, volumes=[f'{humandb:a}:{humandb:a}', f'{x_ref:ad}:{x_ref:ad}'], expand=\"${ }\", stderr=f'{_output:n}.err', stdout=f'{_output:n}.out'\n",
    "    #do not add -intronhgvs as option -> writes cDNA variants as HGVS but creates issues (+2 splice site reported only)\n",
    "    #-nastring . can only be . for VCF files\n",
    "    #regsnpintron might cause shifted lines (be carefull using)\n",
    "    table_annovar.pl \\\n",
    "        ${_input} \\\n",
    "        ${humandb} \\\n",
    "        -buildver ${build} \\\n",
    "        -out ${_output:nn}\\\n",
    "        -remove \\\n",
    "        -polish \\\n",
    "        -nastring . \\\n",
    "        -protocol ${\",\".join(protocol)} \\\n",
    "        -operation ${\",\".join(operation)} \\\n",
    "        -arg ${\",\".join(arg)} \\\n",
    "        -csvout \\\n",
    "        -xreffile ${x_ref}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Create file with sets of genes, using tabix\n",
    "[annovar_2]\n",
    "output: f'{cwd}/{_input:bn}.ordered.vcf.gz'\n",
    "task: trunk_workers = 1, walltime = '10h', mem = '30G', cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "bash: expand= \"${ }\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout'\n",
    "\n",
    "    (grep ^\"#\" ${_input}; grep -v ^\"#\" ${_input}| sed 's:^chr::ig' | sort -k1,1n -k2,2n) | bgzip -c > ${_output} \n",
    "    tabix -f -p vcf ${_output}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### FIXME\n",
    "\n",
    "After perfoming the annotation we need to select the sites that are going to be used for rare-variant analysis.\n",
    "\n",
    "Those whose annotation is nonsyn, frameshift, LoF, splicesite variants and provide this as a site-file for rare variant association testing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "calysto_bash",
     "Bash",
     "#E6EEFF",
     "shell"
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.22.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
