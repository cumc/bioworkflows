{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Annotation of exome variants using Annovar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Aim\n",
    "\n",
    "Prepare the data for further association analyses using the LMM.ipynb on rare variants. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Description of the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "This pipeline provides 3 different possibilities depending on the type of input data you are starting with:\n",
    "\n",
    "### Scenario 1 : you have multiple bim files (e.g. one per chromosome) and you want to merge them into one file for later annotation with annovar\n",
    "\n",
    "Run `bim_merge` to concatenate all the bim files and then run `annovar` to annotate all the variants at once\n",
    "\n",
    "### Scenario 2: you either want to work with common or rare variants.\n",
    "\n",
    "Run `get_snps` using the `--maf` or `max-maf` depending on the type of variants you would like to extract and then run `annovar`\n",
    "\n",
    "### Scenario 3: you already have a specific list of variants you would like to annotate stored in a bim file. \n",
    "\n",
    "Run `annovar`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Command interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mERROR\u001b[0m: \u001b[91mNotebook JSON is invalid: %s\u001b[0m\n",
      "usage: sos run annovar.ipynb [workflow_name | -t targets] [options] [workflow_options]\n",
      "  workflow_name:        Single or combined workflows defined in this script\n",
      "  targets:              One or more targets to generate\n",
      "  options:              Single-hyphen sos parameters (see \"sos run -h\" for details)\n",
      "  workflow_options:     Double-hyphen workflow-specific parameters\n",
      "\n",
      "Workflows:\n",
      "  bim_merge\n",
      "  get_snps\n",
      "  annovar\n",
      "\n",
      "Global Workflow Options:\n",
      "  --cwd VAL (as path, required)\n",
      "                        the output directory for generated files\n",
      "  --numThreads 2 (as int)\n",
      "                        Specific number of threads to use\n",
      "  --job-size 1 (as int)\n",
      "                        For cluster jobs, number commands to run per job\n",
      "  --build hg38\n",
      "                        Human genome build\n",
      "  --bim-name VAL (as path, required)\n",
      "                        Name for the merged bimfiles\n",
      "  --name-prefix VAL (as str, required)\n",
      "                        Prefix for the name based on common/rare variant\n",
      "                        filtering\n",
      "  --annovar-module '\\nmodule load ANNOVAR/2020Jun08-foss-2018b-Perl-5.28.0\\necho \"Module annovar loaded\"\\n{cmd}\\n'\n",
      "                        Load annovar module from cluster\n",
      "  --container-annovar 'gaow/gatk4-annovar'\n",
      "                        Software container option\n",
      "  --container-lmm 'statisticalgenetics/lmm:2.0'\n",
      "\n",
      "Sections\n",
      "  bim_merge:            Merge all the bimfiles into a single file to use later\n",
      "                        with awk Only need to run this cell once\n",
      "    Workflow Options:\n",
      "      --bimfiles  paths\n",
      "\n",
      "  get_snps_1:           Get a list of common SNPs above (--maf) or rare SNPs\n",
      "                        below (--max-maf) certain MAF\n",
      "    Workflow Options:\n",
      "      --bfiles  paths\n",
      "\n",
      "                        bed files plink format\n",
      "      --maf-filter 0.0 (as float)\n",
      "                        Filter based on minor allele frequency (use when\n",
      "                        filtering common variants)\n",
      "      --max-maf-filter 0.001 (as float)\n",
      "                        Filter based on the maximum maf allowed (use when\n",
      "                        filtering rare variants)\n",
      "      --geno-filter 0.0 (as float)\n",
      "                        Filter out variants with missing call rate higher that\n",
      "                        this value\n",
      "      --hwe-filter 0.0 (as float)\n",
      "                        Filter according to Hardy Weiberg Equilibrium\n",
      "      --mind-filter 0.0 (as float)\n",
      "                        Fitler out samples with missing rate higher than this\n",
      "                        value\n",
      "  get_snps_2:           Merge all of the common_var.snplist into a single file\n",
      "                        and all the rare_var.snplist into another single file\n",
      "  get_snps_3:           Search for common or rare variants in bimfile and\n",
      "                        generate annovar input file\n",
      "  annovar_1:            Create annovar input file\n",
      "  annovar_2:            Annotate vcf file using ANNOVAR\n",
      "    Workflow Options:\n",
      "      --humandb VAL (as path, required)\n",
      "                        humandb path for ANNOVAR\n",
      "      --ukbb VAL (as path, required)\n",
      "      --x-ref  path(f\"{ukbb}/mart_export_2019_LOFtools3.txt\")\n",
      "\n",
      "                        add xreffile to option without -exonicsplicing\n",
      "                        mart_export_2019_LOFtools3.txt #xreffile latest option\n",
      "                        -> Phenotype description,HGNC symbol,MIM morbid descript\n",
      "                        ion,CGD_CONDITION,CGD_inh,CGD_man,CGD_comm,LOF_tools\n",
      "      --protocol refGene refGeneWithVer knownGene ensGene phastConsElements30way encRegTfbsClustered gwasCatalog gnomad211_genome gnomad211_exome gme kaviar_20150923 abraom avsnp150 dbnsfp41a dbscsnv11 regsnpintron clinvar_20200316 gene4denovo201907 (as list)\n",
      "                        Annovar protocol\n",
      "      --operation g g g gx r r r f f f f f f f f f f f (as list)\n",
      "                        Annovar operation\n",
      "      --arg \"-splicing 12 -exonicsplicing\" \"-splicing 30\" \"-splicing 12 -exonicsplicing\" \"-splicing 12\"               (as list)\n",
      "                        Annovar args\n"
     ]
    }
   ],
   "source": [
    "!sos run annovar.ipynb -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Illustration with minimal working example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "**Scenario 3:** On Yale's cluster, here modify humandb and ukbb paths to match the location of the databases needed by annovar to function\n",
    "\n",
    "```\n",
    "sos run ~/project/bioworkflows/variant-annotation/annovar.ipynb annovar \\\n",
    "    --cwd output \\\n",
    "    --bim_name ukb23156_c22.merged.filtered.bim \\\n",
    "    --humandb /gpfs/ysm/datasets/db/annovar/humandb \\\n",
    "    --ukbb /gpfs/gibbs/pi/dewan/data/UKBiobank \\\n",
    "    --job_size 1 \\\n",
    "    --name_prefix mwe_chr22 \\\n",
    "    --container_annovar /gpfs/gibbs/pi/dewan/data/UKBiobank/annovar.sif\n",
    "```\n",
    "\n",
    "On Columbia's cluster running `annovar`\n",
    "\n",
    "```\n",
    "sos run ~/project/bioworkflows/variant-annotation/annovar.ipynb annovar \\\n",
    "    --cwd output \\\n",
    "    --bim_name /mnt/mfs/statgen/UKBiobank/data/exome_files/project_VCF/plink_files/ukb23156_c22.merged.filtered.bim \\\n",
    "    --humandb /mnt/mfs/statgen/isabelle/REF/humandb  \\\n",
    "    --ukbb /mnt/mfs/statgen/isabelle/REF/humandb \\\n",
    "    --job_size 1 \\\n",
    "    --name_prefix mwe_chr22 \\\n",
    "    --container_annovar /mnt/mfs/statgen/containers/gatk4-annovar.sif\n",
    "```\n",
    "On Columbia's cluster running `burden_files`\n",
    "```\n",
    "sos run ~/project/bioworkflows/variant-annotation/annovar.ipynb burden_files\\\n",
    "    --cwd ~/output \\\n",
    "    --annotated_file /mnt/mfs/statgen/UKBiobank/results/annovar_exome/ukb32285_exomespb_chr1_22.hg38.hg38_multianno.csv\\\n",
    "    --bim_name /mnt/mfs/statgen/UKBiobank/data/exome_files/project_VCF/plink_files/ukb23156_c1.merged.filtered.bim \\\n",
    "    --job_size 1 \\\n",
    "    --name_prefix test \\\n",
    "    --container_lmm /mnt/mfs/statgen/containers/lmm.sif\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# the output directory for generated files\n",
    "parameter: cwd = path\n",
    "# Specific number of threads to use\n",
    "parameter: numThreads = 2\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 1\n",
    "# Name for the merged bimfiles to use\n",
    "parameter: bim_name = path\n",
    "# Human genome build hg19 or hg38\n",
    "parameter: build = 'hg38'\n",
    "# Prefix for the name based on common/rare variant filtering\n",
    "parameter: name_prefix = str\n",
    "# Wall clock time expected\n",
    "parameter: walltime = \"15h\"\n",
    "# Memory expected\n",
    "parameter: mem = \"30G\"\n",
    "# Load annovar module from cluster\n",
    "parameter: annovar_module = '''\n",
    "module load ANNOVAR/2020Jun08-foss-2018b-Perl-5.28.0\n",
    "echo \"Module annovar loaded\"\n",
    "{cmd}\n",
    "'''\n",
    "# Software container option\n",
    "parameter: container_annovar = 'gaow/gatk4-annovar'\n",
    "parameter: container_lmm = 'statisticalgenetics/lmm:2.4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Format file for plink .bim\n",
    "\n",
    "A text file with no header line, and one line per variant with the following six fields:\n",
    "1. Chromosome code (either an integer, or 'X'/'Y'/'XY'/'MT'; '0' indicates unknown) or name\n",
    "2. Variant identifier\n",
    "3. Position in morgans or centimorgans (safe to use dummy value of '0')\n",
    "4. Base-pair coordinate (1-based; limited to 231-2)\n",
    "5. Allele 1 (corresponding to clear bits in .bed; usually minor)\n",
    "6. Allele 2 (corresponding to set bits in .bed; usually major)\n",
    "\n",
    "In the bim file the second column e.g `1:930232:C:T` contains the alleles in ref/alt mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Step to merge *.bim files from plink formatted data (e.g exome data in the UKBB, genotype array data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Merge all the *.bim files into a single file. Needs to be run once per type of data (e.g. genotype, exome)\n",
    "[bim_from_plink]\n",
    "# Path to the *.bim files to merge\n",
    "parameter: bimfiles= paths\n",
    "# Specify path of the merged bim file\n",
    "parameter: bim_name = path\n",
    "input: bimfiles \n",
    "output: bim_name\n",
    "task: trunk_workers = 1, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "bash: expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout' \n",
    "      cat ${_input} >> ${_output}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Step to create a list of variants from *.bgen files and a merged *.bim file to annotate (e.g imputed genotype data UKBB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Create a merged *.bim file from *.bgen files\n",
    "[bim_from_bgen]\n",
    "# Specify bgen files path\n",
    "parameter: genoFile = paths\n",
    "# Specify name of the merged bim file\n",
    "parameter: bim_name = str\n",
    "# The input here is the bgen file from which to extract the list of variants\n",
    "input: genoFile, group_by=1\n",
    "output: f'{cwd}/{_input:bn}.bim'\n",
    "task: trunk_workers = 1, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "bash: container=container_lmm, expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout'\n",
    "    bgenix -g ${_input} -list | awk 'NR>2 { gsub(\"_\",\":\",$1); print $3, $1, $4, $7, $6 }' | awk 'BEGIN{FS=OFS=\" \"}{$2 = $2 OFS 0}1'  > ${_output}\n",
    "    cat ${_output} | awk '{x=$1+0;print x,$2,$3,$4,$5,$6}' >> ${cwd}/${bim_name}.merged.bim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Get a list of common SNPs above (--maf) or rare SNPs below (--max-maf) certain MAF\n",
    "[get_snps_1]\n",
    "# bed files plink format\n",
    "parameter: bfiles = paths\n",
    "# Filter based on minor allele frequency (use when filtering common variants)\n",
    "parameter: maf_filter = 0.0\n",
    "# Filter based on the maximum maf allowed (use when filtering rare variants)\n",
    "parameter: max_maf_filter = 0.001\n",
    "# Filter out variants with missing call rate higher that this value\n",
    "parameter: geno_filter = 0.0\n",
    "# Filter according to Hardy Weiberg Equilibrium\n",
    "parameter: hwe_filter = 0.0\n",
    "# Fitler out samples with missing rate higher than this value\n",
    "parameter: mind_filter = 0.0\n",
    "input: bfiles, group_by=1\n",
    "output: f'{cwd}/cache/{_input:bn}.{name_prefix}.snplist'\n",
    "task: trunk_workers = 1, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "bash: container=container_lmm, expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout' \n",
    "    plink2 \\\n",
    "      --bfile ${_input:n}\\\n",
    "      ${('--maf %s' % maf_filter) if maf_filter > 0 else ''} \\\n",
    "      ${('--max-maf %s' % max_maf_filter) if max_maf_filter > 0 else ''} \\\n",
    "      ${('--geno %s' % geno_filter) if geno_filter > 0 else ''} \\\n",
    "      ${('--hwe %s' % hwe_filter) if hwe_filter > 0 else ''} \\\n",
    "      ${('--mind %s' % mind_filter) if mind_filter > 0 else ''} \\\n",
    "      --write-snplist --no-id-header\\\n",
    "      --freq \\\n",
    "      --threads ${numThreads} \\\n",
    "      --out ${_output:n} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Merge all of the common_var.snplist into a single file and all the rare_var.snplist into another single file\n",
    "[get_snps_2]\n",
    "input: group_by='all'\n",
    "output: f'{cwd}/cache/{name_prefix}.snplist'\n",
    "task: trunk_workers = 1, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "bash: expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output:n}.stdout' \n",
    "      cat ${_input} > ${_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Search for common or rare variants in bimfile and generate annovar input file\n",
    "[get_snps_3]\n",
    "depends: bim_name\n",
    "output: f'{cwd}/{_input:bn}.avinput'\n",
    "task: trunk_workers = 1, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "bash: expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout' \n",
    "    awk -F\" \" 'FNR==NR {lines[$1]; next} $2 in lines ' ${_input} ${bim_name} > ${_output:n}.tmp\n",
    "    awk '{if ($2 ~ /D/) {print $1, $4, $4 + (length ($6) - length ($5)), $6, $5 } else {print $1, $4, $4, $6, $5 }}'  ${_output:n}.tmp >  ${_output}\n",
    "    # remove temporary files\n",
    "    rm -f ${_output:n}.tmp "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Annovar details\n",
    "\n",
    "For a list of available [databases](https://hgdownload.soe.ucsc.edu/goldenPath/hg38/database/)\n",
    "\n",
    "On Farnam's Yale HPC there is a folder for shared databases\n",
    "```/gpfs/ysm/datasets/db/annovar/humandb``` \n",
    "\n",
    "and a folder for the x_ref database ```/gpfs/gibbs/pi/dewan/data/UKBiobank/mart_export_2019_LOFtools3.txt```\n",
    "\n",
    "On Columbia's cluster there folder for shared databases for build hg19 is under Isabelle's folder\n",
    "```/mnt/mfs/statgen/isabelle/REF/humandb```\n",
    "\n",
    "and the x_ref database is under that same folder ```/mnt/mfs/statgen/isabelle/REF/humandb```\n",
    "\n",
    "\n",
    "### Important note\n",
    "\n",
    "Please make sure you are using the correct build for your annotations UKBB exome data for 200K individuals need hg38 build\n",
    "\n",
    "### Format file for annovar input\n",
    "\n",
    "On each line, the first five space- or tab- delimited columns represent \n",
    "\n",
    "1. chromosome \n",
    "2. start position \n",
    "3. end position \n",
    "4. the reference nucleotides\n",
    "5. the observed nucleotides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Create annovar input file\n",
    "[annovar_1]\n",
    "input: bim_name\n",
    "output: f'{cwd}/{_input:bn}.{build}.avinput'\n",
    "task: trunk_workers = 1, walltime = walltime, mem = mem, cores = numThreads, tags = f'{_output:bn}'\n",
    "bash: expand= \"${ }\", stderr = f'{_output:n}.err', stdout = f'{_output:n}.out' \n",
    "    # $6 ref_allele, $5 alt_allele in the bim files \n",
    "    # Output as annovar avinput chr, start, end (has to be calculated depending on allele length), reference, alternative\n",
    "    awk '{if ($6 > $5) {print $1, $4, $4 + (length ($6) - length ($5)), $6, $5, $2} else {print $1, $4, $4, $6, $5, $2}}'   ${_input} >  ${_output}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Annotate variants file using ANNOVAR\n",
    "[annovar_2]\n",
    "# humandb path for ANNOVAR\n",
    "parameter: humandb = path\n",
    "# Path to x-ref file\n",
    "parameter: xref_path = path\n",
    "# Annovar protocol\n",
    "if build == 'hg19':\n",
    "    protocol = ['refGene', 'refGeneWithVer', 'knownGene', 'ensGene', 'phastConsElements46way', 'tfbsConsSites', 'gwasCatalog', 'gnomad211_genome', 'gnomad211_exome', 'gme', 'kaviar_20150923', 'avsnp150', 'dbnsfp42a', 'dbscsnv11', 'clinvar_20210501', 'gene4denovo201907']\n",
    "else:\n",
    "    protocol = ['refGene', 'refGeneWithVer', 'knownGene', 'ensGene', 'phastConsElements30way', 'encRegTfbsClustered', 'gwasCatalog', 'gnomad30_genome', 'gnomad211_exome', 'gme', 'kaviar_20150923', 'avsnp150', 'dbnsfp41a', 'dbscsnv11', 'clinvar_20200316', 'gene4denovo201907']\n",
    "#add xreffile to option without -exonicsplicing\n",
    "#mart_export_2019_LOFtools3.txt #xreffile latest option -> Phenotype description,HGNC symbol,MIM morbid description,CGD_CONDITION,CGD_inh,CGD_man,CGD_comm,LOF_tools\n",
    "parameter: x_ref = path(f\"{xref_path}/mart_export_2021_LOFtools.txt\")\n",
    "# Annovar operation\n",
    "parameter: operation = ['g', 'g', 'g', 'gx', 'r', 'r', 'r', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f']\n",
    "# Annovar args\n",
    "parameter: arg = ['\"-splicing 12 -exonicsplicing\"', '\"-splicing 30\"', '\"-splicing 12 -exonicsplicing\"', '\"-splicing 12\"', '', '', '', '', '', '', '', '', '', '', '', '']\n",
    "output: f'{cwd}/{_input:bn}.{build}_multianno.csv'\n",
    "task: trunk_workers = 1, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}', template = '{cmd}' if executable('annotate_variation.pl').target_exists() else annovar_module\n",
    "bash: container=container_annovar, volumes=[f'{humandb:a}:{humandb:a}', f'{x_ref:ad}:{x_ref:ad}'], expand=\"${ }\", stderr=f'{_output:n}.err', stdout=f'{_output:n}.out'\n",
    "    #do not add -intronhgvs as option -> writes cDNA variants as HGVS but creates issues (+2 splice site reported only)\n",
    "    #-nastring . can only be . for VCF files\n",
    "    #regsnpintron might cause shifted lines (be carefull using)\n",
    "    table_annovar.pl \\\n",
    "        ${_input} \\\n",
    "        ${humandb} \\\n",
    "        -buildver ${build} \\\n",
    "        -out ${_output:nn}\\\n",
    "        -otherinfo\\\n",
    "        -remove \\\n",
    "        -polish \\\n",
    "        -nastring . \\\n",
    "        -protocol ${\",\".join(protocol)}\\\n",
    "        -operation ${\",\".join(operation)} \\\n",
    "        -arg ${\",\".join(arg)} \\\n",
    "        -csvout \\\n",
    "        -xreffile ${x_ref} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Annotate variants file using ANNOVAR\n",
    "[annovar_2]\n",
    "# humandb path for ANNOVAR\n",
    "parameter: humandb = path\n",
    "# Path to x-ref file\n",
    "parameter: xref_path = path\n",
    "# Annovar protocol\n",
    "if build == 'hg19':\n",
    "    protocol = ['refGene', 'refGeneWithVer', 'knownGene', 'ensGene', 'phastConsElements46way', 'gwasCatalog', 'gnomad211_exome', 'avsnp150', 'dbnsfp42a', 'dbscsnv11', 'gene4denovo201907']\n",
    "    operation = ['g', 'g', 'g', 'g', 'r', 'r', 'f', 'f', 'f', 'f', 'f']\n",
    "    arg = ['\"-splicing 12 -exonicsplicing\"', '\"-splicing 30\"', '\"-splicing 12 -exonicsplicing\"', '\"-splicing 12\"', '', '', '', '', '', '', '']\n",
    "else:\n",
    "    protocol = ['refGene', 'refGeneWithVer', 'knownGene', 'ensGene', 'phastConsElements30way', 'encRegTfbsClustered', 'gwasCatalog', 'gnomad30_genome', 'gnomad211_exome', 'gme', 'kaviar_20150923', 'avsnp150', 'dbnsfp41a', 'dbscsnv11', 'clinvar_20200316', 'gene4denovo201907']\n",
    "    operation = ['g', 'g', 'g', 'gx', 'r', 'r', 'r', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f']\n",
    "    arg = ['\"-splicing 12 -exonicsplicing\"', '\"-splicing 30\"', '\"-splicing 12 -exonicsplicing\"', '\"-splicing 12\"', '', '', '', '', '', '', '', '', '', '', '', '']\n",
    "\n",
    "#add xreffile to option without -exonicsplicing\n",
    "#mart_export_2019_LOFtools3.txt #xreffile latest option -> Phenotype description,HGNC symbol,MIM morbid description,CGD_CONDITION,CGD_inh,CGD_man,CGD_comm,LOF_tools\n",
    "parameter: x_ref = path(f\"{xref_path}/mart_export_2021_LOFtools.txt\")\n",
    "output: f'{cwd}/{_input:bn}.{build}_multianno.csv'\n",
    "task: trunk_workers = 1, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}', template = '{cmd}' if executable('annotate_variation.pl').target_exists() else annovar_module\n",
    "bash: container=container_annovar, volumes=[f'{humandb:a}:{humandb:a}', f'{x_ref:ad}:{x_ref:ad}'], expand=\"${ }\", stderr=f'{_output:n}.err', stdout=f'{_output:n}.out'\n",
    "    #do not add -intronhgvs as option -> writes cDNA variants as HGVS but creates issues (+2 splice site reported only)\n",
    "    #-nastring . can only be . for VCF files\n",
    "    #regsnpintron might cause shifted lines (be carefull using)\n",
    "    table_annovar.pl \\\n",
    "        ${_input} \\\n",
    "        ${humandb} \\\n",
    "        -buildver ${build} \\\n",
    "        -out ${_output:nn}\\\n",
    "        -otherinfo\\\n",
    "        -remove \\\n",
    "        -polish \\\n",
    "        -nastring . \\\n",
    "        -protocol ${\",\".join(protocol)}\\\n",
    "        -operation ${\",\".join(operation)} \\\n",
    "        -arg ${\",\".join(arg)} \\\n",
    "        -csvout \\\n",
    "        -xreffile ${x_ref} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Generate files for burden_test regenie from the annotated file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "This workflow's aim is to generate the `--anno_file` and the `--set_list` files needed to run regenie_burden in the LMM.ipynb\n",
    "\n",
    "Required files\n",
    "1. The anno_files: define variant sets and functional annotations that will be used to generate the masks. The format is `chr:start:ref:alt gene_name functional_annot`\n",
    "2. The set-list-files: lists variants within each set/gene to use when building masks. The format is set/gene chr start_pos and a comma separated list of variants included in that gene\n",
    "3. Mask file: this file specifies which annotation categories should be combined into masks\n",
    "\n",
    "Optional files\n",
    "\n",
    "4. Set inclusion/exclusion file: one column with a list of sets/genes to be included/excluded from the set-list-file\n",
    "5. Alternative allele frequency file (AAF): by default the AAF is computed by the sample but you can specify an AAF for each variant using this file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Make the anno_file and set_list_file for regenie_burden analysis\n",
    "[burden_files]\n",
    "parameter: annotated_file = path\n",
    "parameter: rsid = False\n",
    "input: annotated_file\n",
    "output:f'{cwd}/{_input:bn}.anno_file',\n",
    "       f'{cwd}/{_input:bn}.aff_file',     \n",
    "       f'{cwd}/{_input:bn}.set_list_file'\n",
    "task: trunk_workers = 1, walltime = '10h', mem = '20G', cores = numThreads, tags = f'{_output[0]:bn}'\n",
    "python: container=container_lmm, expand=\"${ }\", stderr=f'{_output[0]:n}.err', stdout=f'{_output[0]:n}.out'\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import csv\n",
    "\n",
    "    columns= ['Chr', \"Start\", \"Ref\", \"Alt\", \"Gene.refGene\", \"ExonicFunc.refGene\", \"avsnp150\", \"AF_nfe_exome\" ]\n",
    "    df = pd.read_csv('${_input}', compression='gzip', header=0, dtype='string', usecols=columns, index_col=False)\n",
    "\n",
    "    # Get the refGene definition\n",
    "\n",
    "    # Get only the first gene name present\n",
    "    df1 = df[\"Gene.refGene\"].str.split(\";\", n = 1, expand = True)\n",
    "    df[\"Gene\"]= df1[0]\n",
    "\n",
    "    # Get the variants in the chr, pos, ref, alt format\n",
    "\n",
    "    if ${rsid} == True:\n",
    "        df[\"varID\"] = df.avsnp150\n",
    "        tmp = df[~df[\"avsnp150\"].str.startswith('rs', na=False)]\n",
    "        tmp[\"varID\"] = tmp.Chr.str.cat(others=[tmp.Start, tmp.Ref, tmp.Alt], sep=':')\n",
    "        df[~df[\"avsnp150\"].str.startswith('rs', na=False)] = tmp\n",
    "    else:\n",
    "        df[\"varID\"] = df.Chr.str.cat(others=[df.Start, df.Ref, df.Alt], sep=':')\n",
    "\n",
    "    # Drop duplicated variants\n",
    "    df2=df.drop_duplicates(subset='varID', keep='first')\n",
    "    df2=df2.replace({'AF_nfe_exome':'.'},'0.0')\n",
    "    df2['AF_nfe_freq'] = df2['AF_nfe_exome'].astype(float)\n",
    "    # Find duplicated genes in the file\n",
    "    genes = dict()\n",
    "    def find_chrom(row):\n",
    "        if row[\"Gene\"] not in genes.keys():\n",
    "            genes[row[\"Gene\"]] = set()\n",
    "        genes[row[\"Gene\"]].add(row[\"Chr\"])\n",
    "    df2[[\"Chr\", \"Gene\"]].apply(find_chrom, axis=1)\n",
    "    \n",
    "    # Rename the duplicated genes adding the chromosome number to make them unique\n",
    "    def rename_chrom(row):\n",
    "        if len(genes[row[\"Gene\"]]) > 1:\n",
    "            return f'{row[\"Gene\"]}-{row[\"Chr\"]}'\n",
    "        return row[\"Gene\"]\n",
    "    df2[\"Gene\"] = df2.apply(rename_chrom, axis=1)\n",
    "    \n",
    "    # Match annovar annotations with regenie_burden needs \n",
    "    annotation_mappings = {\"nonsynonymous\":'missense', \"frameshift\":'LoF', \"stopgain\":'LoF', \"stoploss\":'LoF', \"synonymous\":'synonymous'}\n",
    "    def annotation(x):\n",
    "        x = x.strip().split()\n",
    "        for i in x:\n",
    "            if i in annotation_mappings.keys():\n",
    "                return annotation_mappings[i]\n",
    "        return 'other'\n",
    "    df2[\"anno_cat\"] = df2[\"ExonicFunc.refGene\"].apply(annotation)\n",
    "    \n",
    "    # Create the anno_file and add chr in the beggining to match the plink bim files\n",
    "    with open('${_output[0]}', 'w') as output:\n",
    "        for row in df2[[\"varID\", \"Gene\", \"anno_cat\"]].to_numpy():\n",
    "            if ${rsid} == True:\n",
    "                output.write(f'{row[0]} {row[1]} {row[2]}\\n')\n",
    "            else: \n",
    "                output.write(f'chr{row[0]} {row[1]} {row[2]}\\n')\n",
    "    # Create the aaf-file for allele frequencies using gnomAD dataset\n",
    "    with open('${_output[1]}', 'w') as output:\n",
    "        for row in df2[[\"varID\", \"AF_nfe_freq\"]].to_numpy():\n",
    "            if ${rsid} == True:\n",
    "                output.write(f'{row[0]} {row[1]}\\n')\n",
    "            else: \n",
    "                output.write(f'chr{row[0]} {row[1]}\\n')\n",
    "    \n",
    "    # Create the set_list_file grouping variants per gene. Format gene chrom start varIDs\n",
    "    grouped = df2.groupby([\"Gene\"])\n",
    "    with open('${_output[2]}', 'w') as output:\n",
    "        for key, values in grouped:\n",
    "            s = []\n",
    "            start = values[\"Start\"].min()\n",
    "            chrom = values[\"Chr\"].min()\n",
    "            def addToS(x, s):\n",
    "                if ${rsid} == True:\n",
    "                    s.append(str(x[\"varID\"]))\n",
    "                else:\n",
    "                    s.append(str(\"chr\") + str(x[\"varID\"]))                   \n",
    "            fxn = lambda x: addToS(x, s)\n",
    "            values.apply(fxn, axis=1)\n",
    "            output.write(key + \" \" + chrom + \" \" + start + \" \" + \",\".join(s) + \"\\n\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 5,
           "op": "addrange",
           "valuelist": "4"
          },
          {
           "key": 5,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": 0,
         "op": "patch"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "sos",
     "op": "patch"
    }
   ],
   "remote_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 5,
           "op": "addrange",
           "valuelist": "6"
          },
          {
           "key": 5,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": 0,
         "op": "patch"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "sos",
     "op": "patch"
    }
   ]
  },
  "sos": {
   "kernels": [
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.22.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
