{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb3d85c3-7e5a-428e-b8e5-d518c7336b78",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# VEP annotation of rare variants using plugins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fdd192-d859-4f99-8128-4f5b0f59c865",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "This workflow focuses on using VEP offline command line tool to annotate variants. \n",
    "Plugins like CADD, gnomAD and LOTFEE can be used to create custom annotations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6645ec40-4a31-4a2a-868d-e772a37461d5",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Input file\n",
    "\n",
    "The input file should be a vcf file. Please make sure the format is:\n",
    "\n",
    "`CHROM  POS ID REF  ALT QUAL  FILTER  INFO  FORMAT` and that the chromosome column does not contain chr.\n",
    "\n",
    "\n",
    "If you would like to know more about VEP input formats please take a look at their [documentation](http://useast.ensembl.org/info/docs/tools/vep/vep_formats.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c040b74e-1bc6-4e1b-a8e5-0388eb4b8976",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Databases\n",
    "\n",
    "This workflow needs that very big databases are downloaded first. \n",
    "\n",
    "**1. Download CADD databases**\n",
    "\n",
    "Download CADD files to match your genome build, and place in the folder `data/cadd`\n",
    "\n",
    "The databases for hg38 can be found in the CADD website (make sure the index files are also in the same folder)\n",
    "\n",
    "* SNVs (HG38): \n",
    "    - [tsv.gz](https://krishna.gs.washington.edu/download/CADD/v1.6/GRCh38/whole_genome_SNVs_inclAnno.tsv.gz) (313G)\n",
    "    - [tbi](https://krishna.gs.washington.edu/download/CADD/v1.6/GRCh38/whole_genome_SNVs_inclAnno.tsv.gz.tbi) (2.7M)\n",
    "\n",
    "If you would like to annotate the CADD score for indels, then you also need to download the specific database\n",
    "\n",
    "* Indels (HG38)\n",
    "    - [tsv.gz](https://krishna.gs.washington.edu/download/CADD/v1.6/GRCh38/gnomad.genomes.r3.0.indel_inclAnno.tsv.gz) (7.2G)\n",
    "    - [tbi](https://krishna.gs.washington.edu/download/CADD/v1.6/GRCh38/gnomad.genomes.r3.0.indel_inclAnno.tsv.gz.tbi) (2.5M)\n",
    "    \n",
    "**2. VEP setup and databases**\n",
    "\n",
    "In our case we are using a container for VEP configuration. If you are working with Singularity you can do:\n",
    "\n",
    "If you are working on Columbia's cluster please load the latest version of Singularity, otherwise you will gen an error. The container should be using the latest version of the container. However you can pull the image manually with the commands below\n",
    "\n",
    "```\n",
    "module load Singularity/3.11.4\n",
    "singularity pull oras://ghcr.io/cumc/rare_variation_apptainer:latest\n",
    "```\n",
    "\n",
    "* Download cache files for VEP annotation and place in the folder `data/vep`. Run tar xzf to unzip this after downloading. Please be sure to match the vep version with the corresponding cache files\n",
    "\n",
    "    - [Ensembl 110 / GRCh38](https://ftp.ensembl.org/pub/release-110/variation/indexed_vep_cache/homo_sapiens_vep_110_GRCh38.tar.gz) (13G)\n",
    "    - [Ensembl 110 / GRCh37](https://ftp.ensembl.org/pub/release-110/variation/indexed_vep_cache/#:~:text=homo_sapiens_vep_110_GRCh37.tar.gz) (20G)\n",
    "    - You can find other VEP cache versions [here](https://ftp.ensembl.org/pub)\n",
    "\n",
    "For full VEP installation and instructions look at the documentation [here](https://ftp.ensembl.org/pub)\n",
    "\n",
    "**3. Set up Loftee**\n",
    "\n",
    "* Download Loftee files to `data/vep`\n",
    "\n",
    "    - GERP (GRCh38):\n",
    "        * [bw](https://personal.broadinstitute.org/konradk/loftee_data/GRCh38/gerp_conservation_scores.homo_sapiens.GRCh38.bw) (12G)\n",
    "    - Human ancestor (GRCh38):\n",
    "        * [fa.gz](https://personal.broadinstitute.org/konradk/loftee_data/GRCh38/human_ancestor.fa.gz) (844M)\n",
    "        * [fai](https://personal.broadinstitute.org/konradk/loftee_data/GRCh38/human_ancestor.fa.gz.fai)\n",
    "        * [gzi](https://personal.broadinstitute.org/konradk/loftee_data/GRCh38/human_ancestor.fa.gz.gzi)\n",
    "    - PhyloCSV (GRCh38):\n",
    "        * [sql.gz](https://personal.broadinstitute.org/konradk/loftee_data/GRCh38/loftee.sql.gz) (29M) unzip after downloading\n",
    "\n",
    "**4. Download genecode file**\n",
    "\n",
    "* Download the GTF file corresponding to your build, unzip it, and and place in `data/genocode`\n",
    "\n",
    "    - [Gencode v43 / GRCh37](https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_43/GRCh37_mapping/gencode.v43lift37.annotation.gtf.gz) (62M)\n",
    "    - [Gencode v44 / GRCh38](https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_44/gencode.v44.annotation.gtf.gz) (47M)\n",
    "\n",
    "**5. Download gnomAD databases**\n",
    "\n",
    "* If you wish to annotate both gnomAD genome and exome frequencies you will need to download both databases. Please keep in mind that these are very big databases. For more information please visit the [gnomAD website](https://gnomad.broadinstitute.org/downloads) \n",
    "\n",
    "    - [gnomAD_exome_v2.1.1 / GRCh38](https://storage.googleapis.com/gcp-public-data--gnomad/release/2.1.1/liftover_grch38/vcf/exomes/gnomad.exomes.r2.1.1.sites.liftover_grch38.vcf.bgz)(86G)\n",
    "    - [gnomAD_exome_index_v2.1.1 / GRCh38](https://storage.googleapis.com/gcp-public-data--gnomad/release/2.1.1/liftover_grch38/vcf/exomes/gnomad.exomes.r2.1.1.sites.liftover_grch38.vcf.bgz.tbi)(1.3M)\n",
    "    \n",
    "    - [gnomAD_genome_v2.1.1 / GRCh37/hg19 ](https://storage.googleapis.com/gcp-public-data--gnomad/release/2.1.1/vcf/genomes/gnomad.genomes.r2.1.1.sites.vcf.bgz)(461Gb)\n",
    "    - [gnomAD_genome_index_v2.1.1 / GRCh37/hg19 ](https://storage.googleapis.com/gcp-public-data--gnomad/release/2.1.1/vcf/genomes/gnomad.genomes.r2.1.1.sites.vcf.bgz.tbi)(2.73M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58b83bb-87ba-4d09-8f03-a55f3a065db3",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Run the workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa7efb5-859a-438f-9e49-dc12c4f81543",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "```\n",
    "sos run vep.ipynb \\\n",
    "--cwd output \\\n",
    "--vcf data/test.vcf \\\n",
    "--human_ancestor data/vep/human_ancestor.fa.gz \\\n",
    "--conservation_file data/vep/loftee.sql \\\n",
    "--gerp_bigwig data/vep/gerp_conservation_scores.homo_sapiens.GRCh38.bw \\\n",
    "--cadd_snps data/cadd/whole_genome_SNVs_inclAnno.tsv.gz \\\n",
    "--cad_indels data/cadd/gnomad.genomes.r3.0.indel.tsv.gz \\\n",
    "--dir_cache data/vep \\\n",
    "--walltime 30h \\\n",
    "--mem 30G\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb9208b-6a41-489f-bbfa-35d2b041b50e",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Output file\n",
    "\n",
    "The output file will be formatted as VCF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2d3070-b8e1-4c11-938f-8502f5ed2c6f",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Important notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe961914-34fe-4502-b5c0-8ba1bf4704e2",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "* Please be mindful that when you run this code those SNVs and indels that are not in the CADD SNV or indel database won't be annotated and that could impact your downstream analysis.\n",
    "\n",
    "* In this piepline we are using the cache version 103 with the VEP install 110 if this is not what you want please modify the parameters accordingly `cache_version`\n",
    "\n",
    "* If you would like to annovate clinvar database add the `clinvar_db` paramater which in Columbia's cluster is `/mnt/vast/hpc/csg/data_public/clinvar/clinvar_20231028.vcf.gz`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f877092f-7e2b-4a6f-a907-b50062ce1177",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4b16c6-f74a-450a-a5bb-da5b3e33cb6f",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# the output directory for generated files\n",
    "parameter: cwd = path\n",
    "# Specific number of threads to use\n",
    "parameter: numThreads = 2\n",
    "# Input vcf file to annotate\n",
    "parameter: vcf = path\n",
    "# Human ancestor database\n",
    "parameter: human_ancestor = path\n",
    "# Convervation file path\n",
    "parameter: conservation_file = path\n",
    "# GERP bigwig\n",
    "parameter: gerp_bigwig = path\n",
    "# CADD database for SNV's\n",
    "parameter: cadd_snps = path\n",
    "# CADD databse for indels\n",
    "parameter: cadd_indels = path\n",
    "# Clinvar datavase\n",
    "parameter: clinvar_db = path('.')\n",
    "# Cache version to use\n",
    "parameter: cache_version = int\n",
    "# Genome assembly to use\n",
    "parameter: assembly = 'GRCh38'\n",
    "# Cache dir\n",
    "parameter: dir_cache = path\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 1\n",
    "parameter: mem = '15G'\n",
    "parameter: walltime = '10h'\n",
    "# The container \n",
    "parameter: container = \"oras://ghcr.io/cumc/rare_variation_apptainer:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f7f94c-7108-4e6b-9412-3a4e8030f2ca",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[default_1]\n",
    "input: vcf\n",
    "output: f'{cwd}/{_input:bn}.rare.VEP.CADD_gnomAD.tsv', f'{cwd}/{_input:bn}.rare.VEP.CADD_gnomAD.tsv.gz' \n",
    "parameter: vep_window = 10000\n",
    "parameter: most_severe = False\n",
    "parameter: everything = True\n",
    "bash: container=container, entrypoint=\"micromamba run -a '' -n rare_variation\", expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout'\n",
    "  vep \\\n",
    "    --verbose \\\n",
    "    --tab \\\n",
    "    -i ${_input} \\\n",
    "    -o ${_output[0]} \\\n",
    "    --distance ${vep_window} \\\n",
    "    --no_stats \\\n",
    "    --cache_version ${cache_version} \\\n",
    "    --assembly ${assembly} \\\n",
    "    --force_overwrite \\\n",
    "    --offline \\\n",
    "    ${('--most_severe') if most_severe else ''} \\\n",
    "    --dir_cache ${dir_cache} \\\n",
    "    --dir_plugins $VEP_PLUGIN_DIR \\\n",
    "    ${('--everything') if everything else ''} \\\n",
    "    ${('--custom file='+ str(clinvar_db)+',short_name=ClinVar,format=vcf,type=exact,coords=0,fields=CLNSIG%CLNREVSTAT%CLNDN') if clinvar_db else ''} \\\n",
    "    --plugin LoF,human_ancestor_fa:${human_ancestor},loftee_path:$VEP_PLUGIN_DIR,conservation_file:${conservation_file},gerp_bigwig:${gerp_bigwig} \\\n",
    "    --plugin CADD,snv=${cadd_snps},indels=${cadd_indels}\n",
    "    bgzip --keep ${_output[0]}\n",
    "    tabix ${_output[1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728ff640-3949-4d33-bedd-58da204fc0cc",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1df42c-302b-4d1b-be2b-0c5d9072e99a",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[default_2]\n",
    "input: f'{cwd}/{vcf:bn}.rare.VEP.CADD_gnomAD.tsv.gz'\n",
    "output: f'{cwd}/{_input:bn}.formatted_anno.tsv'\n",
    "python: expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout'\n",
    "    pandas as pd\n",
    "    import gzip\n",
    "    # Initialize an empty list to store the comments\n",
    "    comments = []\n",
    "\n",
    "    # Open the gzip-compressed VCF file and read it line by line\n",
    "    with gzip.open(${_input}, 'rt') as file:\n",
    "        for line in file:\n",
    "            if line.startswith('#'):\n",
    "                comments.append(line.strip())\n",
    "\n",
    "    # The last item in the 'comments' list is the header comment\n",
    "    header_comment = comments[-1]\n",
    "    # Extract the header from the comment\n",
    "    header = header_comment.split('#')[1]\n",
    "    # Split the header into column names\n",
    "    columns = header.split('\\t')\n",
    "    df = pd.read_csv(${_input}, compression='gzip',comment='#', sep='\\t', names=columns)\n",
    "    df.to_csv(${_output}, index=False, sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f12c92-5c4c-43d8-ab07-3124b2d59fba",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "SoS",
     "sos",
     "",
     ""
    ]
   ],
   "version": "0.24.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
