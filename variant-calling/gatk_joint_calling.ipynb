{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# WGS GVCF samples joint calling, filtering and annotation\n",
    "\n",
    "Implementing a GATK + ANNOVAR workflow in [SoS](https://github.com/vatlab/SOS), written by Isabelle Schrauwen with software containers built by Gao Wang. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <table class=\"revision_table\">\n",
       "        <tr>\n",
       "        <th>Revision</th>\n",
       "        <th>Author</th>\n",
       "        <th>Date</th>\n",
       "        <th>Message</th>\n",
       "        <tr>\n",
       "        <tr><td><a target=\"_blank\" href=\"git@github.com:cumc/bioworkflows/blob/476245078e440d285c757da84851c64ba089e66c/variant-calling/gatk_joint_calling.ipynb\"><span class=\"revision_id\">4762450<span></a></td>\n",
       "<td>haoyueshuai</td>\n",
       "<td>2020-08-25</td>\n",
       "<td>update submit_csg</td></tr><tr><td><a target=\"_blank\" href=\"git@github.com:cumc/bioworkflows/blob/b7958f99b89b2b506a26d34da3cb99b28caf5b42/variant-calling/gatk_joint_calling.ipynb\"><span class=\"revision_id\">b7958f9<span></a></td>\n",
       "<td>Gao Wang</td>\n",
       "<td>2020-08-25</td>\n",
       "<td>Fix a bash variable bug</td></tr><tr><td><a target=\"_blank\" href=\"git@github.com:cumc/bioworkflows/blob/2986c3cf90347cea74223ff716eeba0ee5533300/variant-calling/gatk_joint_calling.ipynb\"><span class=\"revision_id\">2986c3c<span></a></td>\n",
       "<td>Gao Wang</td>\n",
       "<td>2020-08-25</td>\n",
       "<td>Update documentation</td></tr><tr><td><a target=\"_blank\" href=\"git@github.com:cumc/bioworkflows/blob/c1da8038573460ac4c17111b5c270fdbbac9f7b9/variant-calling/gatk_joint_calling.ipynb\"><span class=\"revision_id\">c1da803<span></a></td>\n",
       "<td>Gao Wang</td>\n",
       "<td>2020-08-25</td>\n",
       "<td>Add job submission template for CSG cluster</td></tr><tr><td><a target=\"_blank\" href=\"git@github.com:cumc/bioworkflows/blob/69e450aeb224d7d827acc5a80f68c27902876da6/variant-calling/gatk_joint_calling.ipynb\"><span class=\"revision_id\">69e450a<span></a></td>\n",
       "<td>Gao Wang</td>\n",
       "<td>2020-08-24</td>\n",
       "<td>Add documentation</td></tr><tr><td><a target=\"_blank\" href=\"git@github.com:cumc/bioworkflows/blob/3b5a1e8ea9a9afceccf7e0f654fd7fb2f2f96991/variant-calling/gatk_joint_calling.ipynb\"><span class=\"revision_id\">3b5a1e8<span></a></td>\n",
       "<td>Gao Wang</td>\n",
       "<td>2020-08-24</td>\n",
       "<td>Fix ANNOVAR step</td></tr><tr><td><a target=\"_blank\" href=\"git@github.com:cumc/bioworkflows/blob/43b3150d35b36a88df55d1e6a959dcb115ca536f/variant-calling/gatk_joint_calling.ipynb\"><span class=\"revision_id\">43b3150<span></a></td>\n",
       "<td>Gao Wang</td>\n",
       "<td>2020-08-23</td>\n",
       "<td>Update joint variant calling pipeline with minimal working example</td></tr><tr><td><a target=\"_blank\" href=\"git@github.com:cumc/bioworkflows/blob/2cacdc2940900db2d114dac5db66e7d8c57456ef/variant-calling/gatk_joint_calling.ipynb\"><span class=\"revision_id\">2cacdc2<span></a></td>\n",
       "<td>Gao Wang</td>\n",
       "<td>2020-08-20</td>\n",
       "<td>Remove the need to mount workdir due to recent changes in SoS</td></tr><tr><td><a target=\"_blank\" href=\"git@github.com:cumc/bioworkflows/blob/afe343c7a569b6bc3c33146f102a27c056f87614/variant-calling/gatk_joint_calling.ipynb\"><span class=\"revision_id\">afe343c<span></a></td>\n",
       "<td>Gao Wang</td>\n",
       "<td>2020-08-20</td>\n",
       "<td>Add variant calling pipeline</td></tr></table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%revisions -s -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This SoS workflow notebook contains four workflows:\n",
    "\n",
    "- `gatk_call`\n",
    "- `gatk_filter`\n",
    "- `annovar`\n",
    "- `submit_csg`\n",
    "\n",
    "The first three workflows are for the analysis and the last one is for submitting jobs on the cluster.\n",
    "\n",
    "All workflow steps are numerically ordered to reflect the execution logic. This is the most straightforward SoS workflow style, the \"process-oriented\" style. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Input data\n",
    "\n",
    "Samples in `GVCF` format, already indexed:\n",
    "\n",
    "```\n",
    "*.gvcf.gz\n",
    "*.gvcf.gz.tbi\n",
    "```\n",
    "\n",
    "To input the list of samples to the workflow, please include all sample file names you would like to analyze, in a text file. For example:\n",
    "\n",
    "```\n",
    "GH.AR.SAD.P1.001.0_X3547_S42_1180478_GVCF.hard-filtered.gvcf.gz\n",
    "GH.AR.SAD.P1.003.0_92455_S43_1189700_GVCF.hard-filtered.gvcf.gz\n",
    "GH.AR.SAD.P1.004.0_92456_S44_1189701_GVCF.hard-filtered.gvcf.gz\n",
    "GH.AR.SAD.P1.005.0_92457_S20_1189702_GVCF.hard-filtered.gvcf.gz\n",
    "...\n",
    "```\n",
    "\n",
    "and save it as, eg, `20200820_sample_manifest.txt`. This text file will be the input file to the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Reference data preparation\n",
    "\n",
    "Human genome reference files are needed for `GATK` joint calling; `ANNOVAR` database references are needed for `ANNOVAR` annotations.\n",
    "\n",
    "- `GATK` reference files include:\n",
    "\n",
    "```\n",
    "*.fa\n",
    "*.fa.fai\n",
    "*.dict\n",
    "```\n",
    "\n",
    "- `ANNOVAR` reference files ship with `ANNOVAR` software, under a folder called `humandb`.\n",
    "\n",
    "This workflow assumes that the required files already exit. This pipeline does not provide steps to download or to generate them automatically, which you could find in the tutorial slides. The pipeline will indeed check the availability of the reference files and quit on error if they are missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Run the workflow\n",
    "\n",
    "The workflow is currently designed to run on a Linux cluster (via `singularity`) although it can also be executed on a Mac computer\n",
    "(via `docker`). In brief, after installing [SoS](https://github.com/vatlab/SOS) (also see section \"Software Configuration\" below), \n",
    "you can choose to run different workflows modules.\n",
    "\n",
    "For example to run the variant calling workflow,\n",
    "\n",
    "```\n",
    "sos run gatk_joint_calling.ipynb call \\\n",
    "    --vcf-prefix /path/to/some_vcf_file_prefix \\\n",
    "    --samples /path/to/list/of/sample_gvcf.txt \\\n",
    "    --samples-dir /path/to/sample_gvcf \\\n",
    "    --ref-genome /path/to/reference_genome.fa \\\n",
    "    ...\n",
    "```\n",
    "\n",
    "to run variant filtering, \n",
    "\n",
    "\n",
    "```\n",
    "sos run gatk_joint_calling.ipynb filter \\\n",
    "    --vcf-prefix /path/to/some_vcf_file_prefix \\\n",
    "    ...\n",
    "```\n",
    "\n",
    "to run annotation,\n",
    "\n",
    "```\n",
    "sos run gatk_joint_calling.ipynb annovar \\\n",
    "    --vcf-prefix /path/to/some_vcf_file_prefix \\\n",
    "    ...\n",
    "```\n",
    "\n",
    "You can put all these 3 commands to one bash file and execute that, so you run all steps one after another.\n",
    "\n",
    "Note that `...` are additional options that fall into two categories:\n",
    "\n",
    "1. Options needed to run the bioinformatics steps (e.g. ref_genome)\n",
    "2. Options needed for SoS to run on different platforms ( e.g. container-option)\n",
    "\n",
    "To view all options,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: sos run gatk_joint_calling.ipynb\n",
      "               [workflow_name | -t targets] [options] [workflow_options]\n",
      "  workflow_name:        Single or combined workflows defined in this script\n",
      "  targets:              One or more targets to generate\n",
      "  options:              Single-hyphen sos parameters (see \"sos run -h\" for details)\n",
      "  workflow_options:     Double-hyphen workflow-specific parameters\n",
      "\n",
      "Workflows:\n",
      "  call\n",
      "  filter\n",
      "  annovar\n",
      "  submit_csg\n",
      "\n",
      "Global Workflow Options:\n",
      "  --vcf-prefix joint_call_output (as path)\n",
      "                        Combined VCF file prefix, including path to the output\n",
      "                        but without vcf.gz extension, eg\n",
      "                        \"/path/to/output_filename\".\n",
      "  --build hg19\n",
      "                        Human genome build\n",
      "  --mem 12 (as int)\n",
      "                        Memory allocated to a job, in terms of Gigabyte\n",
      "  --container-option 'gaow/gatk4-annovar'\n",
      "                        Software container option\n",
      "\n",
      "Sections\n",
      "  call_1:               Combine GVCF files\n",
      "    Workflow Options:\n",
      "      --samples VAL (as path, required)\n",
      "                        A file listing out all sample GVCF you would like to\n",
      "                        analyze. Each line is one sample GVCF name.\n",
      "      --samples-dir . (as path)\n",
      "                        Directory where sample GVCF files locate.\n",
      "      --ref-genome refs/Homo_sapiens.GRCh37.75.dna_sm.primary_assembly.fa (as path)\n",
      "                        Path to reference genome file\n",
      "  call_2:               Joint calling\n",
      "    Workflow Options:\n",
      "      --ref-genome VAL (as path, required)\n",
      "                        Path to reference genome file\n",
      "  filter_1:             Split into SNP and INDEL for separate PASS filters\n",
      "  filter_2:             PASS or filter for indels and SNPs (Note | not\n",
      "                        recommended for filters) Ignore MQRankSum warnings <-\n",
      "                        can only be calculated for het sites (not homs)\n",
      "    Workflow Options:\n",
      "      --snp-filters QD < 2.0, QD2 QUAL < 30.0, QUAL30 SOR > 3.0, SOR3 FS > 60.0, FS60 MQ < 40.0, MQ40 MQRankSum < -12.5, MQRankSum-12.5 ReadPosRankSum < -8.0, ReadPosRankSum-8 (as list)\n",
      "      --indel-filters QD < 2.0, QD2 QUAL < 30.0, QUAL30 FS > 200.0, FS200 ReadPosRankSum < -20.0, ReadPosRankSum-20 (as list)\n",
      "  filter_3:             Merge back SNP and INDEL\n",
      "  filter_4:             remove non-PASS variants if wanted\n",
      "  annovar_1:            Annotate\n",
      "    Workflow Options:\n",
      "      --humandb VAL (as path, required)\n",
      "                        humandb path for ANNOVAR\n",
      "      --x-ref  path(f\"{humandb}/mart_export_2019_LOFtools3.txt\")\n",
      "\n",
      "                        add xreffile to option without -exonicsplicing\n",
      "                        mart_export_2019_LOFtools3.txt #xreffile latest option\n",
      "                        -> Phenotype description,HGNC symbol,MIM morbid descript\n",
      "                        ion,CGD_CONDITION,CGD_inh,CGD_man,CGD_comm,LOF_tools\n",
      "      --protocol refGene refGeneWithVer knownGene ensGene wgEncodeBroadHmmGm12878HMM wgEncodeBroadHmmHmecHMM wgEncodeBroadHmmHepg2HMM wgEncodeBroadHmmH1hescHMM wgEncodeRegDnaseClusteredV3 wgEncodeRegTfbsClusteredV3 genomicSuperDups wgRna targetScanS phastConsElements46way tfbsConsSites gwasCatalog gnomad211_genome gnomad211_exome popfreq_max_20150413 gme kaviar_20150923 abraom avsnp150 dbnsfp35a dbscsnv11 regsnpintron cadd13gt20 clinvar_20200316 mcap13 gene4denovo201907 (as list)\n",
      "                        Annovar protocol\n",
      "      --operation g g g gx r r r r r r r r r r r r f f f f f f f f f f f f f f (as list)\n",
      "                        Annovar operation\n",
      "      --arg \"-splicing 12 -exonicsplicing\" \"-splicing 30\" \"-splicing 12 -exonicsplicing\" \"-splicing 12\"                           (as list)\n",
      "                        Annovar args\n",
      "  annovar_2:            Filter out common variants (from 3 databases) with\n",
      "                        annovar\n",
      "    Workflow Options:\n",
      "      --humandb humandb (as path)\n",
      "                        humandb path for ANNOVAR\n",
      "      --keep 'splic|exonic'\n",
      "                        keep pathogenic: use 'pathogenic|Pathogenic', keep\n",
      "                        splice_exonic: use 'splic|exonic'\n",
      "  submit_csg:           Job submission on CSG cluster\n",
      "    Workflow Options:\n",
      "      --cmd-file VAL (as path, required)\n",
      "                        Path to job file\n",
      "      --time '24:00:00'\n",
      "                        Total run time allocated to the script\n",
      "      --[no-]dryrun (default to False)\n"
     ]
    }
   ],
   "source": [
    "sos run gatk_joint_calling.ipynb -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "Please read these options carefully before you start running the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Minimal working example\n",
    "\n",
    "A minimal example data-set can be found on CSG cluster. The following commands use this data-set, although in practice you should change the paths to point to your own data of interest.\n",
    "\n",
    "Joint calling:\n",
    "\n",
    "```\n",
    "sos run gatk_joint_calling.ipynb call \\\n",
    "    --container-option /mnt/mfs/statgen/containers/gatk4-annovar.sif \\\n",
    "    --vcf-prefix output/minimal_example \\\n",
    "    --samples /mnt/mfs/statgen/data_private/gatk_joint_call_example/20200820_sample_manifest.txt \\\n",
    "    --samples-dir /mnt/mfs/statgen/data_private/gatk_joint_call_example/ \\\n",
    "    --ref-genome /mnt/mfs/statgen/isabelle/REF/refs/Homo_sapiens.GRCh37.75.dna_sm.primary_assembly.fa\n",
    "```\n",
    "\n",
    "Filtering:\n",
    "\n",
    "```\n",
    "sos run gatk_joint_calling.ipynb filter \\\n",
    "    --container-option /mnt/mfs/statgen/containers/gatk4-annovar.sif \\\n",
    "    --vcf-prefix output/minimal_example\n",
    "```\n",
    "\n",
    "Annotating:\n",
    "\n",
    "```\n",
    "sos run gatk_joint_calling.ipynb annovar \\\n",
    "    --container-option /mnt/mfs/statgen/containers/gatk4-annovar.sif \\\n",
    "    --vcf-prefix output/minimal_example.snp_indel.filter.PASS \\\n",
    "    --keep \"splic|exonic\" \\\n",
    "    --humandb /mnt/mfs/statgen/isabelle/REF/humandb \\\n",
    "    --x-ref /mnt/mfs/statgen/isabelle/REF/humandb/mart_export_2019_LOFtools3.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Share the workflow with people\n",
    "\n",
    "Use \n",
    "\n",
    "```\n",
    "sos convert gatk_joint_calling.ipynb gatk_joint_calling.html --template sos-cm-toc\n",
    "```\n",
    "\n",
    "to convert this workflow to an HTML file, then pass it around to others to read it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Software configuration\n",
    "\n",
    "Instructions on SoS and docker installation can be found on [our CSG wiki](http://statgen.us/lab-wiki/orientation/jupyter-setup.html). \n",
    "The instructions works for both Mac and Linux, unless otherwise specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Global parameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# Combined VCF file prefix, including path to the output but without vcf.gz extension, \n",
    "# eg \"/path/to/output_filename\".\n",
    "parameter: vcf_prefix = path('joint_call_output')\n",
    "# Human genome build\n",
    "parameter: build = 'hg19'\n",
    "# Memory allocated to a job, in terms of Gigabyte\n",
    "parameter: mem=12\n",
    "# Software container option\n",
    "parameter: container_option = 'gaow/gatk4-annovar'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Joint variant calling from GVCF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Combine GVCF files\n",
    "[call_1]\n",
    "# A file listing out all sample GVCF you would like to analyze. \n",
    "# Each line is one sample GVCF name.\n",
    "parameter: samples = path\n",
    "# Directory where sample GVCF files locate.\n",
    "parameter: samples_dir = path()\n",
    "# Path to reference genome file\n",
    "parameter: ref_genome = path('refs/Homo_sapiens.GRCh37.75.dna_sm.primary_assembly.fa')\n",
    "#\n",
    "fail_if(not samples.is_file(), msg = 'Need valid sample name list file input via ``--samples`` option!')\n",
    "import os\n",
    "sample_files = [f'{samples_dir}/{os.path.basename(x.strip())}' for x in open(samples).readlines()]\n",
    "for x in sample_files:\n",
    "    fail_if(not path(x).is_file(), msg = f'Cannot find file ``{x}``. Please use ``--samples-dir`` option to specify the directory for sample files.')\n",
    "    fail_if(not x.endswith('gvcf.gz'), msg = f'Input file ``{x}`` does not have ``.gvcf.gz`` extension.')\n",
    "fail_if(len(sample_files) == 0, msg = 'Need at least one input sample file!')\n",
    "fail_if(not ref_genome.is_file(), msg = f'Cannot find reference genome ``{ref_genome}``. Please use ``--ref-genome`` option to specify it.')\n",
    "fail_if(not path(f\"{ref_genome:a}.fai\").is_file(), msg = f'Cannot find reference genome index file ``{ref_genome}.fai``. Please make sure it exists.')\n",
    "fail_if(not path(f\"{ref_genome:an}.dict\").is_file(), msg = f'Cannot find reference genome dict file ``{ref_genome:n}.dict``. Please make sure it exists.')\n",
    "\n",
    "depends: system_resource(mem = f'{mem}G'), ref_genome\n",
    "input: sample_files\n",
    "output: f'{vcf_prefix:a}.combined.vcf.gz'\n",
    "\n",
    "bash: container=container_option, volumes=[f'{ref_genome:ad}:{ref_genome:ad}'], expand=\"${ }\", stderr=f'{_output:n}.err', stdout=f'{_output:n}.out'\n",
    "    ${'&&'.join([\"tabix -p vcf %s\" % x for x in _input if not path(x + '.tbi').is_file()])}\n",
    "    gatk --java-options \"-Xmx${mem}g\" CombineGVCFs \\\n",
    "        -R ${ref_genome} \\\n",
    "        ${' '.join(['--variant %s' % x for x in _input])} \\\n",
    "        -O ${_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Joint calling\n",
    "[call_2]\n",
    "# Path to reference genome file\n",
    "parameter: ref_genome = path\n",
    "output: f'{vcf_prefix:a}.vcf.gz'\n",
    "\n",
    "\n",
    "bash: container=container_option, volumes=[f'{ref_genome:ad}:{ref_genome:ad}'], expand=\"${ }\", stderr=f'{_output:nn}.err', stdout=f'{_output:nn}.out'\n",
    "    gatk --java-options \"-Xmx${mem}g\" GenotypeGVCFs \\\n",
    "        -R ${ref_genome} \\\n",
    "        -V ${_input} \\\n",
    "        -O ${_output}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Variant filtering\n",
    "\n",
    "Since we have two types of variants SNP and Indels, the first two steps of the filter workflow pipeline process the two variant types in parallel, then merge them and do additional filtering wiht steps 3 and 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Split into SNP and INDEL for separate PASS filters\n",
    "[filter_1]\n",
    "variant_type = ['SNP', 'INDEL']\n",
    "input: f'{vcf_prefix:a}.vcf.gz', for_each='variant_type', concurrent = True\n",
    "output: f'{vcf_prefix:a}.{_variant_type.lower()}.vcf.gz'\n",
    "\n",
    "\n",
    "bash: container=container_option, expand=\"${ }\", stderr=f'{_output:nn}.err', stdout=f'{_output:nn}.out'\n",
    "    gatk --java-options '-Xmx${mem}g' SelectVariants \\\n",
    "        -V ${_input} \\\n",
    "        -select-type ${_variant_type} \\\n",
    "        -O ${_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# PASS or filter for indels and SNPs (Note | not recommended for filters)\n",
    "# Ignore MQRankSum warnings <- can only be calculated for het sites (not homs)\n",
    "[filter_2]\n",
    "parameter: snp_filters = ['QD < 2.0, QD2', 'QUAL < 30.0, QUAL30', 'SOR > 3.0, SOR3', 'FS > 60.0, FS60', 'MQ < 40.0, MQ40', 'MQRankSum < -12.5, MQRankSum-12.5', 'ReadPosRankSum < -8.0, ReadPosRankSum-8']\n",
    "parameter: indel_filters = [\"QD < 2.0, QD2\", \"QUAL < 30.0, QUAL30\", \"FS > 200.0, FS200\", \"ReadPosRankSum < -20.0, ReadPosRankSum-20\"]\n",
    "input: paired_with = dict(filter_option=[snp_filters, indel_filters])\n",
    "output: f'{_input:nn}.filter.vcf.gz'\n",
    "\n",
    "\n",
    "bash: container=container_option, expand=\"${ }\", stderr=f'{_output:nn}.err', stdout=f'{_output:nn}.out'\n",
    "    gatk --java-options '-Xmx${mem}g' VariantFiltration \\\n",
    "        -V ${_input} \\\n",
    "        ${\" \".join(['-filter \"%s\" --filter-name \"%s\"' % tuple([y.strip() for y in x.split(',')]) for x in _input.filter_option])} \\\n",
    "        -O ${_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Merge back SNP and INDEL\n",
    "[filter_3]\n",
    "input: group_by = 'all'\n",
    "output: f'{vcf_prefix:a}.snp_indel.filter.vcf.gz'\n",
    "\n",
    "\n",
    "bash: container=container_option, expand=\"${ }\", stderr=f'{_output:nn}.err', stdout=f'{_output:nn}.out'\n",
    "    gatk --java-options '-Xmx${mem}g' MergeVcfs \\\n",
    "     -I ${_input[0]} -I ${_input[1]} -O ${_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# remove non-PASS variants if wanted\n",
    "[filter_4]\n",
    "output: f'{vcf_prefix:a}.snp_indel.filter.PASS.vcf.gz'\n",
    "\n",
    "\n",
    "bash: container=container_option, expand=\"${ }\", stderr=f'{_output:nn}.err', stdout=f'{_output:nn}.out'\n",
    "    gatk --java-options '-Xmx${mem}g' SelectVariants \\\n",
    "        -V ${_input} -O ${_output} \\\n",
    "        --exclude-filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# convert vcf to annovar input format\n",
    "[annovar_1]\n",
    "input: f'{vcf_prefix:a}.vcf.gz'\n",
    "output: f'{_input:nn}.avinput'\n",
    "\n",
    "bash: container=container_option,expand=\"${ }\", stderr=f'{_output[0]:n}.err', stdout=f'{_output[0]:n}.out'\n",
    "\n",
    "    convert2annovar.pl \\\n",
    "        -includeinfo \\\n",
    "        -allsample \\\n",
    "        -withfreq \\\n",
    "        -format vcf4 ${_input} > ${_output[0]} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Annotate \n",
    "[annovar_2]\n",
    "# humandb path for ANNOVAR\n",
    "parameter: humandb = path\n",
    "#add xreffile to option without -exonicsplicing\n",
    "#mart_export_2019_LOFtools3.txt #xreffile latest option -> Phenotype description,HGNC symbol,MIM morbid description,CGD_CONDITION,CGD_inh,CGD_man,CGD_comm,LOF_tools\n",
    "parameter: x_ref = path(f\"{humandb}/mart_export_2019_LOFtools3.txt\")\n",
    "# Annovar protocol\n",
    "parameter: protocol = ['refGene', 'refGeneWithVer', 'knownGene', 'ensGene', 'wgEncodeBroadHmmGm12878HMM', 'wgEncodeBroadHmmHmecHMM', 'wgEncodeBroadHmmHepg2HMM', 'wgEncodeBroadHmmH1hescHMM', 'wgEncodeRegDnaseClusteredV3', 'wgEncodeRegTfbsClusteredV3', 'genomicSuperDups', 'wgRna', 'targetScanS', 'phastConsElements46way', 'tfbsConsSites', 'gwasCatalog', 'gnomad211_genome', 'gnomad211_exome', 'popfreq_max_20150413', 'gme', 'kaviar_20150923', 'abraom', 'avsnp150', 'dbnsfp35a', 'dbscsnv11', 'regsnpintron', 'cadd13gt20', 'clinvar_20200316', 'mcap13', 'gene4denovo201907']\n",
    "# Annovar operation\n",
    "parameter: operation = ['g', 'g', 'g', 'gx', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f']\n",
    "# Annovar args\n",
    "parameter: arg = ['\"-splicing 12 -exonicsplicing\"', '\"-splicing 30\"', '\"-splicing 12 -exonicsplicing\"', '\"-splicing 12\"', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
    "input: f'{vcf_prefix:a}.avinput'\n",
    "output: f'{vcf_prefix:a}.{build}_multianno.csv'\n",
    "\n",
    "bash: container=container_option, volumes=[f'{humandb:a}:{humandb:a}', f'{x_ref:ad}:{x_ref:ad}'], expand=\"${ }\", stderr=f'{vcf_prefix:a}.err', stdout=f'{vcf_prefix:a}.out'\n",
    "    #do not add -intronhgvs as option -> writes cDNA variants as HGVS but creates issues (+2 splice site reported only)\n",
    "    #-nastring . can only be . for VCF files\n",
    "    #regsnpintron might cause shifted lines (be carefull using)\n",
    "    table_annovar.pl \\\n",
    "        ${_input} \\\n",
    "        ${humandb} \\\n",
    "        -buildver ${build} \\\n",
    "        -out ${_output:nn}\\\n",
    "        -remove \\\n",
    "        -polish \\\n",
    "        -nastring . \\\n",
    "        -protocol ${\",\".join(protocol)} \\\n",
    "        -operation ${\",\".join(operation)} \\\n",
    "        -arg ${\",\".join(arg)} \\\n",
    "        -csvout \\\n",
    "        -xreffile ${x_ref}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "The step below provides some annotation filtered results. If you want to run your own annotation you can do it by running `ANNOVAR` from the singularity image directly, for example:\n",
    "\n",
    "```\n",
    "singularity exec  /mnt/mfs/statgen/containers/gatk4-annovar.sif annotate_variation.pl \\\n",
    "    -filter -dbtype gnomad211_exome \\\n",
    "    -build hg19 \\\n",
    "    -score_threshold 0.005 \\\n",
    "    minimal_example.snp_indel.filter.PASS.hg19_multianno.exonic_splic.txt \\\n",
    "    humandb \\\n",
    "    -out minimal_example.snp_indel.filter.PASS.hg19_multianno.exonic_splic\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Filter out common variants (from 3 databases) with annovar \n",
    "[annovar_3]\n",
    "# humandb path for ANNOVAR\n",
    "parameter: humandb = path(\"humandb/\")\n",
    "# keep pathogenic: use 'pathogenic|Pathogenic',\n",
    "# keep splice_exonic: use 'splic|exonic'\n",
    "parameter: keep=\"splic|exonic\"\n",
    "tag = '_'.join(sorted(set(keep.lower().split('|'))))\n",
    "input: f'{vcf_prefix:a}.vcf.gz'\n",
    "output: f'{_input[0]:n}.{tag}.txt', \n",
    "        f'{_input[0]:n}.{tag}.exome_genome.{build}_popfreq_max_20150413_filtered'\n",
    "\n",
    "bash: container=container_option, volumes=[f'{humandb:a}:{humandb:a}'], expand=\"${ }\", stderr=f'{_output[0]:n}.err', stdout=f'{_output[0]:n}.out'\n",
    "    set -e\n",
    "    awk 'FNR == 1 {print} /${keep}/{print}' ${_input[0]}  > ${_output[0]}\n",
    "    \n",
    "    annotate_variation.pl -filter -dbtype gnomad211_exome \\\n",
    "        -build ${build} \\\n",
    "        -score_threshold 0.005 \\\n",
    "        ${_output[0]} \\\n",
    "        ${humandb} \\\n",
    "        -out ${_output[0]:n}\n",
    "    \n",
    "    annotate_variation.pl -filter -dbtype gnomad211_genome \\\n",
    "        -build ${build} \\\n",
    "        -score_threshold 0.005 \\\n",
    "        ${_output[0]:n}.${build}_gnomad211_exome_filtered \\\n",
    "        ${humandb} \\\n",
    "        -out ${_output[0]:n}.exome\n",
    "\n",
    "    annotate_variation.pl -filter -dbtype popfreq_max_20150413 \\\n",
    "        -build ${build} \\\n",
    "        -score_threshold 0.005 \\\n",
    "        ${_output[0]:n}.exome.${build}_gnomad211_genome_filtered \\\n",
    "        ${humandb} \\\n",
    "        -out ${_output[0]:n}.exome_genome\n",
    "    rm ${_output[0]:nn}*_dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Submit jobs to the cluster\n",
    "\n",
    "Suppose we would like to submit these lines of commands to the cluster:\n",
    "\n",
    "```\n",
    "sos run gatk_joint_calling.ipynb call \\\n",
    "    --container-option /mnt/mfs/statgen/containers/gatk4-annovar.sif \\\n",
    "    --vcf-prefix output/minimal_example \\\n",
    "    --samples /mnt/mfs/statgen/data_private/gatk_joint_call_example/20200820_sample_manifest.txt \\\n",
    "    --samples-dir /mnt/mfs/statgen/data_private/gatk_joint_call_example/ \\\n",
    "    --ref-genome /mnt/mfs/statgen/isabelle/REF/refs/Homo_sapiens.GRCh37.75.dna_sm.primary_assembly.fa\n",
    "\n",
    "sos run gatk_joint_calling.ipynb filter \\\n",
    "    --container-option /mnt/mfs/statgen/containers/gatk4-annovar.sif \\\n",
    "    --vcf-prefix output/minimal_example\n",
    "\n",
    "sos run gatk_joint_calling.ipynb annovar \\\n",
    "    --container-option /mnt/mfs/statgen/containers/gatk4-annovar.sif \\\n",
    "    --vcf-prefix output/minimal_example.snp_indel.filter.PASS \\\n",
    "    --keep \"splic|exonic\" \\\n",
    "    --humandb /mnt/mfs/statgen/isabelle/REF/humandb \\\n",
    "    --x-ref /mnt/mfs/statgen/isabelle/REF/humandb/mart_export_2019_LOFtools3.txt\n",
    "    \n",
    "    \n",
    "```\n",
    "\n",
    "First, we save the above lines to a text file, e.g. call it `analysis_commands_20200825.txt`, then use the following workflow steps to allocate resources and submit the jobs.\n",
    "\n",
    "Example to submit a job:\n",
    "\n",
    "```\n",
    "sos run gatk_joint_calling.ipynb submit_csg \\\n",
    "    --cmd_file command_1027.txt \n",
    "    \n",
    "sos run ~/gatk_joint_calling_test.ipynb submit_csg     --cmd_file ~/gatk_joint_calling/command_1027.txt \n",
    "```\n",
    "\n",
    "\n",
    "If you want to run in a dryrun mode, meaning just simply test the process but do not genrate results\n",
    "```\n",
    "sos run gatk_joint_calling.ipynb submit_csg \\\n",
    "    --cmd_file analysis_commands_20200825.txt \\\n",
    "    --dryrun True\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Job submission on CSG cluster\n",
    "[submit_csg]\n",
    "# Path to job file\n",
    "parameter: cmd_file=path\n",
    "# Total run time allocated to the script\n",
    "parameter: time='36:00:00'\n",
    "parameter: dryrun = False\n",
    "input: cmd_file\n",
    "python3: expand = '$[ ]'\n",
    "    tpl = '''\n",
    "    #!/bin/sh\n",
    "    #$ -l h_rt=$[time]\n",
    "    #$ -l h_vmem=$[mem+6]G\n",
    "    #$ -N gatk_joint_call\n",
    "    #$ -cwd\n",
    "    #$ -j y\n",
    "    #$ -S /bin/bash\n",
    "    module load Singularity\n",
    "    export PATH=$HOME/miniconda3/bin:$PATH\n",
    "    set -e\n",
    "    '''\n",
    "    script = tpl.lstrip() + ''.join(open($[_input:r]).readlines())\n",
    "    exe = 'cat' if $[dryrun] else 'qsub'\n",
    "    from subprocess import Popen, PIPE\n",
    "    import sys\n",
    "    p = Popen(exe, shell = False, stdin = PIPE, stdout = PIPE, stderr = PIPE, close_fds = True)\n",
    "    for item in p.communicate(script.encode(sys.getdefaultencoding())):\n",
    "        output = item.decode(sys.getdefaultencoding()).rstrip()\n",
    "        if output:\n",
    "            print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "default_kernel": "SoS",
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF",
     ""
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "panel": {
    "displayed": true,
    "height": 0,
    "style": "side"
   },
   "version": "0.22.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
