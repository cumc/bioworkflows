{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# A multivariate EBNM approach for mixture multivariate distribution estimate\n",
    "\n",
    "An earlier version of the approach is outlined in Urbut et al 2019. This workflow implements a few improvements including using additional EBMF methods as well as the new `udr` package to fit the mixture model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Overview of approach\n",
    "\n",
    "1. A workflow step is provided to merge PLINK univariate association analysis results to RDS files for extracting effect estimate samples\n",
    "2. Estimated effects are analyzed by FLASH and PCA to extract patterns of sharing\n",
    "3. Estimate the weights for patterns extracted from previous step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Minimal working example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "To see the input requirements and output data formats, please [download a minimal working example here](https://drive.google.com/file/d/1838xUOQuWTszQ0WJGXNiJMszY05cw3RS/view?usp=sharing), and run the following:\n",
    "\n",
    "### Merge univariate results\n",
    "\n",
    "```\n",
    "sos run mixture_prior.ipynb merge \\\n",
    "    --analysis-units <FIXME> \\\n",
    "    --plink-sumstats <FIXME> \\\n",
    "    --name gtex_mixture\n",
    "```\n",
    "\n",
    "### Select and merge univariate effects\n",
    "\n",
    "```\n",
    "m=/path/to/data\n",
    "cd $m && ls *.rds | sed 's/\\.rds//g' > analysis_units.txt && cd -\n",
    "sos run mixture_prior.ipynb extract_effects \\\n",
    "        --analysis-units $m/analysis_units.txt \\\n",
    "        --datadir $m --name `basename $m`\n",
    "```\n",
    "\n",
    "### Perform mixture model fitting\n",
    "\n",
    "```\n",
    "sos run mixture_prior.ipynb ud \\\n",
    "    --datadir $m --name `basename $m` &> ed_$m.log\n",
    "sos run mixture_prior.ipynb ud --ud-method ted \\\n",
    "    --datadir $m --name `basename $m` &> ted_$m.log\n",
    "sos run mixture_prior.ipynb ed \\\n",
    "    --datadir $m --name `basename $m` &> bovy_$m.log\n",
    "```\n",
    "\n",
    "### Plot results\n",
    "\n",
    "```\n",
    "sos run mixture_prior.ipynb plot_U --model-data /path/to/mixture_model.rds --cwd output\n",
    "```\n",
    "\n",
    "Notice that for production use, each `sos run` command should be submitted to the cluster as a job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "import os\n",
    "# Work directory & output directory\n",
    "parameter: cwd = path('./output')\n",
    "# The filename prefix for output data\n",
    "parameter: name = str\n",
    "parameter: mixture_components = ['flash', 'flash_nonneg', 'pca', 'canonical']\n",
    "parameter: job_size = 1# Residual correlatoin file\n",
    "parameter: resid_cor = path(\".\")\n",
    "fail_if(not (resid_cor.is_file() or resid_cor == path('.')), msg = f'Cannot find ``{resid_cor}``')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Merge PLINK univariate association summary statistic to RDS format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[merge]\n",
    "parameter: molecular_pheno = path\n",
    "# Analysis units file. For RDS files it can be generated by `ls *.rds | sed 's/\\.rds//g' > analysis_units.txt`\n",
    "parameter: analysis_units = path\n",
    "regions = [x.strip().split() for x in open(analysis_units).readlines() if x.strip() and not x.strip().startswith('#')]\n",
    "input:  molecular_pheno, for_each = \"regions\"\n",
    "output: f'{cwd:a}/RDS/{_regions[0]}'\n",
    "\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '4h',  mem = '6G', tags = f'{step_name}_{_output:bn}'  \n",
    "\n",
    "R: expand = \"$[ ]\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout'\n",
    "    library(\"dplyr\")\n",
    "    library(\"tibble\")\n",
    "    library(\"purrr\")\n",
    "    library(\"readr\")\n",
    "    molecular_pheno = read_delim($[molecular_pheno:r], delim = \"\\t\")\n",
    "    molecular_pheno = molecular_pheno%>%mutate(dir = map_chr(`#molc_pheno`,~paste(c(`.x`,\"$[_regions[0]]\"),collapse = \"\")))\n",
    "    n = nrow(molecular_pheno)\n",
    "    # For every condition read rds and extract the bhat and sbhat.\n",
    "    genos = tibble( i = 1:n)\n",
    "    genos = genos%>%mutate(bhat = map(i, ~readRDS(molecular_pheno[[.x,2]])$bhat%>%as.data.frame%>%rownames_to_column),\n",
    "                           sbhat = map(i, ~readRDS(molecular_pheno[[.x,2]])$sbhat%>%as.data.frame%>%rownames_to_column))\n",
    "                      \n",
    "    # Join first two conditions\n",
    "    genos_join_bhat = full_join((genos%>%pull(bhat))[[1]],(genos%>%pull(bhat))[[2]],by = \"rowname\")\n",
    "    genos_join_sbhat = full_join((genos%>%pull(sbhat))[[1]],(genos%>%pull(sbhat))[[2]],by = \"rowname\")\n",
    "    \n",
    "    # If there are more conditions, join the rest\n",
    "    if(n > 2){\n",
    "        for(j in 3:n){\n",
    "            genos_join_bhat = full_join(genos_join_bhat,(genos%>%pull(bhat))[[j]],by = \"rowname\")%>%select(-rowname)%>%as.matrix\n",
    "            genos_join_sbhat = full_join(genos_join_sbhat,(genos%>%pull(sbhat))[[j]],by = \"rowname\")%>%select(-rowname)%>%as.matrix\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    name = molecular_pheno%>%mutate(name = map(`#molc_pheno`, ~read.table(text = .x,sep = \"/\")),\n",
    "                                    name = map_chr(name, ~.x[,ncol(.x)-2]%>%as.character) )%>%pull(name)\n",
    "    colnames(genos_join_bhat) = name\n",
    "    colnames(genos_join_sbhat) = name\n",
    "    \n",
    "    \n",
    "    # save the rds file\n",
    "    saveRDS(file = \"$[_output]\", list(bhat=genos_join_bhat, sbhat=genos_join_sbhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Get top, random and null effects per analysis unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# extract data for MASH from summary stats\n",
    "[extract_effects_1]\n",
    "parameter: table_name = \"\"\n",
    "parameter: bhat = \"bhat\"\n",
    "parameter: sbhat = \"sbhat\"\n",
    "parameter: expected_ncondition = 0\n",
    "parameter: datadir = path\n",
    "parameter: seed = 999\n",
    "parameter: n_random = 4\n",
    "parameter: n_null = 4\n",
    "parameter: z_only = True\n",
    "# Analysis units file. For RDS files it can be generated by `ls *.rds | sed 's/\\.rds//g' > analysis_units.txt`\n",
    "parameter: analysis_units = path\n",
    "# handle N = per_chunk data-set in one job\n",
    "parameter: per_chunk = 1000\n",
    "regions = [x.strip().split() for x in open(analysis_units).readlines() if x.strip() and not x.strip().startswith('#')]\n",
    "input: [f'{datadir}/{x[0]}.rds' for x in regions], group_by = per_chunk\n",
    "output: f\"{cwd}/{name}/cache/{name}_{_index+1}.rds\"\n",
    "task: trunk_workers = 1, walltime = '1h', trunk_size = 1, mem = '4G', cores = 1, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\"\n",
    "    set.seed(${seed})\n",
    "    matxMax <- function(mtx) {\n",
    "      return(arrayInd(which.max(mtx), dim(mtx)))\n",
    "    }\n",
    "    remove_rownames = function(x) {\n",
    "        for (name in names(x)) rownames(x[[name]]) = NULL\n",
    "        return(x)\n",
    "    }\n",
    "    handle_nan_etc = function(x) {\n",
    "      x$bhat[which(is.nan(x$bhat))] = 0\n",
    "      x$sbhat[which(is.nan(x$sbhat) | is.infinite(x$sbhat))] = 1E3\n",
    "      return(x)\n",
    "    }\n",
    "    extract_one_data = function(dat, n_random, n_null, infile) {\n",
    "        if (is.null(dat)) return(NULL)\n",
    "        z = abs(dat$${bhat}/dat$${sbhat})\n",
    "        max_idx = matxMax(z)\n",
    "        # strong effect samples\n",
    "        strong = list(bhat = dat$${bhat}[max_idx[1],,drop=F], sbhat = dat$${sbhat}[max_idx[1],,drop=F])\n",
    "        # random samples excluding the top one\n",
    "        if (max_idx[1] == 1) {\n",
    "            sample_idx = 2:nrow(z)\n",
    "        } else if (max_idx[1] == nrow(z)) {\n",
    "            sample_idx = 1:(max_idx[1]-1)\n",
    "        } else {\n",
    "            sample_idx = c(1:(max_idx[1]-1), (max_idx[1]+1):nrow(z))\n",
    "        }\n",
    "        random_idx = sample(sample_idx, min(n_random, length(sample_idx)), replace = F)\n",
    "        random = list(bhat = dat$${bhat}[random_idx,,drop=F], sbhat = dat$${sbhat}[random_idx,,drop=F])\n",
    "        # null samples defined as |z| < 2\n",
    "        null.id = which(apply(abs(z), 1, max) < 2)\n",
    "        if (length(null.id) == 0) {\n",
    "          warning(paste(\"Null data is empty for input file\", infile))\n",
    "          null = list()\n",
    "        } else {\n",
    "          null_idx = sample(null.id, min(n_null, length(null.id)), replace = F)\n",
    "          null = list(bhat = dat$${bhat}[null_idx,,drop=F], sbhat = dat$${sbhat}[null_idx,,drop=F])\n",
    "        }\n",
    "        dat = (list(random = remove_rownames(random), null = remove_rownames(null), strong = remove_rownames(strong)))\n",
    "        dat$random = handle_nan_etc(dat$random)\n",
    "        dat$null = handle_nan_etc(dat$null)\n",
    "        dat$strong = handle_nan_etc(dat$strong)\n",
    "        return(dat)\n",
    "    }\n",
    "    reformat_data = function(dat, z_only = TRUE) {\n",
    "        # make output consistent in format with \n",
    "        # https://github.com/stephenslab/gtexresults/blob/master/workflows/mashr_flashr_workflow.ipynb      \n",
    "        res = list(random.z = dat$random$bhat/dat$random$sbhat, \n",
    "                  strong.z = dat$strong$bhat/dat$strong$sbhat,  \n",
    "                  null.z = dat$null$bhat/dat$null$sbhat)\n",
    "        if (!z_only) {\n",
    "          res = c(res, list(random.b = dat$random$bhat,\n",
    "           strong.b = dat$strong$bhat,\n",
    "           null.b = dat$null$bhat,\n",
    "           null.s = dat$null$sbhat,\n",
    "           random.s = dat$random$sbhat,\n",
    "           strong.s = dat$strong$sbhat))\n",
    "      }\n",
    "      return(res)\n",
    "    }\n",
    "    merge_data = function(res, one_data) {\n",
    "      if (length(res) == 0) {\n",
    "          return(one_data)\n",
    "      } else if (is.null(one_data)) {\n",
    "          return(res)\n",
    "      } else {\n",
    "          for (d in names(one_data)) {\n",
    "            if (is.null(one_data[[d]])) {\n",
    "              next\n",
    "            } else {\n",
    "                res[[d]] = rbind(res[[d]], one_data[[d]])\n",
    "            }\n",
    "          }\n",
    "          return(res)\n",
    "      }\n",
    "    }\n",
    "    res = list()\n",
    "    for (f in c(${_input:r,})) {\n",
    "      # If cannot read the input for some reason then we just skip it, assuming we have other enough data-sets to use.\n",
    "      dat = tryCatch(readRDS(f), error = function(e) return(NULL))${(\"$\"+table_name) if table_name != \"\" else \"\"}\n",
    "      if (is.null(dat)) {\n",
    "          message(paste(\"Skip loading file\", f, \"due to load failure.\"))\n",
    "          next\n",
    "      }\n",
    "      if (${expected_ncondition} > 0 && (ncol(dat$${bhat}) != ${expected_ncondition} || ncol(dat$${sbhat}) != ${expected_ncondition})) {\n",
    "          message(paste(\"Skip loading file\", f, \"because it has\", ncol(dat$${bhat}), \"columns different from required\", ${expected_ncondition}))\n",
    "          next\n",
    "      }\n",
    "      res = merge_data(res, reformat_data(extract_one_data(dat, ${n_random}, ${n_null}, f), ${\"TRUE\" if z_only else \"FALSE\"}))\n",
    "    }\n",
    "    saveRDS(res, ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[extract_effects_2]\n",
    "input: group_by = \"all\"\n",
    "output: f\"{cwd}/{name}.rds\"\n",
    "task: trunk_workers = 1, walltime = '1h', trunk_size = 1, mem = '16G', cores = 1, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\"\n",
    "    merge_data = function(res, one_data) {\n",
    "      if (length(res) == 0) {\n",
    "          return(one_data)\n",
    "      } else {\n",
    "          for (d in names(one_data)) {\n",
    "            res[[d]] = rbind(res[[d]], one_data[[d]])\n",
    "          }\n",
    "          return(res)\n",
    "      }\n",
    "    }\n",
    "    dat = list()\n",
    "    for (f in c(${_input:r,})) {\n",
    "      dat = merge_data(dat, readRDS(f))\n",
    "    }\n",
    "    # compute empirical covariance XtX\n",
    "    X = dat$strong.z\n",
    "    X[is.na(X)] = 0\n",
    "    dat$XtX = t(as.matrix(X)) %*% as.matrix(X) / nrow(X)\n",
    "    saveRDS(dat, ${_output:r})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Factor analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[flash]\n",
    "input: f\"{cwd}/{name}.rds\"\n",
    "output: f\"{cwd}/{name}.flash.rds\"\n",
    "task: trunk_workers = 1, walltime = '6h', trunk_size = 1, mem = '8G', cores = 2, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout'\n",
    "    library(\"mashr\")\n",
    "    bhat = readRDS(${_input:r})$strong.z\n",
    "    sbhat = bhat\n",
    "    sbhat[!is.na(sbhat)] = 1\n",
    "    dat = mashr::mash_set_data(bhat,sbhat)\n",
    "    res = mashr::cov_flash(dat, factors=\"default\", remove_singleton=${\"TRUE\" if \"canonical\" in mixture_components else \"FALSE\"}, output_model=\"${_output:n}.model.rds\")\n",
    "    saveRDS(res, ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[flash_nonneg]\n",
    "input: f\"{cwd}/{name}.rds\"\n",
    "output: f\"{cwd}/{name}.flash_nonneg.rds\"\n",
    "task: trunk_workers = 1, walltime = '6h', trunk_size = 1, mem = '8G', cores = 2, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout'\n",
    "    library(\"mashr\")\n",
    "    bhat = readRDS(${_input:r})$strong.z\n",
    "    sbhat = bhat\n",
    "    sbhat[!is.na(sbhat)] = 1\n",
    "    dat = mashr::mash_set_data(bhat,sbhat)\n",
    "    res = mashr::cov_flash(dat, factors=\"nonneg\", remove_singleton=${\"TRUE\" if \"canonical\" in mixture_components else \"FALSE\"}, output_model=\"${_output:n}.model.rds\")\n",
    "    saveRDS(res, ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[pca]\n",
    "parameter: npc = 3\n",
    "input: f\"{cwd}/{name}.rds\"\n",
    "output: f\"{cwd}/{name}.pca.rds\"\n",
    "task: trunk_workers = 1, walltime = '2h', trunk_size = 1, mem = '8G', cores = 2, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout'\n",
    "    library(\"mashr\")\n",
    "    bhat = readRDS(${_input:r})$strong.z\n",
    "    sbhat = bhat\n",
    "    sbhat[!is.na(sbhat)] = 1\n",
    "    dat = mashr::mash_set_data(bhat,sbhat)\n",
    "    res = mashr::cov_pca(dat, ${npc})\n",
    "    saveRDS(res, ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[canonical]\n",
    "input: f\"{cwd}/{name}.rds\"\n",
    "output: f\"{cwd}/{name}.canonical.rds\"\n",
    "task: trunk_workers = 1, walltime = '1h', trunk_size = 1, mem = '8G', cores = 1, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout'\n",
    "    library(\"mashr\")\n",
    "    bhat = readRDS(${_input:r})$strong.z\n",
    "    sbhat = bhat\n",
    "    sbhat[!is.na(sbhat)] = 1\n",
    "    dat = mashr::mash_set_data(bhat,sbhat)\n",
    "    res = mashr::cov_canonical(dat)\n",
    "    saveRDS(res, ${_output:r})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Fit mixture model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Installed commit d6d4c0e\n",
    "[ud]\n",
    "# Method is `ed` or `ted`\n",
    "parameter: ud_method = \"ed\"\n",
    "# A list of models where we only update the scales and not the matrices\n",
    "# A typical choice is to estimate scales only for canonical components\n",
    "parameter: scale_only = []\n",
    "# Tolerance for change in likelihood\n",
    "parameter: ud_tol_lik = 1e-3\n",
    "input: [f\"{cwd}/{name}.rds\"] + [f\"{cwd}/{name}.{m}.rds\" for m in mixture_components]\n",
    "output: f'{cwd}/{name}.{ud_method}{\"_unconstrained\" if len(scale_only) == 0 else \"\"}{(\".\" + os.path.basename(resid_cor)[:-4]) if resid_cor.is_file() else \"\"}.rds'\n",
    "task: trunk_workers = 1, walltime = '36h', trunk_size = 1, mem = '10G', cores = 4, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", stderr = f\"{_output:n}.stderr\", stdout = f\"{_output:n}.stdout\"\n",
    "    library(stringr)\n",
    "    rds_files = c(${_input:r,})\n",
    "    dat = readRDS(rds_files[1])\n",
    "    U = list(XtX = dat$XtX)\n",
    "    U_scaled = list()\n",
    "    mixture_components =  c(${paths(mixture_components):r,})\n",
    "    scale_only =  c(${paths(scale_only):r,})\n",
    "    scale_idx = which(mixture_components %in% scale_only )\n",
    "    for (f in 2:length(rds_files) ) {\n",
    "        if ((f - 1) %in% scale_idx ) {\n",
    "          U_scaled = c(U_scaled, readRDS(rds_files[f]))\n",
    "        } else {\n",
    "          U = c(U, readRDS(rds_files[f]))\n",
    "        }\n",
    "    }\n",
    "    #\n",
    "    if (${\"TRUE\" if resid_cor.is_file() else \"FALSE\"}) {\n",
    "      V = readRDS(${resid_cor:r})\n",
    "    } else {\n",
    "      V = cor(dat$null.z)\n",
    "    }\n",
    "    # Fit mixture model using udr package\n",
    "    library(udr)\n",
    "    message(paste(\"Running ${ud_method.upper()} via udr package for\", length(U), \"mixture components\"))\n",
    "    f0 = ud_init(X = as.matrix(dat$strong.z), V = V, U_scaled = U_scaled, U_unconstrained = U, n_rank1=0)\n",
    "    res = ud_fit(f0, X = na.omit(f0$X), control = list(unconstrained.update = \"${ud_method}\", resid.update = 'none', scaled.update = \"fa\", maxiter=5000, tol.lik = ${ud_tol_lik}), verbose=TRUE)\n",
    "    saveRDS(list(U=res$U, w=res$w, loglik=res$loglik), ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[ed]\n",
    "parameter: ed_tol = 1e-6\n",
    "input: [f\"{cwd}/{name}.rds\"] + [f\"{cwd}/{name}.{m}.rds\" for m in mixture_components]\n",
    "output: f'{cwd}/{name}.ed_bovy{(\".\" + os.path.basename(resid_cor)[:-4]) if resid_cor.is_file() else \"\"}.rds'\n",
    "task: trunk_workers = 1, walltime = '36h', trunk_size = 1, mem = '10G', cores = 4, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", stderr = f\"{_output:n}.stderr\", stdout = f\"{_output:n}.stdout\"\n",
    "    rds_files = c(${_input:r,})\n",
    "    dat = readRDS(rds_files[1])\n",
    "    U = list(XtX = dat$XtX)\n",
    "    for (f in rds_files[2:length(rds_files)]) U = c(U, readRDS(f))\n",
    "    if (${\"TRUE\" if resid_cor.is_file() else \"FALSE\"}) {\n",
    "      V = readRDS(${resid_cor:r})\n",
    "    } else {\n",
    "      V = cor(dat$null.z)\n",
    "    }\n",
    "    # Fit mixture model using ED code by J. Bovy\n",
    "    mash_data = mashr::mash_set_data(dat$strong.z, V=V)\n",
    "    message(paste(\"Running ED via J. Bovy's code for\", length(U), \"mixture components\"))\n",
    "    res = mashr:::bovy_wrapper(mash_data, U, logfile=${_output:nr}, tol = ${ed_tol})\n",
    "    saveRDS(list(U=res$Ulist, w=res$pi, loglik=scan(\"${_output:n}_loglike.log\")), ${_output:r})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Plot patterns of sharing\n",
    "\n",
    "This is a simple utility function that takes the output from the pipeline above and make some heatmap to show major patterns of multivariate effects. The plots will be ordered by their mixture weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[plot_U]\n",
    "parameter: model_data = path\n",
    "# number of components to show\n",
    "parameter: max_comp = -1\n",
    "# whether or not to convert to correlation\n",
    "parameter: to_cor = False\n",
    "parameter: tol = \"1E-6\"\n",
    "parameter: remove_label = False\n",
    "parameter: name = \"\"\n",
    "input: model_data\n",
    "output: f'{cwd:a}/{_input:bn}{(\"_\" + name.replace(\"$\", \"_\")) if name != \"\" else \"\"}.pdf'\n",
    "R: expand = \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout'\n",
    "    library(reshape2)\n",
    "    library(ggplot2)\n",
    "    plot_sharing = function(X, col = 'black', to_cor=FALSE, title=\"\", remove_names=F) {\n",
    "        clrs <- colorRampPalette(rev(c(\"#D73027\",\"#FC8D59\",\"#FEE090\",\"#FFFFBF\",\n",
    "            \"#E0F3F8\",\"#91BFDB\",\"#4575B4\")))(128)\n",
    "        if (to_cor) lat <- cov2cor(X)\n",
    "        else lat = X/max(diag(X))\n",
    "        lat[lower.tri(lat)] <- NA\n",
    "        n <- nrow(lat)\n",
    "        if (remove_names) {\n",
    "            colnames(lat) = paste('t',1:n, sep = '')\n",
    "            rownames(lat) = paste('t',1:n, sep = '')\n",
    "        }\n",
    "        melted_cormat <- melt(lat[n:1,], na.rm = TRUE)\n",
    "        p = ggplot(data = melted_cormat, aes(Var2, Var1, fill = value))+\n",
    "            geom_tile(color = \"white\")+ggtitle(title) + \n",
    "            scale_fill_gradientn(colors = clrs, limit = c(-1,1), space = \"Lab\") +\n",
    "            theme_minimal()+ \n",
    "            coord_fixed() +\n",
    "            theme(axis.title.x = element_blank(),\n",
    "                  axis.title.y = element_blank(),\n",
    "                  axis.text.x = element_text(color=col, size=8,angle=45,hjust=1),\n",
    "                  axis.text.y = element_text(color=rev(col), size=8),\n",
    "                  title =element_text(size=10),\n",
    "                  # panel.grid.major = element_blank(),\n",
    "                  panel.border = element_blank(),\n",
    "                  panel.background = element_blank(),\n",
    "                  axis.ticks = element_blank(),\n",
    "                  legend.justification = c(1, 0),\n",
    "                  legend.position = c(0.6, 0),\n",
    "                  legend.direction = \"horizontal\")+\n",
    "            guides(fill = guide_colorbar(title=\"\", barwidth = 7, barheight = 1,\n",
    "                   title.position = \"top\", title.hjust = 0.5))\n",
    "        if(remove_names){\n",
    "            p = p + scale_x_discrete(labels= 1:n) + scale_y_discrete(labels= n:1)\n",
    "        }\n",
    "        return(p)\n",
    "    }\n",
    "  \n",
    "    dat = readRDS(${_input:r})\n",
    "    name = \"${name}\"\n",
    "    if (name != \"\") {\n",
    "      if (is.null(dat[[name]])) stop(\"Cannot find data ${name} in ${_input}\")\n",
    "        dat = dat[[name]]\n",
    "    }\n",
    "    if (is.null(names(dat$U))) names(dat$U) = paste0(\"Comp_\", 1:length(dat$U))\n",
    "    meta = data.frame(names(dat$U), dat$w, stringsAsFactors=F)\n",
    "    colnames(meta) = c(\"U\", \"w\")\n",
    "    tol = ${tol}\n",
    "    n_comp = length(meta$U[which(meta$w>tol)])\n",
    "    meta = head(meta[order(meta[,2], decreasing = T),], ${max_comp if max_comp > 1 else \"nrow(meta)\"})\n",
    "    message(paste(n_comp, \"components out of\", length(dat$w), \"total components have weight greater than\", tol))\n",
    "    res = list()\n",
    "    for (i in 1:n_comp) {\n",
    "        title = paste(meta$U[i], \"w =\", round(meta$w[i], 6))\n",
    "        ##Handle updated udr data structure\n",
    "        if(is.list(dat$U[[meta$U[i]]])){\n",
    "          res[[i]] = plot_sharing(dat$U[[meta$U[i]]]$mat, to_cor = ${\"T\" if to_cor else \"F\"}, title=title, remove_names = ${\"TRUE\" if remove_label else \"FALSE\"})\n",
    "        } else if(is.matrix(dat$U[[meta$U[i]]])){\n",
    "          res[[i]] = plot_sharing(dat$U[[meta$U[i]]], to_cor = ${\"T\" if to_cor else \"F\"}, title=title, remove_names = ${\"TRUE\" if remove_label else \"FALSE\"})\n",
    "        }\n",
    "    }\n",
    "    unit = 4\n",
    "    n_col = 5\n",
    "    n_row = ceiling(n_comp / n_col)\n",
    "    pdf(${_output:r}, width = unit * n_col, height = unit * n_row)\n",
    "    do.call(gridExtra::grid.arrange, c(res, list(ncol = n_col, nrow = n_row, bottom = \"Data source: readRDS(${_input:br})${('$'+name) if name else ''}\")))\n",
    "    dev.off()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "R"
    ],
    [
     "SoS"
    ]
   ],
   "version": "0.22.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
